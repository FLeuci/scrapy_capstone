[{'title': 'Semantic query parsing blueprint', 'article url': 'https://blog.griddynamics.com/semantic-query-parsing-blueprint/', 'first 160': ['These days, when machine learning-based result ', ' becomes a de-facto standard for e-commerce search implementations, it still relies on the high quality of the ', '. No matter how sophisticated your ML ranking algorithm is, if relevant products are not retrieved from the search index, it will not be able to surface them to the first page. Similarly, if irrelevant products are retrieved, ML ranking can bury them to the last pages of the results, yet they still impact the quality of the facets and will surface when results are sorted by price or newness. ', 'In the blog post ', ' we explored some approaches to achieve high-quality matches. In this blog post, we will describe a practical blueprint of building a concept-oriented query parser based on open source search engines, such as Solr or Elasticsearch.', 'Mainstream modern search engines are essentially "token machines" and are based on the ', ' paradigm. Boolean search engines model documents as a collection of fields that contain ', 'Before a search query can be executed by a search engine, it has to be converted into a combination of elementary queries which are requesting search engine to match a particular ', ' in a particular ', '. For example, the query "gucci dress" has to be converted into something like', 'This process is called "query parsing" and often search engines are supplied with a bunch of alternative query parsers, designed to deal with large full-text documents like articles or webpages.', 'However, standard query parsers fall short when it comes to e-commerce.', 'The e-commerce search system has to deal with ', ' product data, where product data contains both structured attributes and free text titles and descriptions: ', 'If we look at the most popular customer queries, they usually can be easily mapped to combinations of product attributes:', "For such queries, which typically dominate the search query logs, product relevance largely depends on the way how the query can be interpreted and matched to the product data. Let's consider a few examples:", 'The ideal scenario is represented by the top right corner of the diagram, where all terms from the query successfully and exactly match the corresponding attributes within data. This ', 'scenario typically indicates the highest quality match and generally leads to relevant results. ', 'However, in many scenarios query terms match ', 'meaning that only a part of the corresponding field matches. This indicates a weaker match and the corresponding product should be penalized in the ranking of the result set. Still, if all query terms were matched, such products may still be highly relevant', 'Another possible scenario is the ', ' which represents the situation when not all query concepts were matched in the data, but those that did were matched exactly. Even though we couldn’t capture all intents of the user, if we got key attributes such as product type correctly, we are likely to find the relevant results.', 'The last scenario is the ', ' — a situation when we could only match parts of the query terms in the data through a pure textual match. Unless we have strong collocated matches in the important fields like title, it is very unlikely to produce high-quality results.', 'All the scenarios described above can be easily handled by a concept-oriented search. By', ' we understand a semantically atomic sub-phrase, which loses its meaning if further split into terms. For example, the concept ', ' loses its meaning when divided into ', ' and ', '. Concepts are also field-specific, e.g. they may mean different things in a different context. For example, the sub-phrase ', ' has a different meaning in the phrase ', ' and ', ' ', 'The concept-oriented search mission is to be able to recognize and respect “concepts” in queries and data. To identify and tag concepts, we can leverage two main data sources: ', ' and domain-specific ', '. Many concepts are implicitly available in the product data and can be automatically extracted from it. ', 'The knowledge base contains explicit concepts which are connected into graph structure and maintained manually or semi-automatically. ', 'Both data sources can be converted into the ', ' and used to recognize concepts at both query and index time.', 'The concept-oriented query parser should support a flexible configuration which allows us to select what attributes will be searched, what normalizations can be applied to achieve the match, and what types of matches are allowed. This allows a tightly controlled balance between precision and recall in the search results.', 'Such flexible precision control allows us to organize a ', ', where we can start the search flow with strictest match conditions and gradually relax them in subsequent index sweeps until the best quality results are found.', 'Our goal is to create a query parser well-suited for online commerce needs, capable of dealing with semistructured data common for e-commerce products. We will deal with data tokenization, normalization, spelling correction, concept tagging, query expansion, and many other steps to achieve high-quality query interpretation which will help the search engine to retrieve the most relevant results. ', 'To implement all this step cleanly, efficiently, and transparently, we need to have a good abstraction to represent and visualize intermediate steps of data processing. We will use a ', ' as such an abstraction.', ' is a graph where each node represents a position within a token stream and edges represent metadata about the corresponding token stream span. ', 'The semantic query graph above describes multiple alternative interpretations of the query "the tall blablabla navy shirt dress". Each interpretation is represented as a different path in the graph.', 'The main information about the query interpretation is carried around by the graph edges, which can have different types to represent variety of metadata. In this particular example we have the following types of edges:', 'Please note that “shirt” + “dress” has a parallel edge “shirt dress” and is an example of an ambiguous query, allowing alternative interpretations that can be evaluated through a disjunction.', 'We will process the query using a pipeline of ', '. Each step will be responsible for an individual task. Every stage will accept a semantic graph as an input, modify it by adding or removing nodes and edges, and then pass the resulting graph to a subsequent step in a pipeline.', 'Why is graph abstraction so useful for our query analysis tasks? Here are some of the reasons:', 'Let’s consider an example of the analysis of the following query', 'On the initial step, the graph has only one edge which represents a full search phrase. At this stage, we will perform the initial query analysis, such as ASCII folding and tokenization. Each token can represent a word, a number, and size. In many cases, it is a good idea to have domain-specific tokenizers which are aware of units, dimensions, monetary values, and sizes', 'Spelling correction helps to deal with tokens that do not exist in the catalog or the knowledge base and try to correct them to something search system and understand. There are many approaches to implement spelling correction, from the basic edit distance search to the advanced ML-based language models. ', 'Spelling correction produces an alternative interpretation of the search terms, which we can conveniently visualize using query graph:', 'As you can see, one of the spelling hypothesis is to correct "geen" to "green". ', 'There are many concepts that allow different possible spellings which are space-separated, hyphenated, or glued in a single word. The compound word step takes care of normalizing compound words into their canonical concept. Results are added as alternative token span interpretation spanning several tokens.', 'crewneck recognized as crewneck and crew neck', 'Our knowledge base allows us to perform query expansion, e.g. enrich the query with alternative interpretations of query terms based on synonyms, hypernyms, and other types of domain-specific data. In our case, we can expand the term "green" with its hyponym, "olive".', 'Concept tagging is the main step in the concept-oriented query parsing pipeline. Here we take all the possible token spans and, using the concept index, trying to match them to recognized concepts from the catalog or the knowledge base. Results will be naturally ambiguous as the same term or set of terms can be found in many different fields with a different confidence. Our query graph representation allows us to conveniently \xa0store and visualize all this information.', 'A search phrase can contain a token span which can be interpreted in several different ways. This ambiguity can come from several sources', 'The ambiguity resolution step is filtering the query graph and removing interpretations which are likely to be the accidental concept or token matches. It uses information about field importance to properly rank alternative interpretations and cut off those edges which correspond to weaker matches. For example, if a token stream can be represented as a single multi-token concept, we can remove an individual token matches from the graph. ', ' is a part of the phrase which should not be directly matched on attributes in the index. Typically, there is several common phrases customers use to refine their intent. Examples would be "jeans under $50", "new summer dresses", "best selling watches". \xa0Those phrases should not be matched to the index directly. They have to be detected and converted to an appropriate filter or boosting factor. Graph abstraction provides a convenient way of detecting such comprehensions:', 'At this stage, we group concepts and tokens which provide alternative terms for the same field. This way, it is more convenient to create a boolean query for the search engine. ', 'Finally, our interpretation graph is ready to be converted into a boolean query that the search engine can execute.', 'Despite a large variety of search ecosystems implemented in the industry, they typically have the following key elements:', 'Integration of semantic query parsing requires two components added to the search ecosystem:', 'Internally, Semantic Search API can use the query analysis capabilities of the search engine to assist in particular query analysis steps. ', 'In this example, query analysis chains that can be configured in the Solr or Elasticsearch help with the initial steps of query analysis, and the concept index is queried using multi-search functionality to perform the concept tagging. ', 'The resulting query graph can be transferred back to the Search API and converted into the actual boolean search query which will be executed against the product index. The search API query processing pipeline should take care of organizing multi-stage search, as well as other processing steps such as the execution of business rules, search results enrichment, and re-ranking using learn-to-rank models.', 'In this blog post, we described the essential technique of semantic query parsing. This technique can be integrated straightforwardly into an existing open source-based search ecosystem and produces strong practical results when dealing with the most popular "head" and "torso" e-commerce queries. In provides a strong foundation for matching relevant results which enables the collection of high-quality clickstream data for ML-based ranking models.', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tSep 16, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tStanislav Livotov', 'author_url': 'https://blog.griddynamics.com/author/stanislav/', 'tag': ['Solr', 'Elasticsearch', 'Semantic query parsing', 'relevancy']}, {'title': 'Semantic query parsing blueprint', 'article url': 'https://blog.griddynamics.com/semantic-query-parsing-blueprint/', 'first 160': ['These days, when machine learning-based result ', ' becomes a de-facto standard for e-commerce search implementations, it still relies on the high quality of the ', '. No matter how sophisticated your ML ranking algorithm is, if relevant products are not retrieved from the search index, it will not be able to surface them to the first page. Similarly, if irrelevant products are retrieved, ML ranking can bury them to the last pages of the results, yet they still impact the quality of the facets and will surface when results are sorted by price or newness. ', 'In the blog post ', ' we explored some approaches to achieve high-quality matches. In this blog post, we will describe a practical blueprint of building a concept-oriented query parser based on open source search engines, such as Solr or Elasticsearch.', 'Mainstream modern search engines are essentially "token machines" and are based on the ', ' paradigm. Boolean search engines model documents as a collection of fields that contain ', 'Before a search query can be executed by a search engine, it has to be converted into a combination of elementary queries which are requesting search engine to match a particular ', ' in a particular ', '. For example, the query "gucci dress" has to be converted into something like', 'This process is called "query parsing" and often search engines are supplied with a bunch of alternative query parsers, designed to deal with large full-text documents like articles or webpages.', 'However, standard query parsers fall short when it comes to e-commerce.', 'The e-commerce search system has to deal with ', ' product data, where product data contains both structured attributes and free text titles and descriptions: ', 'If we look at the most popular customer queries, they usually can be easily mapped to combinations of product attributes:', "For such queries, which typically dominate the search query logs, product relevance largely depends on the way how the query can be interpreted and matched to the product data. Let's consider a few examples:", 'The ideal scenario is represented by the top right corner of the diagram, where all terms from the query successfully and exactly match the corresponding attributes within data. This ', 'scenario typically indicates the highest quality match and generally leads to relevant results. ', 'However, in many scenarios query terms match ', 'meaning that only a part of the corresponding field matches. This indicates a weaker match and the corresponding product should be penalized in the ranking of the result set. Still, if all query terms were matched, such products may still be highly relevant', 'Another possible scenario is the ', ' which represents the situation when not all query concepts were matched in the data, but those that did were matched exactly. Even though we couldn’t capture all intents of the user, if we got key attributes such as product type correctly, we are likely to find the relevant results.', 'The last scenario is the ', ' — a situation when we could only match parts of the query terms in the data through a pure textual match. Unless we have strong collocated matches in the important fields like title, it is very unlikely to produce high-quality results.', 'All the scenarios described above can be easily handled by a concept-oriented search. By', ' we understand a semantically atomic sub-phrase, which loses its meaning if further split into terms. For example, the concept ', ' loses its meaning when divided into ', ' and ', '. Concepts are also field-specific, e.g. they may mean different things in a different context. For example, the sub-phrase ', ' has a different meaning in the phrase ', ' and ', ' ', 'The concept-oriented search mission is to be able to recognize and respect “concepts” in queries and data. To identify and tag concepts, we can leverage two main data sources: ', ' and domain-specific ', '. Many concepts are implicitly available in the product data and can be automatically extracted from it. ', 'The knowledge base contains explicit concepts which are connected into graph structure and maintained manually or semi-automatically. ', 'Both data sources can be converted into the ', ' and used to recognize concepts at both query and index time.', 'The concept-oriented query parser should support a flexible configuration which allows us to select what attributes will be searched, what normalizations can be applied to achieve the match, and what types of matches are allowed. This allows a tightly controlled balance between precision and recall in the search results.', 'Such flexible precision control allows us to organize a ', ', where we can start the search flow with strictest match conditions and gradually relax them in subsequent index sweeps until the best quality results are found.', 'Our goal is to create a query parser well-suited for online commerce needs, capable of dealing with semistructured data common for e-commerce products. We will deal with data tokenization, normalization, spelling correction, concept tagging, query expansion, and many other steps to achieve high-quality query interpretation which will help the search engine to retrieve the most relevant results. ', 'To implement all this step cleanly, efficiently, and transparently, we need to have a good abstraction to represent and visualize intermediate steps of data processing. We will use a ', ' as such an abstraction.', ' is a graph where each node represents a position within a token stream and edges represent metadata about the corresponding token stream span. ', 'The semantic query graph above describes multiple alternative interpretations of the query "the tall blablabla navy shirt dress". Each interpretation is represented as a different path in the graph.', 'The main information about the query interpretation is carried around by the graph edges, which can have different types to represent variety of metadata. In this particular example we have the following types of edges:', 'Please note that “shirt” + “dress” has a parallel edge “shirt dress” and is an example of an ambiguous query, allowing alternative interpretations that can be evaluated through a disjunction.', 'We will process the query using a pipeline of ', '. Each step will be responsible for an individual task. Every stage will accept a semantic graph as an input, modify it by adding or removing nodes and edges, and then pass the resulting graph to a subsequent step in a pipeline.', 'Why is graph abstraction so useful for our query analysis tasks? Here are some of the reasons:', 'Let’s consider an example of the analysis of the following query', 'On the initial step, the graph has only one edge which represents a full search phrase. At this stage, we will perform the initial query analysis, such as ASCII folding and tokenization. Each token can represent a word, a number, and size. In many cases, it is a good idea to have domain-specific tokenizers which are aware of units, dimensions, monetary values, and sizes', 'Spelling correction helps to deal with tokens that do not exist in the catalog or the knowledge base and try to correct them to something search system and understand. There are many approaches to implement spelling correction, from the basic edit distance search to the advanced ML-based language models. ', 'Spelling correction produces an alternative interpretation of the search terms, which we can conveniently visualize using query graph:', 'As you can see, one of the spelling hypothesis is to correct "geen" to "green". ', 'There are many concepts that allow different possible spellings which are space-separated, hyphenated, or glued in a single word. The compound word step takes care of normalizing compound words into their canonical concept. Results are added as alternative token span interpretation spanning several tokens.', 'crewneck recognized as crewneck and crew neck', 'Our knowledge base allows us to perform query expansion, e.g. enrich the query with alternative interpretations of query terms based on synonyms, hypernyms, and other types of domain-specific data. In our case, we can expand the term "green" with its hyponym, "olive".', 'Concept tagging is the main step in the concept-oriented query parsing pipeline. Here we take all the possible token spans and, using the concept index, trying to match them to recognized concepts from the catalog or the knowledge base. Results will be naturally ambiguous as the same term or set of terms can be found in many different fields with a different confidence. Our query graph representation allows us to conveniently \xa0store and visualize all this information.', 'A search phrase can contain a token span which can be interpreted in several different ways. This ambiguity can come from several sources', 'The ambiguity resolution step is filtering the query graph and removing interpretations which are likely to be the accidental concept or token matches. It uses information about field importance to properly rank alternative interpretations and cut off those edges which correspond to weaker matches. For example, if a token stream can be represented as a single multi-token concept, we can remove an individual token matches from the graph. ', ' is a part of the phrase which should not be directly matched on attributes in the index. Typically, there is several common phrases customers use to refine their intent. Examples would be "jeans under $50", "new summer dresses", "best selling watches". \xa0Those phrases should not be matched to the index directly. They have to be detected and converted to an appropriate filter or boosting factor. Graph abstraction provides a convenient way of detecting such comprehensions:', 'At this stage, we group concepts and tokens which provide alternative terms for the same field. This way, it is more convenient to create a boolean query for the search engine. ', 'Finally, our interpretation graph is ready to be converted into a boolean query that the search engine can execute.', 'Despite a large variety of search ecosystems implemented in the industry, they typically have the following key elements:', 'Integration of semantic query parsing requires two components added to the search ecosystem:', 'Internally, Semantic Search API can use the query analysis capabilities of the search engine to assist in particular query analysis steps. ', 'In this example, query analysis chains that can be configured in the Solr or Elasticsearch help with the initial steps of query analysis, and the concept index is queried using multi-search functionality to perform the concept tagging. ', 'The resulting query graph can be transferred back to the Search API and converted into the actual boolean search query which will be executed against the product index. The search API query processing pipeline should take care of organizing multi-stage search, as well as other processing steps such as the execution of business rules, search results enrichment, and re-ranking using learn-to-rank models.', 'In this blog post, we described the essential technique of semantic query parsing. This technique can be integrated straightforwardly into an existing open source-based search ecosystem and produces strong practical results when dealing with the most popular "head" and "torso" e-commerce queries. In provides a strong foundation for matching relevant results which enables the collection of high-quality clickstream data for ML-based ranking models.', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tSep 16, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitriy Bobkov', 'author_url': 'https://blog.griddynamics.com/author/dmitriy/', 'tag': ['Solr', 'Elasticsearch', 'Semantic query parsing', 'relevancy']}, {'title': 'Semantic query parsing blueprint', 'article url': 'https://blog.griddynamics.com/semantic-query-parsing-blueprint/', 'first 160': ['These days, when machine learning-based result ', ' becomes a de-facto standard for e-commerce search implementations, it still relies on the high quality of the ', '. No matter how sophisticated your ML ranking algorithm is, if relevant products are not retrieved from the search index, it will not be able to surface them to the first page. Similarly, if irrelevant products are retrieved, ML ranking can bury them to the last pages of the results, yet they still impact the quality of the facets and will surface when results are sorted by price or newness. ', 'In the blog post ', ' we explored some approaches to achieve high-quality matches. In this blog post, we will describe a practical blueprint of building a concept-oriented query parser based on open source search engines, such as Solr or Elasticsearch.', 'Mainstream modern search engines are essentially "token machines" and are based on the ', ' paradigm. Boolean search engines model documents as a collection of fields that contain ', 'Before a search query can be executed by a search engine, it has to be converted into a combination of elementary queries which are requesting search engine to match a particular ', ' in a particular ', '. For example, the query "gucci dress" has to be converted into something like', 'This process is called "query parsing" and often search engines are supplied with a bunch of alternative query parsers, designed to deal with large full-text documents like articles or webpages.', 'However, standard query parsers fall short when it comes to e-commerce.', 'The e-commerce search system has to deal with ', ' product data, where product data contains both structured attributes and free text titles and descriptions: ', 'If we look at the most popular customer queries, they usually can be easily mapped to combinations of product attributes:', "For such queries, which typically dominate the search query logs, product relevance largely depends on the way how the query can be interpreted and matched to the product data. Let's consider a few examples:", 'The ideal scenario is represented by the top right corner of the diagram, where all terms from the query successfully and exactly match the corresponding attributes within data. This ', 'scenario typically indicates the highest quality match and generally leads to relevant results. ', 'However, in many scenarios query terms match ', 'meaning that only a part of the corresponding field matches. This indicates a weaker match and the corresponding product should be penalized in the ranking of the result set. Still, if all query terms were matched, such products may still be highly relevant', 'Another possible scenario is the ', ' which represents the situation when not all query concepts were matched in the data, but those that did were matched exactly. Even though we couldn’t capture all intents of the user, if we got key attributes such as product type correctly, we are likely to find the relevant results.', 'The last scenario is the ', ' — a situation when we could only match parts of the query terms in the data through a pure textual match. Unless we have strong collocated matches in the important fields like title, it is very unlikely to produce high-quality results.', 'All the scenarios described above can be easily handled by a concept-oriented search. By', ' we understand a semantically atomic sub-phrase, which loses its meaning if further split into terms. For example, the concept ', ' loses its meaning when divided into ', ' and ', '. Concepts are also field-specific, e.g. they may mean different things in a different context. For example, the sub-phrase ', ' has a different meaning in the phrase ', ' and ', ' ', 'The concept-oriented search mission is to be able to recognize and respect “concepts” in queries and data. To identify and tag concepts, we can leverage two main data sources: ', ' and domain-specific ', '. Many concepts are implicitly available in the product data and can be automatically extracted from it. ', 'The knowledge base contains explicit concepts which are connected into graph structure and maintained manually or semi-automatically. ', 'Both data sources can be converted into the ', ' and used to recognize concepts at both query and index time.', 'The concept-oriented query parser should support a flexible configuration which allows us to select what attributes will be searched, what normalizations can be applied to achieve the match, and what types of matches are allowed. This allows a tightly controlled balance between precision and recall in the search results.', 'Such flexible precision control allows us to organize a ', ', where we can start the search flow with strictest match conditions and gradually relax them in subsequent index sweeps until the best quality results are found.', 'Our goal is to create a query parser well-suited for online commerce needs, capable of dealing with semistructured data common for e-commerce products. We will deal with data tokenization, normalization, spelling correction, concept tagging, query expansion, and many other steps to achieve high-quality query interpretation which will help the search engine to retrieve the most relevant results. ', 'To implement all this step cleanly, efficiently, and transparently, we need to have a good abstraction to represent and visualize intermediate steps of data processing. We will use a ', ' as such an abstraction.', ' is a graph where each node represents a position within a token stream and edges represent metadata about the corresponding token stream span. ', 'The semantic query graph above describes multiple alternative interpretations of the query "the tall blablabla navy shirt dress". Each interpretation is represented as a different path in the graph.', 'The main information about the query interpretation is carried around by the graph edges, which can have different types to represent variety of metadata. In this particular example we have the following types of edges:', 'Please note that “shirt” + “dress” has a parallel edge “shirt dress” and is an example of an ambiguous query, allowing alternative interpretations that can be evaluated through a disjunction.', 'We will process the query using a pipeline of ', '. Each step will be responsible for an individual task. Every stage will accept a semantic graph as an input, modify it by adding or removing nodes and edges, and then pass the resulting graph to a subsequent step in a pipeline.', 'Why is graph abstraction so useful for our query analysis tasks? Here are some of the reasons:', 'Let’s consider an example of the analysis of the following query', 'On the initial step, the graph has only one edge which represents a full search phrase. At this stage, we will perform the initial query analysis, such as ASCII folding and tokenization. Each token can represent a word, a number, and size. In many cases, it is a good idea to have domain-specific tokenizers which are aware of units, dimensions, monetary values, and sizes', 'Spelling correction helps to deal with tokens that do not exist in the catalog or the knowledge base and try to correct them to something search system and understand. There are many approaches to implement spelling correction, from the basic edit distance search to the advanced ML-based language models. ', 'Spelling correction produces an alternative interpretation of the search terms, which we can conveniently visualize using query graph:', 'As you can see, one of the spelling hypothesis is to correct "geen" to "green". ', 'There are many concepts that allow different possible spellings which are space-separated, hyphenated, or glued in a single word. The compound word step takes care of normalizing compound words into their canonical concept. Results are added as alternative token span interpretation spanning several tokens.', 'crewneck recognized as crewneck and crew neck', 'Our knowledge base allows us to perform query expansion, e.g. enrich the query with alternative interpretations of query terms based on synonyms, hypernyms, and other types of domain-specific data. In our case, we can expand the term "green" with its hyponym, "olive".', 'Concept tagging is the main step in the concept-oriented query parsing pipeline. Here we take all the possible token spans and, using the concept index, trying to match them to recognized concepts from the catalog or the knowledge base. Results will be naturally ambiguous as the same term or set of terms can be found in many different fields with a different confidence. Our query graph representation allows us to conveniently \xa0store and visualize all this information.', 'A search phrase can contain a token span which can be interpreted in several different ways. This ambiguity can come from several sources', 'The ambiguity resolution step is filtering the query graph and removing interpretations which are likely to be the accidental concept or token matches. It uses information about field importance to properly rank alternative interpretations and cut off those edges which correspond to weaker matches. For example, if a token stream can be represented as a single multi-token concept, we can remove an individual token matches from the graph. ', ' is a part of the phrase which should not be directly matched on attributes in the index. Typically, there is several common phrases customers use to refine their intent. Examples would be "jeans under $50", "new summer dresses", "best selling watches". \xa0Those phrases should not be matched to the index directly. They have to be detected and converted to an appropriate filter or boosting factor. Graph abstraction provides a convenient way of detecting such comprehensions:', 'At this stage, we group concepts and tokens which provide alternative terms for the same field. This way, it is more convenient to create a boolean query for the search engine. ', 'Finally, our interpretation graph is ready to be converted into a boolean query that the search engine can execute.', 'Despite a large variety of search ecosystems implemented in the industry, they typically have the following key elements:', 'Integration of semantic query parsing requires two components added to the search ecosystem:', 'Internally, Semantic Search API can use the query analysis capabilities of the search engine to assist in particular query analysis steps. ', 'In this example, query analysis chains that can be configured in the Solr or Elasticsearch help with the initial steps of query analysis, and the concept index is queried using multi-search functionality to perform the concept tagging. ', 'The resulting query graph can be transferred back to the Search API and converted into the actual boolean search query which will be executed against the product index. The search API query processing pipeline should take care of organizing multi-stage search, as well as other processing steps such as the execution of business rules, search results enrichment, and re-ranking using learn-to-rank models.', 'In this blog post, we described the essential technique of semantic query parsing. This technique can be integrated straightforwardly into an existing open source-based search ecosystem and produces strong practical results when dealing with the most popular "head" and "torso" e-commerce queries. In provides a strong foundation for matching relevant results which enables the collection of high-quality clickstream data for ML-based ranking models.', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tSep 16, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAleksandr Kvasov', 'author_url': 'https://blog.griddynamics.com/author/aleksandr-kvasov/', 'tag': ['Solr', 'Elasticsearch', 'Semantic query parsing', 'relevancy']}, {'title': 'Why Flutter should be your next mobile development framework', 'article url': 'https://blog.griddynamics.com/why-flutter-should-be-your-next-mobile-development-framework/', 'first 160': [' This is what mobile application developers have long been promised.', 'Every year, frameworks get closer to "The Promise", but never quite achieve it. Native mobile applications deliver the best performance, but require separate teams with platform-specific skills. Also, the maintenance of native apps is resource-intensive. Cross-platform application development requires only one team, but introduces new performance limitations. Each additional mobile platform adds more overhead, and requires more resources to support. As the code size grows, execution speed suffers.', 'For mobile application development, Flutter may be the cross-platform development framework that provides the right balance of performance and resource management to finally achieve “The Promise”. We will compare Flutter with some of its competitors: Native, Ionic and React Native, to show why Flutter is the best choice.', 'To ensure the best possible performance on all platforms, applications must be developed using each platform’s native code. The code will be specifically optimized to take advantage of each platform’s features. The block diagram in Figure 1 shows the native code interfaces that connect directly to the platform hardware.', 'However, this approach contains significant drawbacks. Each platform you want to run the app on, requires developing a separate application, duplicating efforts. The same duplication occurs for QA and maintenance cycles. Therefore, the advantages of native code development comes at a heavy price.', 'Ionic, along with other hybrid frameworks, attempts to utilize some common features of multiple platforms, namely HTML, CSS and JavaScript. Your application code executes inside the platform’s native web browser as shown in Figure 2. The hybrid framework then creates a software bridge to call platform-specific services.', 'The first version of Ionic was released in 2013, supporting Android and iOS. The Ionic framework is mixed with native platform code utilizing AngularJS, leveraging CSS for styling and animation. Ionic uses Cordova plugins to gain access to the host operating system features, such as the camera, GPS, flashlight, etc.', 'Ionic is good, but has performance issues with animation-intense applications: modern animated UI is an Ionic weak point. Native code may be used to solve some of the performance problems with graphics, but this hybrid approach loses some of the cross-platform capabilities. This framework is adequate for applications without heavy graphic requirements, but comes up short of "The Promise".', 'In 2015, Facebook adapted ReactJS, a tool for large websites, to create React Native for mobile applications. React Native uses familiar JavaScript, which calls native code. This allows the developer to use components written in JavaScript, as well as native code. React Native also provides many ready-to-use components to aid in application development.', 'The React Native framework, shown in Figure 3, is a step closer to fulfilling "The Promise", but it is still not perfect. Because a software bridge is needed to connect the developer’s JavaScript to the execution platform, performance suffers. There is constant interaction over the JavaScript bridge, with permanent JSON serialization and deserialization. This file I/O slows graphics performance, effectively disabling any fancy UI effects.', "In 2017, Google introduced its own clean-sheet design for cross-platform application development: Flutter, which they hoped would finally fulfill “The Promise”. Flutter apps are written in the Google development language, Dart, and make use of many of the language's more advanced features. Dart is closely tied to Java, so it’s easy to learn, and more powerful than JavaScript.", "Flutter is an open-source SDK used to create superior-quality apps for Android and iOS using a single codebase. It is designed and optimized for fast rendering and complex animations, making the UI coding even easier than it would be with native development code. The architecture is optimized for vector graphics animation. Animations can be easily run inside Flutter, even achieving 120 frames per second. Fuchsia, Google’s next generation OS, will support Flutter, in addition to supporting Android and iOS. Fuchsia's user interface and apps are written with Flutter.", 'Flutter is comprised of four main components:', 'Google has assigned considerable resources to the Flutter ecosystem, resulting in an extremely stable framework. Indeed, the framework is considered more stable than React Native, despite the latter being a more mature product. This can be attributed to Google’s long-term commitment to Flutter, and its strategic importance to the Google OS roadmap.', 'Both the biggest advantage and disadvantage of Flutter is the use of a clean-sheet design. By starting anew, accomplishing "The Promise" is more achievable without having to deal with the baggage of legacy code. However, it also creates a slightly steeper learning curve, since the developer must now learn a new programming language. Although Flutter is newer than other cross-platform frameworks, it has had less time to “prove itself” to the developer community.', 'Flutter offers the strong support and backing of Google with a highly-attractive UI and excellent native performance. Once the developer is up-to-speed using the Dart programming environment, development time is faster than any alternatives.', "If you're starting a new ground-up project, Grid Dynamics recommends investing resources in the Flutter ecosystem. Grid Dynamics is experienced in application development and can assist you in your next project. Please ", ' at +1 (650)523-5000.'], 'date': '\r\n\t\t\t\t\t\t\tFeb 13, 2019\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Sumtsov', 'author_url': 'https://blog.griddynamics.com/author/dmitry-sumtsov/', 'tag': ['Mobile']}, {'title': 'More details on building cloud-portable DevOps stack with Mesos and Marathon', 'article url': 'https://blog.griddynamics.com/more-details-on-building-cloud-portable-devops-stack-with-mesos-and-marathon/', 'first 160': ['In the previous blog post we explained our overall approach to the DevOps stack used for the deployment and management of the In-Stream Processing blueprint. In this post we’ll focus on more details of Mesos and Marathon, and provide you with scripts to provision the complete computational environment on any cloud of your choice.\xa0', 'The advantage of Docker containers over configuration management tools for DevOps operations:', 'Let’s quickly recap, once again, why the Docker-based DevOps stack is better than traditional configuration management tools.', 'Docker-based application deployment solutions represent the second-generation approach to DevOps. The traditional first-generation approach was based on configuration management tools such as ', ', ', ', ', ', and ', '. These tools share the same core idea: to provide a layer of abstraction above the OS in order to simplify routine operations and deliver, more or less, application portability across different operating environments. Their deployment scripts can run on different environments, and produce the required application configurations with little or no modification.\xa0', 'This approach worked well in most cases — but not without limitations, such as:', 'Containers are rapidly gaining momentum because they take the “build once, run everywhere” promise to the next level, and provide significantly lighter-weight application virtualization. Effectively, they do for process management what Java did for application management. We can now package all the dependencies between an application and its underlying operating system into a container that can be reused all the way from developer workstation to production infrastructure.', '\xa0', 'Let’s review the container-centric DevOps stack we are using for deployment and management of our in-Stream Processing platform.', 'Mesos has a number of outstanding features that makes it one of the best modern choices as a container management platform.', 'A typical deployment topology of Mesos master nodes and managed nodes looks like this:', 'Basic Mesos features permit resource allocation out of the VM fabrics to a specific task, like running a Docker container, but Mesos does not itself handle that. The role of process management is delivered via the framework, in our case, ', '.', 'Marathon is one of the most commonly used Mesos frameworks that provides the following standard capabilities:', 'We can have a perfectly fine working setup with a “plainvanilla” installation of Mesos and Marathon, but some features can provide additional functionality. Here are a few we found useful in our \xa0reference architecture:', 'Since the “Persistent volumes” feature is still shown as “experimental,” it needs to be explicitly enabled. (The\xa0', ' is a big help here.)', 'As we described in some detail in the previous post, we will use Ansible to deploy Mesos and Marathon on a collection of VMs, some of which will be used as Master nodes and the rest as a resource pool.', 'The process is very simple and straightforward, requiring only a minimal learning curve. Here is a quick-start guide on how to do it:\xa0', 'For super-quick learning, you can spin up the complete environment right on your workstation, in which case no additional cloud infrastructure will be required until you are ready to scale your cluster.', '-----', ' Now you know the basics of setting up Mesos and Marathon as a DevOps foundation for Docker, and you have scripts you can install on your workstation (or wherever) to get some hands-on experience with them. If you you have any questions, Grid Dynamics is happy to help. Please ', '\xa0and we’ll get back to you shortly. And don’t forget to subscribe to our blog. We have lots more useful information about Mesos, Marathon, Docker, DevOps, and other interesting topics coming up.', 'We will continue to cover technology behind the scene of our project deeper in next posts, however here are some links if you want to know more:'], 'date': '\r\n\t\t\t\t\t\t\tJan 03, 2017\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'Deploying In-Stream Processing']}, {'title': 'DevOps stack for In-Stream Processing Service using AWS, Docker, Mesos, Marathon, Ansible and Tonomi', 'article url': 'https://blog.griddynamics.com/devops-stack-for-in-stream-processing-service-using-aws-docker-mesos-marathon-ansible-and-tonomi/', 'first 160': ['This post is about the approach to the “DevOps” part of our In-Stream Processing blueprint — namely, deploying the platform on a dynamic cloud infrastructure, making the service available to its intended users and supporting it through the continuous lifecycle of development, testing, and roll-out of new features.\xa0', 'Doing “DevOps” right or wrong can bring shine or shame to the project, so it is important to pay close attention to this aspect of the platform architecture and delivery. So let’s begin the discussion of the design, implementation, and maintenance of infrastructure-related aspects for our blueprint with selection of the operational technology stack.', 'We have analyzed a number of technology choices to use for the reference implementation of the service and chose the following stack:', 'Let’s discuss each technology in some details and explain our rationale for the choice.\xa0', 'Since we are designing a service that can process massive amounts of events and scale easily with growing loads, it is obvious that the computing infrastructure should reside in a cloud. Faced with a cloud choice for the reference implementation, we considered a number of popular platforms including AWS, Google Cloud and Azure, and chose AWS as the most mature, most commonly used, best-documented, and most feature-rich cloud service on the market.', 'It is important to point out that we consider a choice of the specific cloud a matter of convenience, not architectural significance. Moreover, we fully expect each customer to make their own choice of the cloud platform, which may differ from AWS. The blueprint goes to great lengths to make sure a specific cloud can be simply “plugged in,” with the rest of the system working without any modification. This is a bit idealistic, perhaps, since the choice of a particular cloud carries a number of operational considerations. Still, we purposefully avoided relying on any AWS-specific APIs that would lock the solution into a single platform.', 'In a nutshell, cloud portability is achieved by placing the responsibility for application deployment and management on other layers of the stack — Docker, Mesos, Marathon, Ansible and Tonomi. AWS simply provides the VMs that are used by Mesos to place and manage containers. Any cloud that can provide its VMs to Mesos should work equally well, at least conceptually.', 'Porting our reference implementation to another cloud would require replacing AWS APIs with the equivalent APIs from a different cloud provider. We will discuss this in detail later on, when we deal with deploying and configuring Mesos on AWS.', 'We had a choice between two approaches for deployment and management of our platform: “classic” VM-based and “modern” container-based. These two approaches can be roughly described as follows:', 'The advantage of the “classic” approach is its maturity and wide usage in production today. We can rely on many existing operational practices and cookbooks, but it has two principal weaknesses:', 'By contrast, containers in general, and Docker specifically, take application portability and on-demand launching and scaling to a different level:', 'For these reasons, containers are quickly becoming the mainstream technology of choice for new application deployment. We decided that the benefits of containers outweigh their challenges, and implemented operational support using Docker and related technologies from the Docker ecosystem.', 'Once we figured out that we are going ahead with Docker, we had to decide how to manage the container fleet, at scale. There are several possibilities available on the market that required consideration, most notable being:', 'Docker Swarm is a “native” solution that offers portability across a wide range of infrastructure choices, from laptops to production clusters. Unfortunately, by mid-2016 the technology is still too young and according to many reports from early adopters, suffering from “childhood issues”. While we decided to pass on this technology for the time being due to its current maturity level, we will follow its progress and reevaluate in the future. Here are some links to discussions of Docker’s “teething pains”:', 'The last two choices, AWS ECS/BeanStalk and Google Container Engine have to be eliminated since they are cloud-specific and we are looking for a portable solution that can run on any cloud.', 'So, in reality we are down to 2 main competitors, Mesos + Marathon and Kubernetes. Both technologies are relatively new, yet reasonable choices. There are several nuances between the two that make them somewhat more suitable for different use cases. Since this a hotly debated topic in the industry, there is no shortage of the opinions. Here are a few quality white papers and articles that describe the differences, if you would like to read more about this topic:', 'We chose Mesos + Marathon because it is more mature, in wider use, and has a good track record regarding production use cases — and also because our team has more experience with Mesos and its popular container fleet management framework, Marathon, than with Kubernetes.', 'Mesos is designed to help manage a fleet of individual compute units (hardware, VMs) as a whole, giving access to joint resources (cpu, ram, disk) as if they’re one huge VM, providing resource allocation with load balancing and migration of individual tasks across an entire cluster when needed, at least to a large extent. There are some exceptions: for example, you cannot allocate a single chunk of resources larger than you have it on an individual compute unit. If we continue drawing parallels between the cluster as a whole and a single VM, Mesos is the kernel in charge of resource allocation.', 'It’s hard to run modern software using only the capabilities of the OS kernel, so in order to extend its functionality, Mesos has a powerful extension mechanism called frameworks. One of the most commonly used Mesos frameworks is Marathon — which in our analogy will substitute for a process manager. It allows us to easily run and manage Mesos tasks, and one of the supported task types is running a Docker container.\xa0', 'Marathon also provides some very handy features, such as easy scaling and integration with load balancers for running tasks, which we are going to leverage.', 'The desired deployment topology of the In-Stream Processing platform we want to get from the DevOps platform looks like this:', 'To achieve this, Mesos will be managing VMs used underneath of all these application containers so that Marathon can launch and place the containers in the respective groups during the initial application deployment, or in response to scaling or failover situations.', 'A Mesos cluster must be deployed once to bootstrap the cloud deployment of any containerized application service before the application environments can be provisioned with Mesos. This is pretty obvious when you think about it, so we have to answer the question, “How do we automate bootstrapping the initial Mesos deployment on the pristine cloud infrastructure?”', 'In this case, Mesos will be running as a VM, not as a Docker container. We chose Ansible as our configuration management tool for this bootstrapping. Alternatives include other configuration management tools such as Chef, Puppet or SaltStack, but we chose Ansible for its simplicity, popularity and — and for existing deployment scripts we had already written in-house for other projects that we could easily reuse.\xa0', 'The goal of using the Ansible scripts is to achieve the following state of the system after bootstrapping is complete:', 'Let’s stop here for a second to reflect on the following idea: Deploying the Mesos platform as \xa0part of the bootstrapping operation is somewhat specific to every cloud service because we are working with VM-based infrastructure. But once Mesos is up and running, the remaining application deployment and management is handled in a totally cloud-independent way, relying only on the APIs of Mesos and its framework, Marathon.', 'Finally, with the Mesos infrastructure in place, so it can provision any Docker container on demand, it is time to ask this last question:', '“How will we automate the complex orchestration workflows that will deploy different application clusters for Kafka, Spark Streaming, Redis, Cassandra, Twitter API and social analytics visualization web application, configure all these services, resolve their dependencies on each other, and later handle complex re-configuration change management requests such as scaling and failover?”\xa0', 'Put even more simply, we need something to take care of the interactions between the services in our platform, tie them together, and provide the ability to easily deploy the complete In-Stream Processing platform to the Mesos cluster.\xa0', 'It is worth pointing out that, unlike the bootstrapping of the Mesos cluster that only happens once, spinning up application environments holding the In-Stream Processing platform, or its reconfiguration, will happen all the time, sometimes on a daily basis. Events that lead to fresh deployment or reconfiguration include:\xa0', 'We will use ', ' to provide application management automation. Sadly, the market doesn’t have a robust selection of open source tools capable of application management automation of this type. Various PaaS solutions, like Cloud Foundry, are not suitable for this use case so we chose Tonomi, which is a SaaS-based DevOps platform.\xa0', 'Admittedly, this is an in-house choice since Grid Dynamics acquired ', ' in 2015 to provide these capabilities. There is a free version of Tonomi\xa0available on AWS that has all the features we need to provide one-click deployment of the complete In-Stream Processing service over Mesos infrastructure. Tonomi supports Ansible, and runs on AWS itself.\xa0', 'We also happen to know that Grid Dynamics has plans to open source the next version of Tonomi as soon as practical. While we cannot commit our colleagues to any specific schedule, these intentions played a role in our decision to use Tonomi in this reference implementation.', 'In the next post we’ll delve into the details of how Docker, Mesos and Marathon are used together to deliver fully-automated portable management of application containers.\xa0'], 'date': '\r\n\t\t\t\t\t\t\tDec 13, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'Deploying In-Stream Processing', 'Kubernetes vs. Mesos']}, {'title': 'Creating training and test data sets and preparing the data', 'article url': 'https://blog.griddynamics.com/creating-training-and-test-data-sets-and-preparing-the-data-for-twitter-stream-sentiment-analysis-of-social-movie-reviews/', 'first 160': ['In the previous post we discussed how we created an appropriate data dictionary. In this post we’ll address the process of building the training data sets and preparing the data for analysis.', 'The training process aims to reveal hidden dependencies and patterns in the data that will be analyzed. Therefore, the training and test data set must be a representative sample of the target data. It has to be broad enough to cover all cases; e.g. if it is time-series data about sales, the training and test dataset ought to represent a reasonable business cycle that covers peak and off-peak times, weekends, etc.\xa0', 'Coming back to natural language processing, of which sentiment analysis is a subset, it’s important to cover as many words as possible that express sentiment and represent the lexicon used in the target texts. When we speak about supervised learning for natural language processing, the labeling has to be performed manually to guarantee correct classification of the training and test data. The training dataset must contain enough, and balanced enough, entries representing each value for the target variable to be representative. In our case, these are text perceived as either positive or negative. There is no universal guidance for the size of the training data set; it depends on the number of variables used for training and it must contain several examples of every typical observation. In our case variables are words from the dictionary. Every of this word must be be represented in several texts of corresponding sentiment. Texts may contain several dictionary words of opposite sentiment so we need several such examples to understand which word bears stronger sentiment. Because of all that. the number or texts in our training dataset must be at least 10 times the number of words in our dictionary.', 'As a training data set we use ', '\xa0 The dataset consists of two subsets\xa0', ' training and test data — that are located in separate sub-folders (test and train). Every subset contains 25000 reviews including 12500 positive and 12500 negative. Every review is stored in a separate file corresponding to the sentiment (train/neg or train/pos) subfolder. The size of a review can vary from 100 bytes to 14 kilobytes with a median size of about 850 bytes.', 'Here is a typical viewers’ movie review:', 'This dataset was used in a research paper titled Learning Word Vectors for Sentiment Analysis published by Stanford University, which also includes additional data related to the research. ', 'We will use 25000 reviews from the “train” subfolder. As you can see from the movie review above, many of them don’t look like tweets. Tweets tend to be much shorter than typical IMDB reviews, emojis are used in tweets much much often than in IMDB reviews, And there tweets often contain less rigorous grammar and spelling than IMDB reviews. All that may negatively impact the efficiency of the learning process where the training set will be used, namey, with tweets. On the positive side, the texts are about the same topic. Ideally, we would need an appropriate library of tweets. Unfortunately, there is no available labeled tweet library of a reasonable size (several dozens of thousands tweets) pertaining to movie reviews. Therefore, IMDb became a reasonable tradeoff between the level of resources needed to get the data and the data set quality.', 'As a test dataset we will use ', ' from Niek Sanders’. It contains tweets labeled as positive, negative, neutral or irrelevant.', 'Many tweets labeled as irrelevant are in languages other than English, so we removed them from the test dataset. We also combined positive- and neutral-labeled tweets in one category as our task is to identify (specifically)negative tweets so we must be able to separate negative from neutral tweets, but separating neutral from positive is beyond our scope. So we can see that the negative and positive classes are very skewed: 572 negative and 2852 positive samples. It will be crucial during evaluation of the modeling results to understand which part of the ROC-diagram is more important in our case.\xa0', 'Twitter users have widely varying social power based on their reputations and number of followers. Reputation is a latent characteristic, while number of followers is observable, therefore especially important. Any Twitter user’s tweet or retweet produces light or heavy social impact directly linked to the number of the tweeter’s followers as that is the number of people who will see the user’s post.', 'It’s interesting to see who makes the main buzz around a movie. What segments should we explore? The “followers number” can range\xa0zero to hundreds of thousands. \xa0About 50% of Twitter users have 300-600 followers.\xa0', 'In 2012 an average twitter user had 208 followers. Unfortunately there is no more recent information, but we can guess that now, in 2016, an average Twitter user has more followers than that, but not a huge increase, so we will call users who have fewer than 500 followers regular users. ', 'Another ~45% have up to 5000 followers. These users are likely not professional tweeter but their influence is much greater than regular users’ since their tweets or retweets will be seen by more than 500, possibly even thousands of people. Let’s call them opinion leaders.', 'Apparently, about 5% of Twitter users have more than 5000 followers. In many cases these Twitter accounts belong to business like magazines, cinemas and movie studios.\xa0They are professionals. We will explore which of them\xa0', ' regular users, opinion leaders or professionals', ' make the greatest buzz about a movie.', 'Now it’s time to do the last task in the scope of the data understanding phase: drafting the solution. We will build the simplest but still effective model. By “effective,” I mean the model should work significantly better than random guessing. In other words the percentage of correctly classified tweets with the suggested solution ought to be at least 10% higher than random guessing.', 'Here is a list of the basic principles we will follow while building the solution:', 'In order to train and later apply the models to the Twitter data it is first important to clean and prepare the data. This is the “dirty work” that occupies large swaths of data scientists’ time. In our specific case, models accept binary matrices indicating whether or not a word was present in the text as an input. So we have to transform every tweet into a vector of binary values. This operation is called “feature extraction.” This is accomplished in a number of steps.\xa0', 'First the text is extracted (assume we got a text filtered by the keyword “Doctor Strange”):', 'Then several normalization steps are performed\xa0', ' removing capitalization, punctuation, keywords used for tweet filtering, and unicode emoticons. Ideally we should preserve emoticons by replacing them with simple “synonyms” like “:)” or “:(“.', 'Leveraging synonyms, hyponyms, and hypernyms is a much more comprehensive natural language processing technique than we’re using here; indeed, this topic deserves its own dedicated article, so we’re consciously omitting\xa0it here for the sake of simplification.\xa0', 'Now the text looks like this:', 'Next the text is tokenized', ' split into tokens. Pay attention at the “cool stuff” substring. While performing tokenization we should recall that some dictionaries contain n-grams like “cool stuff” or “does not work.” The tokenization process should not lose those tokens. The described cleansing and tokenization may be accomplished by many means, and the most\xa0comprehensive solution deserves a separate article. We do it here in a very straightforward way: unnecessary symbols are removed with a regular expression, the text is separated spaces considering exceptions (like “cool stuff”).', 'Now we are almost ready to search for word matches in texts and dictionary entries. The last processing step we are going to perform is stemming. It’s a reduction of various forms of a word to their so-called stem. In fact, this transformation is a type of dimensionality reduction that solves two challenges: It increases matching rate and reduces the computational complexity of the machine learning algorithm. Stemming has to be performed on both the text and the dictionary. After this step the text from the tweet looks like this:', 'Once the words are stemmed, it is possible to search for dictionary words within the text. \xa0', 'So summarizing the simplified process of data preparation:', 'Now we have all data necessary and properly prepared to perform the modeling or, in other words, build a classifier that will tell whether a tweet is negative or positive. How to do that\xa0', ' that is, how to evaluate classification performance and determine which approach appears to be better\xa0', 'is what we will discuss in the next blog post.'], 'date': '\r\n\t\t\t\t\t\t\tNov 18, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Creating training and test data sets and preparing the data', 'article url': 'https://blog.griddynamics.com/creating-training-and-test-data-sets-and-preparing-the-data-for-twitter-stream-sentiment-analysis-of-social-movie-reviews/', 'first 160': ['In the previous post we discussed how we created an appropriate data dictionary. In this post we’ll address the process of building the training data sets and preparing the data for analysis.', 'The training process aims to reveal hidden dependencies and patterns in the data that will be analyzed. Therefore, the training and test data set must be a representative sample of the target data. It has to be broad enough to cover all cases; e.g. if it is time-series data about sales, the training and test dataset ought to represent a reasonable business cycle that covers peak and off-peak times, weekends, etc.\xa0', 'Coming back to natural language processing, of which sentiment analysis is a subset, it’s important to cover as many words as possible that express sentiment and represent the lexicon used in the target texts. When we speak about supervised learning for natural language processing, the labeling has to be performed manually to guarantee correct classification of the training and test data. The training dataset must contain enough, and balanced enough, entries representing each value for the target variable to be representative. In our case, these are text perceived as either positive or negative. There is no universal guidance for the size of the training data set; it depends on the number of variables used for training and it must contain several examples of every typical observation. In our case variables are words from the dictionary. Every of this word must be be represented in several texts of corresponding sentiment. Texts may contain several dictionary words of opposite sentiment so we need several such examples to understand which word bears stronger sentiment. Because of all that. the number or texts in our training dataset must be at least 10 times the number of words in our dictionary.', 'As a training data set we use ', '\xa0 The dataset consists of two subsets\xa0', ' training and test data — that are located in separate sub-folders (test and train). Every subset contains 25000 reviews including 12500 positive and 12500 negative. Every review is stored in a separate file corresponding to the sentiment (train/neg or train/pos) subfolder. The size of a review can vary from 100 bytes to 14 kilobytes with a median size of about 850 bytes.', 'Here is a typical viewers’ movie review:', 'This dataset was used in a research paper titled Learning Word Vectors for Sentiment Analysis published by Stanford University, which also includes additional data related to the research. ', 'We will use 25000 reviews from the “train” subfolder. As you can see from the movie review above, many of them don’t look like tweets. Tweets tend to be much shorter than typical IMDB reviews, emojis are used in tweets much much often than in IMDB reviews, And there tweets often contain less rigorous grammar and spelling than IMDB reviews. All that may negatively impact the efficiency of the learning process where the training set will be used, namey, with tweets. On the positive side, the texts are about the same topic. Ideally, we would need an appropriate library of tweets. Unfortunately, there is no available labeled tweet library of a reasonable size (several dozens of thousands tweets) pertaining to movie reviews. Therefore, IMDb became a reasonable tradeoff between the level of resources needed to get the data and the data set quality.', 'As a test dataset we will use ', ' from Niek Sanders’. It contains tweets labeled as positive, negative, neutral or irrelevant.', 'Many tweets labeled as irrelevant are in languages other than English, so we removed them from the test dataset. We also combined positive- and neutral-labeled tweets in one category as our task is to identify (specifically)negative tweets so we must be able to separate negative from neutral tweets, but separating neutral from positive is beyond our scope. So we can see that the negative and positive classes are very skewed: 572 negative and 2852 positive samples. It will be crucial during evaluation of the modeling results to understand which part of the ROC-diagram is more important in our case.\xa0', 'Twitter users have widely varying social power based on their reputations and number of followers. Reputation is a latent characteristic, while number of followers is observable, therefore especially important. Any Twitter user’s tweet or retweet produces light or heavy social impact directly linked to the number of the tweeter’s followers as that is the number of people who will see the user’s post.', 'It’s interesting to see who makes the main buzz around a movie. What segments should we explore? The “followers number” can range\xa0zero to hundreds of thousands. \xa0About 50% of Twitter users have 300-600 followers.\xa0', 'In 2012 an average twitter user had 208 followers. Unfortunately there is no more recent information, but we can guess that now, in 2016, an average Twitter user has more followers than that, but not a huge increase, so we will call users who have fewer than 500 followers regular users. ', 'Another ~45% have up to 5000 followers. These users are likely not professional tweeter but their influence is much greater than regular users’ since their tweets or retweets will be seen by more than 500, possibly even thousands of people. Let’s call them opinion leaders.', 'Apparently, about 5% of Twitter users have more than 5000 followers. In many cases these Twitter accounts belong to business like magazines, cinemas and movie studios.\xa0They are professionals. We will explore which of them\xa0', ' regular users, opinion leaders or professionals', ' make the greatest buzz about a movie.', 'Now it’s time to do the last task in the scope of the data understanding phase: drafting the solution. We will build the simplest but still effective model. By “effective,” I mean the model should work significantly better than random guessing. In other words the percentage of correctly classified tweets with the suggested solution ought to be at least 10% higher than random guessing.', 'Here is a list of the basic principles we will follow while building the solution:', 'In order to train and later apply the models to the Twitter data it is first important to clean and prepare the data. This is the “dirty work” that occupies large swaths of data scientists’ time. In our specific case, models accept binary matrices indicating whether or not a word was present in the text as an input. So we have to transform every tweet into a vector of binary values. This operation is called “feature extraction.” This is accomplished in a number of steps.\xa0', 'First the text is extracted (assume we got a text filtered by the keyword “Doctor Strange”):', 'Then several normalization steps are performed\xa0', ' removing capitalization, punctuation, keywords used for tweet filtering, and unicode emoticons. Ideally we should preserve emoticons by replacing them with simple “synonyms” like “:)” or “:(“.', 'Leveraging synonyms, hyponyms, and hypernyms is a much more comprehensive natural language processing technique than we’re using here; indeed, this topic deserves its own dedicated article, so we’re consciously omitting\xa0it here for the sake of simplification.\xa0', 'Now the text looks like this:', 'Next the text is tokenized', ' split into tokens. Pay attention at the “cool stuff” substring. While performing tokenization we should recall that some dictionaries contain n-grams like “cool stuff” or “does not work.” The tokenization process should not lose those tokens. The described cleansing and tokenization may be accomplished by many means, and the most\xa0comprehensive solution deserves a separate article. We do it here in a very straightforward way: unnecessary symbols are removed with a regular expression, the text is separated spaces considering exceptions (like “cool stuff”).', 'Now we are almost ready to search for word matches in texts and dictionary entries. The last processing step we are going to perform is stemming. It’s a reduction of various forms of a word to their so-called stem. In fact, this transformation is a type of dimensionality reduction that solves two challenges: It increases matching rate and reduces the computational complexity of the machine learning algorithm. Stemming has to be performed on both the text and the dictionary. After this step the text from the tweet looks like this:', 'Once the words are stemmed, it is possible to search for dictionary words within the text. \xa0', 'So summarizing the simplified process of data preparation:', 'Now we have all data necessary and properly prepared to perform the modeling or, in other words, build a classifier that will tell whether a tweet is negative or positive. How to do that\xa0', ' that is, how to evaluate classification performance and determine which approach appears to be better\xa0', 'is what we will discuss in the next blog post.'], 'date': '\r\n\t\t\t\t\t\t\tNov 18, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Selecting, training, evaluating, and tuning the model', 'article url': 'https://blog.griddynamics.com/selecting-training-evaluating-and-tuning-the-model-for-twitter-stream-sentiment-analysis-of-social-movie-review/', 'first 160': [' we have discussed the steps needed to understand and prepare the data for Social Movie Reviews. Finally, it is time to run the models and learn how to extract meanings hidden in the data. This blog post deals with the modeling step in the Data Scientist’s Kitchen.', 'At the modeling phase we should select a particular model (or several of them), design the experiment, build the model(s), and assess them. First, however, let’s remind ourselves that our approach is to use as simple a model as possible that can still be effective. Model performance assessment will be based on the corpus of ', ' from Niek Sanders, and visualized with an ', '. We will build two models: \xa0Naive Plus/Minus and Logistic regression. The feature extraction process will use the root word dictionary. So...', 'The simplest model possible is a lexicon-based “Naive Plus/Minus.” For this model a text is shown as a point on a plane. (See below).', 'The positive and negative scores are two coordinates. They are calculated as the sum of individual scores for positive and negative words matched to dictionary entries. If the point is above the dotted line the overall text is considered positive. Otherwise, it’s considered as containing a negative sentiment. (See the red arrow.) The separating-line (the dotted one) may have different incline angle resulting in different classification results. This approach is based on the “naive” assumption that an entire text sentiment may be concluded from individual words in the tweet bearing negative and positive emotional loads. That’s why the approach is called “Naive Plus/Minus.” Continuing with our example from the “Data preparation” section in ', '\xa0in this series, we have a tweet transformed in a set of words from the dictionary.', 'Now we need to transform it into a vector of scores for every word taken from the dictionary.', 'The simplest way to understand where the point with coordinates (1Neg, 5Pos) is to sum the coordinates. In our case the final score for the tweet “Doctor Strange is truly an amazing piece of movie.. fighting the devil.. cool stuff.” is 4. The zero threshold corresponds to the 450 separating line. Given that the threshold is 0, we classify the tweet as positive. For another threshold, say 5, classification would be the opposite. We ran our ', ', ', ' from Niek Sanders, through the classifier. On the diagram below an ROC curve is shown for the Naive Plus/Minus model with the changing threshold', 'On the curve you can notice numbers: 1 (the most left), 0 and -1. It positions of corresponding threshold values. The colorful scale on the right shows distribution of the threshold values across the curve.', 'Another diagram showing the model performance is precision vs. recall:', ' is a binary statistical model used to predict a probability of one event based on several independent observable variables. The idea behind this is that there is a ', ' that looks pretty much like continuous probability distribution.', 'The logistic function formula is\xa0', '\xa0. In order to use the model to predict negative tweets, we should transform the tweet into a number so that a negative tweet would reflect a big positive value on the graph (so the predicted probability of it “being a negative” event would be close to 1), while a positive tweet would reflect in a big negative value (predicted probability of “being a negative” event would be close to 0).\xa0', 'Although we could use all words that we have in our ', ' dataset, that would give us a very high dimensional task. To deal with it, we would need to reduce dimensionality by identifying and excluding unrelated words like prepositions, pronouns, articles, sentiment-neutral nouns, verbs, and adjectives. This is the work of building a dictionary. So to maintain focus on our main goal, we decided to reuse an existing dictionary, the one with root words.', 'To go with Logistic Regression we should transform our tweet into a vector of binary variables reflecting presence/absence of every word from the dictionary in the tweet. A short sample of the full vector is shown below:', 'Having transformed all texts from the ', ' dataset, we get the binary matrix and can train the model. The training will result in a set of coefficients corresponding to every word from the dictionary. The final training dataset consists of 25000 vectors of 1268 words. In turn the test dataset, ', ' from Niek Sanders, is run through the logistic model.', 'To decide which model performs better, and what to do next, let’s first have a look at the ROC curve and Precision vs Recall diagrams for both models.', 'Does point A or B correspond to better performance? To read the above diagram properly and to understand the meaning of a \xa00.1 change in TPR or FPR, we need to recall that the test dataset has skewed classes. There are 572 negative and 2852 positive samples. So an increase of 0.1 in TPR means 57 more correctly identified negative tweets while an increase of 0.1 in FPR means 285 misclassified positive tweets. The difference between points A and B is equal to random guessing -- that is clearly confirmed by the A-B line, which is almost perfectly parallel to the diagonal line. That means the Naive Plus/Minus model demonstrated better performance than Logistic regression. And there is another angle to evaluate the model quality, the ', 'The scale for both axes is equal on this diagram. Here we can clearly see that the Logistic regression outperforms Naive Plus/Minus in some areas, however this area corresponds to a pretty low Recall level --less than 40%. Points A and B on the diagram correspond to the respective points on the ROC diagram. So, the Precision vs Recall diagram clearly confirms that point A corresponds to the better model. In terms of correctly classified negative and misclassified positive tweets, it looks like in the ', ' below.', 'So our research for a sentiment analysis model ended up with rather surprising result: the ultra-simple Naive Plus/Minus model outperforms the more comprehensive Logistic regression model while using the same dictionary. That means the coefficients in the dictionary created by ', ' are more efficient comparing to “weights” or “estimates” by the Machine Learning algorithm based on positive and negative movie reviews -- in our case from IMDB. This is surprising, as the dictionary author states in the paper,', '\xa0that the words were scored manually based on a subjective perception.', 'Is there a room for improvement in the model? Definitely. Please recall that we heavily simplified the feature extraction process. This is definitely the first place for improvement. We could address synonym challenges, ', ', typo correction, and many other aspects of our samples. We could build our own dictionary. Do you remember that we mentioned three dictionaries in the beginning? But in the end we are speaking about just one. Why? Of course, we are curious, so we quickly tried ', ' Both models performed much worse with this dictionary. See for yourself; check the ROC curve for logistic regression with both dictionaries.', 'As you can see, the dictionary with 4K words didn’t bring any value comparing to 1.5K dictionary. That might mean the performance of our model is rather limited with the training dataset we used. It pretty well corresponds with the fact that the manually-scored dictionary works better. So one additional way to improve the model’s performance is finding a better training set, e.g. by employing the ', ' project, as was mentioned by Finn Arup. On the way of feature extraction we also could try to extract more information from the text e.g. perform part-of-speech tagging as this data is used in ', '. Another dimension for possible improvements is trying very different models, for example, nonparametric regression like Random Forest or create other ensembles based on models we have already built. Also we could try non linear models like ', '\xa0with neural networks.', 'We now have a model that can classify tweets as positive or negative, so we have everything we need to perform further analysis and visualise our insights. In our next post we’ll talk about how to do that.'], 'date': '\r\n\t\t\t\t\t\t\tNov 28, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Selecting, training, evaluating, and tuning the model', 'article url': 'https://blog.griddynamics.com/selecting-training-evaluating-and-tuning-the-model-for-twitter-stream-sentiment-analysis-of-social-movie-review/', 'first 160': [' we have discussed the steps needed to understand and prepare the data for Social Movie Reviews. Finally, it is time to run the models and learn how to extract meanings hidden in the data. This blog post deals with the modeling step in the Data Scientist’s Kitchen.', 'At the modeling phase we should select a particular model (or several of them), design the experiment, build the model(s), and assess them. First, however, let’s remind ourselves that our approach is to use as simple a model as possible that can still be effective. Model performance assessment will be based on the corpus of ', ' from Niek Sanders, and visualized with an ', '. We will build two models: \xa0Naive Plus/Minus and Logistic regression. The feature extraction process will use the root word dictionary. So...', 'The simplest model possible is a lexicon-based “Naive Plus/Minus.” For this model a text is shown as a point on a plane. (See below).', 'The positive and negative scores are two coordinates. They are calculated as the sum of individual scores for positive and negative words matched to dictionary entries. If the point is above the dotted line the overall text is considered positive. Otherwise, it’s considered as containing a negative sentiment. (See the red arrow.) The separating-line (the dotted one) may have different incline angle resulting in different classification results. This approach is based on the “naive” assumption that an entire text sentiment may be concluded from individual words in the tweet bearing negative and positive emotional loads. That’s why the approach is called “Naive Plus/Minus.” Continuing with our example from the “Data preparation” section in ', '\xa0in this series, we have a tweet transformed in a set of words from the dictionary.', 'Now we need to transform it into a vector of scores for every word taken from the dictionary.', 'The simplest way to understand where the point with coordinates (1Neg, 5Pos) is to sum the coordinates. In our case the final score for the tweet “Doctor Strange is truly an amazing piece of movie.. fighting the devil.. cool stuff.” is 4. The zero threshold corresponds to the 450 separating line. Given that the threshold is 0, we classify the tweet as positive. For another threshold, say 5, classification would be the opposite. We ran our ', ', ', ' from Niek Sanders, through the classifier. On the diagram below an ROC curve is shown for the Naive Plus/Minus model with the changing threshold', 'On the curve you can notice numbers: 1 (the most left), 0 and -1. It positions of corresponding threshold values. The colorful scale on the right shows distribution of the threshold values across the curve.', 'Another diagram showing the model performance is precision vs. recall:', ' is a binary statistical model used to predict a probability of one event based on several independent observable variables. The idea behind this is that there is a ', ' that looks pretty much like continuous probability distribution.', 'The logistic function formula is\xa0', '\xa0. In order to use the model to predict negative tweets, we should transform the tweet into a number so that a negative tweet would reflect a big positive value on the graph (so the predicted probability of it “being a negative” event would be close to 1), while a positive tweet would reflect in a big negative value (predicted probability of “being a negative” event would be close to 0).\xa0', 'Although we could use all words that we have in our ', ' dataset, that would give us a very high dimensional task. To deal with it, we would need to reduce dimensionality by identifying and excluding unrelated words like prepositions, pronouns, articles, sentiment-neutral nouns, verbs, and adjectives. This is the work of building a dictionary. So to maintain focus on our main goal, we decided to reuse an existing dictionary, the one with root words.', 'To go with Logistic Regression we should transform our tweet into a vector of binary variables reflecting presence/absence of every word from the dictionary in the tweet. A short sample of the full vector is shown below:', 'Having transformed all texts from the ', ' dataset, we get the binary matrix and can train the model. The training will result in a set of coefficients corresponding to every word from the dictionary. The final training dataset consists of 25000 vectors of 1268 words. In turn the test dataset, ', ' from Niek Sanders, is run through the logistic model.', 'To decide which model performs better, and what to do next, let’s first have a look at the ROC curve and Precision vs Recall diagrams for both models.', 'Does point A or B correspond to better performance? To read the above diagram properly and to understand the meaning of a \xa00.1 change in TPR or FPR, we need to recall that the test dataset has skewed classes. There are 572 negative and 2852 positive samples. So an increase of 0.1 in TPR means 57 more correctly identified negative tweets while an increase of 0.1 in FPR means 285 misclassified positive tweets. The difference between points A and B is equal to random guessing -- that is clearly confirmed by the A-B line, which is almost perfectly parallel to the diagonal line. That means the Naive Plus/Minus model demonstrated better performance than Logistic regression. And there is another angle to evaluate the model quality, the ', 'The scale for both axes is equal on this diagram. Here we can clearly see that the Logistic regression outperforms Naive Plus/Minus in some areas, however this area corresponds to a pretty low Recall level --less than 40%. Points A and B on the diagram correspond to the respective points on the ROC diagram. So, the Precision vs Recall diagram clearly confirms that point A corresponds to the better model. In terms of correctly classified negative and misclassified positive tweets, it looks like in the ', ' below.', 'So our research for a sentiment analysis model ended up with rather surprising result: the ultra-simple Naive Plus/Minus model outperforms the more comprehensive Logistic regression model while using the same dictionary. That means the coefficients in the dictionary created by ', ' are more efficient comparing to “weights” or “estimates” by the Machine Learning algorithm based on positive and negative movie reviews -- in our case from IMDB. This is surprising, as the dictionary author states in the paper,', '\xa0that the words were scored manually based on a subjective perception.', 'Is there a room for improvement in the model? Definitely. Please recall that we heavily simplified the feature extraction process. This is definitely the first place for improvement. We could address synonym challenges, ', ', typo correction, and many other aspects of our samples. We could build our own dictionary. Do you remember that we mentioned three dictionaries in the beginning? But in the end we are speaking about just one. Why? Of course, we are curious, so we quickly tried ', ' Both models performed much worse with this dictionary. See for yourself; check the ROC curve for logistic regression with both dictionaries.', 'As you can see, the dictionary with 4K words didn’t bring any value comparing to 1.5K dictionary. That might mean the performance of our model is rather limited with the training dataset we used. It pretty well corresponds with the fact that the manually-scored dictionary works better. So one additional way to improve the model’s performance is finding a better training set, e.g. by employing the ', ' project, as was mentioned by Finn Arup. On the way of feature extraction we also could try to extract more information from the text e.g. perform part-of-speech tagging as this data is used in ', '. Another dimension for possible improvements is trying very different models, for example, nonparametric regression like Random Forest or create other ensembles based on models we have already built. Also we could try non linear models like ', '\xa0with neural networks.', 'We now have a model that can classify tweets as positive or negative, so we have everything we need to perform further analysis and visualise our insights. In our next post we’ll talk about how to do that.'], 'date': '\r\n\t\t\t\t\t\t\tNov 28, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Understanding the structure of the data in Twitter streams for sentiment analysis applications', 'article url': 'https://blog.griddynamics.com/understanding-the-structure-of-the-data-in-twitter-streams-for-sentiment-analysis-applications/', 'first 160': ['Our goal in the earliest stage of the project is to understand as much as we can about the data: what data sources are available; how much of the data is being produced; how is it captured and transmitted, with what latencies and on what channels; how long it stays available; how secure is it; how accurate it is, and so on. In our case, we need the following types of data:', 'Once the data scientists build the basic understanding of the data, they may begin formulating the hypotheses on the insights that might be minable from the data and on approach they may use to gain these insights. ', 'The first task is to get the stream of tweets related to some specific movie. We will employ the filtering capability of the Twitter streaming API. Every tweet containing words similar to a movie name is considered as the movie-related. E.g. for the movie “Lights Out” both texts will match. Examples of the data we’ll be dealing with quickly reveal quality issues we’ll have to deal with:', 'A data received with every tweet looks like this, after being captured in a json format:', 'The tweet screenshot is\xa0', 'What potentially valuable data we can see here? ', 'What are potential issues and challenges with the data?', 'The most important data, naturally, is the field “text” containing the tweet itself. Also there is a location information in fields “location”, “coordinates”, “place”. The information reflecting the tweeter’s social power like “followers_count” also could be interesting. Let’s have a quick overview of followers distribution based on a data sample of about 220K tweets collected during July 22-27, 2016 for several movies.', 'Quantiles of the “followers number”', 'An ', ' test says the “followers” number is statistically different for different movies.', 'Analysis of Variance Table', 'Response: followers_count', ' Df Sum Sq Mean Sq F value Pr(>F) ', 'movie 4 1.2078e+12 3.0195e+11 10.464 1.789e-08 ***', 'Residuals 219302 6.3280e+15 2.8855e+10 ', '---', "Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1", 'The next important aspect of data understanding is amount of data. Working with Twitter Streaming API we get several dozens tweets per second for a stream filtered for one movie name.\xa0', 'Once our data science team looked at enough data samples, they could summarize the initial findings:', 'That all gives us insight into what kind of data we have and stimulates our thinking on hypotheses and directions for further data exploration. It also gives us the necessary ground to proceed with selection of the right dictionary, which is the subject of the next blog post.\xa0'], 'date': '\r\n\t\t\t\t\t\t\tNov 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Understanding the structure of the data in Twitter streams for sentiment analysis applications', 'article url': 'https://blog.griddynamics.com/understanding-the-structure-of-the-data-in-twitter-streams-for-sentiment-analysis-applications/', 'first 160': ['Our goal in the earliest stage of the project is to understand as much as we can about the data: what data sources are available; how much of the data is being produced; how is it captured and transmitted, with what latencies and on what channels; how long it stays available; how secure is it; how accurate it is, and so on. In our case, we need the following types of data:', 'Once the data scientists build the basic understanding of the data, they may begin formulating the hypotheses on the insights that might be minable from the data and on approach they may use to gain these insights. ', 'The first task is to get the stream of tweets related to some specific movie. We will employ the filtering capability of the Twitter streaming API. Every tweet containing words similar to a movie name is considered as the movie-related. E.g. for the movie “Lights Out” both texts will match. Examples of the data we’ll be dealing with quickly reveal quality issues we’ll have to deal with:', 'A data received with every tweet looks like this, after being captured in a json format:', 'The tweet screenshot is\xa0', 'What potentially valuable data we can see here? ', 'What are potential issues and challenges with the data?', 'The most important data, naturally, is the field “text” containing the tweet itself. Also there is a location information in fields “location”, “coordinates”, “place”. The information reflecting the tweeter’s social power like “followers_count” also could be interesting. Let’s have a quick overview of followers distribution based on a data sample of about 220K tweets collected during July 22-27, 2016 for several movies.', 'Quantiles of the “followers number”', 'An ', ' test says the “followers” number is statistically different for different movies.', 'Analysis of Variance Table', 'Response: followers_count', ' Df Sum Sq Mean Sq F value Pr(>F) ', 'movie 4 1.2078e+12 3.0195e+11 10.464 1.789e-08 ***', 'Residuals 219302 6.3280e+15 2.8855e+10 ', '---', "Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1", 'The next important aspect of data understanding is amount of data. Working with Twitter Streaming API we get several dozens tweets per second for a stream filtered for one movie name.\xa0', 'Once our data science team looked at enough data samples, they could summarize the initial findings:', 'That all gives us insight into what kind of data we have and stimulates our thinking on hypotheses and directions for further data exploration. It also gives us the necessary ground to proceed with selection of the right dictionary, which is the subject of the next blog post.\xa0'], 'date': '\r\n\t\t\t\t\t\t\tNov 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Visualizing insights with an analytics dashboard', 'article url': 'https://blog.griddynamics.com/visualizing-the-insights-with-analytics-dashboard/', 'first 160': ['In the previous post we discussed which models we tried for sentiment classification and which one has demonstrated the best performance. In this post, we’ll show you how to visualize our under-the-hood findings so that others can see the results of our analysis. You can see our twitter sentiment analysis insights with our demo application ', '.', 'Let’s briefly recall what we know about the tweets after the sentiment classification is performed. We know the keyword (movie name), the tweet text, the date and exact time, the sentiment (positive or negative), and the number of followers the user who posted the tweet has.', 'We know that the number of followers can be between zero and hundreds of thousands, and that 50% of Twitter users have 300-600 followers. These are regular users like you or me. There are another ~45% who have up to 5000 followers. These users are likely not professional tweeters, but their influence is much greater than most peoples since their posts or repost will be seen by many hundreds or even thousands of people. Let’s call them opinion leaders. And apparently about 5% of Twitter users have more than 5000 followers. In many cases these Twitter accounts belong to businesses like magazines, cinemas, and other movie industry companies. They are professionals. Who among them — regular users, opinion makers or professionals\xa0', 'make the greatest buzz around a movie? Let’s make this all visible.', 'Imagine it’s 5pm on a \xa0Friday. What are people tweeting about the Star Wars movie that will be released this weekend? How many tweets in total, how many of them are buzz from, regular users, and how many are part of promo campaigns “warming people up” and “pushing them to spontaneous decisions?” A real-time chart showing a cumulative number of tweets split by sentiment and user category will show us how the situation is developing.', 'This kind of diagram gives a clear picture, so we want to use it to see long-term trends for the same movie. It might include several days or several months. So we need a chart showing historical data that gives us the ability to choose a range of dates.', 'Will all that give us a really clear picture? It definitely won’t. Our sentiment classification is far from perfect, so it’s good to see the real tweet stream to understand what people are actually tweeting about. The next useful capability is to see negative or positive tweets representing historical data. Having all that in front of you, you get a comprehensive picture of what is happening now, and what happened hours or days or a month ago, as you can see from the picture below.', 'This informative application can stil be improved. Cumulative diagrams are good to show the total number of tweets received in a given period of time, but they are not very clear in showing cycles and patterns in the number of tweets. Stacked-type diagrams are good for showing the relative contribution of every user type, but they don’t make it easy to perceive absolute values. So the next step could be to check different visualization options to show more “contrasts” in different data aspects for the same movie in the same range of dates.', 'We can easily recognize cyclic patterns with the “stacked bars” diagram and also see the contribution from regular users, opinion leaders, and professionals with the “bubble diagram.” Now we can say we have a pretty comprehensive picture of the sentiments around one selected movie. Our ultimate goal, of course, is to see if we can visually recognize patterns specific for a particular movie. That can be possible if we see the data for several movies simultaneously. So let’s select two movies and a particular type of diagram to check if there is a significant difference in contribution of various classes of Twitter users in the actual information atmosphere around those movies.', 'Now we can clearly see that the structure of the public sentiments around the movies is very different. In the picture above, we can see that the activity level of professional Twitter accounts might be significantly higher for one movie than for the other. The diagram shows that the activity of professional users toward “Suicide Squad,” which was released in August, looks very different from activity for the just-released “Inferno.” That might mean a significant difference in social media promotion strategies for these two movies. And that small finding may become the basis for further, deeper investigation into both products’ successes.', 'With this blog post we close the series of Data Scientist Kitchen blog posts, but we will keep developing new approaches and techniques for data analysis using open source projects. Please subscribe to our blog to keep up to date on the newest posts.'], 'date': '\r\n\t\t\t\t\t\t\tDec 02, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Visualizing insights with an analytics dashboard', 'article url': 'https://blog.griddynamics.com/visualizing-the-insights-with-analytics-dashboard/', 'first 160': ['In the previous post we discussed which models we tried for sentiment classification and which one has demonstrated the best performance. In this post, we’ll show you how to visualize our under-the-hood findings so that others can see the results of our analysis. You can see our twitter sentiment analysis insights with our demo application ', '.', 'Let’s briefly recall what we know about the tweets after the sentiment classification is performed. We know the keyword (movie name), the tweet text, the date and exact time, the sentiment (positive or negative), and the number of followers the user who posted the tweet has.', 'We know that the number of followers can be between zero and hundreds of thousands, and that 50% of Twitter users have 300-600 followers. These are regular users like you or me. There are another ~45% who have up to 5000 followers. These users are likely not professional tweeters, but their influence is much greater than most peoples since their posts or repost will be seen by many hundreds or even thousands of people. Let’s call them opinion leaders. And apparently about 5% of Twitter users have more than 5000 followers. In many cases these Twitter accounts belong to businesses like magazines, cinemas, and other movie industry companies. They are professionals. Who among them — regular users, opinion makers or professionals\xa0', 'make the greatest buzz around a movie? Let’s make this all visible.', 'Imagine it’s 5pm on a \xa0Friday. What are people tweeting about the Star Wars movie that will be released this weekend? How many tweets in total, how many of them are buzz from, regular users, and how many are part of promo campaigns “warming people up” and “pushing them to spontaneous decisions?” A real-time chart showing a cumulative number of tweets split by sentiment and user category will show us how the situation is developing.', 'This kind of diagram gives a clear picture, so we want to use it to see long-term trends for the same movie. It might include several days or several months. So we need a chart showing historical data that gives us the ability to choose a range of dates.', 'Will all that give us a really clear picture? It definitely won’t. Our sentiment classification is far from perfect, so it’s good to see the real tweet stream to understand what people are actually tweeting about. The next useful capability is to see negative or positive tweets representing historical data. Having all that in front of you, you get a comprehensive picture of what is happening now, and what happened hours or days or a month ago, as you can see from the picture below.', 'This informative application can stil be improved. Cumulative diagrams are good to show the total number of tweets received in a given period of time, but they are not very clear in showing cycles and patterns in the number of tweets. Stacked-type diagrams are good for showing the relative contribution of every user type, but they don’t make it easy to perceive absolute values. So the next step could be to check different visualization options to show more “contrasts” in different data aspects for the same movie in the same range of dates.', 'We can easily recognize cyclic patterns with the “stacked bars” diagram and also see the contribution from regular users, opinion leaders, and professionals with the “bubble diagram.” Now we can say we have a pretty comprehensive picture of the sentiments around one selected movie. Our ultimate goal, of course, is to see if we can visually recognize patterns specific for a particular movie. That can be possible if we see the data for several movies simultaneously. So let’s select two movies and a particular type of diagram to check if there is a significant difference in contribution of various classes of Twitter users in the actual information atmosphere around those movies.', 'Now we can clearly see that the structure of the public sentiments around the movies is very different. In the picture above, we can see that the activity level of professional Twitter accounts might be significantly higher for one movie than for the other. The diagram shows that the activity of professional users toward “Suicide Squad,” which was released in August, looks very different from activity for the just-released “Inferno.” That might mean a significant difference in social media promotion strategies for these two movies. And that small finding may become the basis for further, deeper investigation into both products’ successes.', 'With this blog post we close the series of Data Scientist Kitchen blog posts, but we will keep developing new approaches and techniques for data analysis using open source projects. Please subscribe to our blog to keep up to date on the newest posts.'], 'date': '\r\n\t\t\t\t\t\t\tDec 02, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Constructing a data dictionary for Twitter stream sentiment analysis of social movie reviews', 'article url': 'https://blog.griddynamics.com/constructing-a-data-dictionary-for-twitter-stream-sentiment-analysis-of-social-movie-reviews/', 'first 160': ['In the previous post we discussed the structure of the tweet data. In this post we’ll address the process of selecting or building the right data dictionary for our purpose.', 'What constitutes a good dictionary?', 'A crucial data set for any kind of text mining is a dictionary. As for sentiment analysis there are ', '. Both leverage dictionaries. The lexicon-based approach is where the ultimate score is calculated based on a per-word score from the dictionary and machine learning approach, where dictionaries are used to reduce data dimensionality. A good dictionary for our purpose contains words bearing a strong positive or negative connotation, but does not contain neutral words. It may include two- or three-word phrases to improve handling negations and other common \xa0cases, like “can’t stand.” Ideally, the dictionary should come from the same topic and writing style that will be analysed. \xa0(A lexicon for short texts like tweets written by teenagers will be quite different from a lexicon for diplomatic messages.) So, the best dictionary for our case would be based on tweets about movies, and if we want to get sophisticated we may need it will need to go beyond words and short phrases and include the role of a word in a sentence. For that level of model we would need a dictionary that would include features such as the part of speech for each word. But in our simple Social Movie Reviews demo, we will stick to simple lexicons that only require words. And, of course, we want them to be open source. Constructing a dictionary suitable for Twitter stream sentiment analysis of movie reviews\xa0', 'There are many open source sentiment analysis projects but most of them are based on just a few dictionaries:', 'Because of the large number (and credibility) of projects relying on these dictionaries, we consider them solid and suitable for the purpose of the concept demonstration. So let’s take a closer look at these dictionaries to understand their peculiarities.', 'The root word dictionary contains about 1506 entries that looks like this (selected examples):', 'Actually, as you can see from the selected example, the “root word dictionary” contains not root words but ', ' in most cases. However, in some cases it contains full forms of the words (for example, “battles” and “battle”). Another interesting observation is that, in some cases, reducing a word to its root may result in losing the original sentiment; \xa0“affected” is scored as -1, for example, while “affection” is scored as +3. The dictionary author tried to address at least some challenges related to variability in spelling (example: glamorous vs glamourous). And last but not least, the dictionary contains ', ' like “cool stuff” that directly impact how we will process the data at the data preparation phase.', 'The ', ' dictionary contains 8222 entries that look like this (again, selected examples)', 'As you can see, this dictionary contains additional information like a “part of speech” label for every entry. That is useful, especially together with additional Natural Language Processing (NLP) techniques such as ', '\xa0tagging. But if we go with a simple technique that works at a ', ' granularity level, information like which part of speech a word \xa0is will be ignored. As a result, the dictionary will have far fewer entries than 8222, since many of them will be considered duplicates. Also, we can see from the sample above that the dictionary suggests three-level polarity: positive, negative and neutral. In case we are going to classify tweets in negative/positive space we should think how to deal with “neutral” entries in the dictionary. Having very granular separation of the word-form from its sentiment has a side effect; if the form of a word in the text doesn’t match the dictionary entry, we can’t use that word for the analysis.', 'Jeffrey Breen’s dictionary consists of two files: positive words (2006 entries) and negative words (4783 entries) without any other attributes or labels. Small example from the”positive” file.', 'Again, the dictionary contains exact word forms and if we perform text normalization using ', ' techniques, the actual number of entries in the dictionary will be much less due to duplication of stems.', 'Having closely reviewed the dictionaries mentioned above, we will use the root word dictionary as our primary choice, with Jeffrey Breen’s dictionary as an alternative. The root word dictionary is based on microblogs and very likely represents the specific lexicon we need for Social Movie Reviews. As one sign of its suitability, it’s the only dictionary that contains emojis. It also provides strong lexicon coverage (1500 entries) and this is the only dictionary available to us that reflects the strength of a word’s sentiment on an eight-level scale (-4 to+4).', 'Jeffrey Breen’s dictionary is a strong second choice. After stemming, it contains about 4000 entries. That means wide lexicon coverage, so it gives us a wide selection of words that reflect sentiments. But it only has two sentiment levels: positive and negative.', 'We will leave the MPQA lexicon alone for the moment. It has a more complex structure and more features than the others, which might be useful for a more sophisticated model that considers sentence syntax. However, after simplification via stemming, removing duplications because of part of speech ID and identical stems, it has about 4400 entries, and\xa0that is similar for Jeffrey Breen’s dictionary.\xa0', 'In contrast to MPQA, Jeffrey Breen’s dictionary is built for opinion mining in social media, which is why we chose it over MPQA for Social Movie Reviews,\xa0but is still our second choice. The root word dictionary is number one. ', 'Therefore, we will use the root word dictionary for both the lexicon-based model and the machine learning one. The lexicon-based model (the simplest possible) based on the root word dictionary will be our baseline. Additionally we will create a machine learning model with Jeffrey Breen’s dictionary. Then we’ll compare the two models’ performance to each other and the baseline. Then we will use the best performing model in the deployment phase — with that, we’ll save further discussion of training and test data sets for the next blog post.'], 'date': '\r\n\t\t\t\t\t\t\tNov 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Constructing a data dictionary for Twitter stream sentiment analysis of social movie reviews', 'article url': 'https://blog.griddynamics.com/constructing-a-data-dictionary-for-twitter-stream-sentiment-analysis-of-social-movie-reviews/', 'first 160': ['In the previous post we discussed the structure of the tweet data. In this post we’ll address the process of selecting or building the right data dictionary for our purpose.', 'What constitutes a good dictionary?', 'A crucial data set for any kind of text mining is a dictionary. As for sentiment analysis there are ', '. Both leverage dictionaries. The lexicon-based approach is where the ultimate score is calculated based on a per-word score from the dictionary and machine learning approach, where dictionaries are used to reduce data dimensionality. A good dictionary for our purpose contains words bearing a strong positive or negative connotation, but does not contain neutral words. It may include two- or three-word phrases to improve handling negations and other common \xa0cases, like “can’t stand.” Ideally, the dictionary should come from the same topic and writing style that will be analysed. \xa0(A lexicon for short texts like tweets written by teenagers will be quite different from a lexicon for diplomatic messages.) So, the best dictionary for our case would be based on tweets about movies, and if we want to get sophisticated we may need it will need to go beyond words and short phrases and include the role of a word in a sentence. For that level of model we would need a dictionary that would include features such as the part of speech for each word. But in our simple Social Movie Reviews demo, we will stick to simple lexicons that only require words. And, of course, we want them to be open source. Constructing a dictionary suitable for Twitter stream sentiment analysis of movie reviews\xa0', 'There are many open source sentiment analysis projects but most of them are based on just a few dictionaries:', 'Because of the large number (and credibility) of projects relying on these dictionaries, we consider them solid and suitable for the purpose of the concept demonstration. So let’s take a closer look at these dictionaries to understand their peculiarities.', 'The root word dictionary contains about 1506 entries that looks like this (selected examples):', 'Actually, as you can see from the selected example, the “root word dictionary” contains not root words but ', ' in most cases. However, in some cases it contains full forms of the words (for example, “battles” and “battle”). Another interesting observation is that, in some cases, reducing a word to its root may result in losing the original sentiment; \xa0“affected” is scored as -1, for example, while “affection” is scored as +3. The dictionary author tried to address at least some challenges related to variability in spelling (example: glamorous vs glamourous). And last but not least, the dictionary contains ', ' like “cool stuff” that directly impact how we will process the data at the data preparation phase.', 'The ', ' dictionary contains 8222 entries that look like this (again, selected examples)', 'As you can see, this dictionary contains additional information like a “part of speech” label for every entry. That is useful, especially together with additional Natural Language Processing (NLP) techniques such as ', '\xa0tagging. But if we go with a simple technique that works at a ', ' granularity level, information like which part of speech a word \xa0is will be ignored. As a result, the dictionary will have far fewer entries than 8222, since many of them will be considered duplicates. Also, we can see from the sample above that the dictionary suggests three-level polarity: positive, negative and neutral. In case we are going to classify tweets in negative/positive space we should think how to deal with “neutral” entries in the dictionary. Having very granular separation of the word-form from its sentiment has a side effect; if the form of a word in the text doesn’t match the dictionary entry, we can’t use that word for the analysis.', 'Jeffrey Breen’s dictionary consists of two files: positive words (2006 entries) and negative words (4783 entries) without any other attributes or labels. Small example from the”positive” file.', 'Again, the dictionary contains exact word forms and if we perform text normalization using ', ' techniques, the actual number of entries in the dictionary will be much less due to duplication of stems.', 'Having closely reviewed the dictionaries mentioned above, we will use the root word dictionary as our primary choice, with Jeffrey Breen’s dictionary as an alternative. The root word dictionary is based on microblogs and very likely represents the specific lexicon we need for Social Movie Reviews. As one sign of its suitability, it’s the only dictionary that contains emojis. It also provides strong lexicon coverage (1500 entries) and this is the only dictionary available to us that reflects the strength of a word’s sentiment on an eight-level scale (-4 to+4).', 'Jeffrey Breen’s dictionary is a strong second choice. After stemming, it contains about 4000 entries. That means wide lexicon coverage, so it gives us a wide selection of words that reflect sentiments. But it only has two sentiment levels: positive and negative.', 'We will leave the MPQA lexicon alone for the moment. It has a more complex structure and more features than the others, which might be useful for a more sophisticated model that considers sentence syntax. However, after simplification via stemming, removing duplications because of part of speech ID and identical stems, it has about 4400 entries, and\xa0that is similar for Jeffrey Breen’s dictionary.\xa0', 'In contrast to MPQA, Jeffrey Breen’s dictionary is built for opinion mining in social media, which is why we chose it over MPQA for Social Movie Reviews,\xa0but is still our second choice. The root word dictionary is number one. ', 'Therefore, we will use the root word dictionary for both the lexicon-based model and the machine learning one. The lexicon-based model (the simplest possible) based on the root word dictionary will be our baseline. Additionally we will create a machine learning model with Jeffrey Breen’s dictionary. Then we’ll compare the two models’ performance to each other and the baseline. Then we will use the best performing model in the deployment phase — with that, we’ll save further discussion of training and test data sets for the next blog post.'], 'date': '\r\n\t\t\t\t\t\t\tNov 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Using CRISP-DM methodology for Twitter sentiment analysis', 'article url': 'https://blog.griddynamics.com/data-scientists-vocabulary-and-scientific-process-applied-to-social-movie-reviews/', 'first 160': ['As we explained in our introduction to this series of posts, we are exploring\xa0a data scientist’s methods of extracting hidden patterns and meanings from big data in order to make better applications, services, and business decisions. We will perform a simple ', ' of a ', ', and explain\xa0the data science process.', 'In this blog post, we discuss the general-purpose scientific process behind data science and how it was applied to our project.\xa0', 'A good place to start is a quick review of the glossary of basic terms from the data science domain we’ll be using throughout the project:', 'Specifically, in the context of our project, definitions of our terms:', 'Before the trendy phrase “data science” captured the imagination of the industry, there was a mature discipline —\xa0data mining — that concerned itself with similar questions. Since every data mining project followed the same patterns, good folks created a Cross Industry Standard Process for Data Mining or simply ', '. It includes six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment. The diagram below shows the sequence of phases and reflects the iterative nature of the design process:', 'This approach is simple and pragmatic — and useful\xa0in explaining the scientific process behind the work of data scientists. This is the process we use in our project as well. Below, we’ll discuss how our understanding of social sentiment analysis has progressed from business understanding to data understanding to data preparation to data modeling, evaluation of the models, and, ultimately, a scalable implementation of machine learning.\xa0', 'Specifically, this process is applied in the following way:', 'To formulate a precise business problem that will be solved by the predictive model, it is critical to understand data sources and the structure of the data itself, and formulate a set of hypothesis of what range of business business “questions” can be answered predictively. This is an iterative process — initial analysis of the data leads to an initial formulation of the business problem and a hypothesis of how that problem can be solved. As the project matures, more is known about the data and the model’s predictive powers, and a more refined understanding of the business applications of predictive modeling emerges.', 'In our case, the team will begin by looking at samples of tweets related to movies to formulate a set of assumptions about what kind of models can be used, and what kind of business questions these models can answer. For example, the decision to segment the population of Twitter users into “power groups” based on the number of followers so that the business can distinguish the sentiments of highly influential individuals (with over 5,000 followers) from non-influential ones (fewer than 500) came pretty late in the project and altered the business understanding, and thus the way the models were used.\xa0', 'Study samples of the data to answer critical questions about this data’s structure; sources; volumes; frequency; fields that are critical, promising or irrelevant; security; accessibility, and so on. In our case, this means initially studying the structure of Twitter streams to learn enough to begin building data dictionaries and training data sets. As the project progresses, understanding of the data grows and so does the quality of the training data sets and the performance of the model.', 'For starters, we will assume that every tweet attributed to movie X can be measured on a numeric scale between positive and negative sentiment. The scale depends on the model employed. It will be either a probability (0;1) or a sentiment score (-∞;+∞). Our models will produce that measure for every tweet.', 'Data is never as “clean” and error-free as we would like it to be. Common offenses include', 'The goal of the data preparation stage is to identify data quality issues and write code that automatically detects data quality problems and corrects them', 'The process typically begins by choosing one or more models from a standard library of mathematical models that are deemed the most “promising” based on the hypotheses generated in the previous stages. Subsequently, systematic training and tuning leads to the gradual improvement of the models’ performance. This tends to be an iterative process. Based on the performance of the models, the team will likely need to go back and revisit the assumptions from the data understanding or even business understanding states. After some number of iterations, the models start producing increasingly more promising results until they eventually become good enough to be production-worthy.\xa0', 'Evaluation of the models’ performance and assessment of whether it is adequate to meet the business objectives. For example, a model that delivers 70% accuracy in predictions might be “good enough” for some applications but not for others. If the accuracy is deemed insufficient or there are still promising hypotheses on how the accuracy might be improved to deliver better business results, the team goes back to the data preparation / modeling stages. Once a model and its results have been validated and deemed acceptable by the business team, the data science team is ready to go further and productize the solution.', 'Now the job of the data scientists is complete and the software developers’ job begins. In order to implement the model at scale, it needs to change from a data scientist’s tool into production-ready code running on a scalable platform. Developers can then use technologies like Hadoop, Kafka, Spark Streaming, Cassandra, Tableau and others to capture data streams, run thousands of concurrent events through the model, persist all intermediate and final results to survive any failures, and ultimately deliver resulting insights to different business systems that will use them.', 'After the model is up and running, delivering the algorithmic discovery\xa0of business insights and application of those insights to marketing, sales and operations, the data science team continues to refine the model, identify more insights, and drive new business applications.\xa0', 'Armed with CRISP-DM methodology, we are ready to take you step by step through the process of creating predictive sentiment analytics system for real-time social movie\xa0reviews and attempt to tackle the following business problem:', 'Based on tweets from the \xa0English-speaking population of the United States related to selected new movie releases, can we identify patterns in the public’s sentiments towards these movies in real -time and track the progression of these sentiments over time?', 'Our quest begins with ', 'understanding part of our data science process.'], 'date': '\r\n\t\t\t\t\t\t\tNov 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Using CRISP-DM methodology for Twitter sentiment analysis', 'article url': 'https://blog.griddynamics.com/data-scientists-vocabulary-and-scientific-process-applied-to-social-movie-reviews/', 'first 160': ['As we explained in our introduction to this series of posts, we are exploring\xa0a data scientist’s methods of extracting hidden patterns and meanings from big data in order to make better applications, services, and business decisions. We will perform a simple ', ' of a ', ', and explain\xa0the data science process.', 'In this blog post, we discuss the general-purpose scientific process behind data science and how it was applied to our project.\xa0', 'A good place to start is a quick review of the glossary of basic terms from the data science domain we’ll be using throughout the project:', 'Specifically, in the context of our project, definitions of our terms:', 'Before the trendy phrase “data science” captured the imagination of the industry, there was a mature discipline —\xa0data mining — that concerned itself with similar questions. Since every data mining project followed the same patterns, good folks created a Cross Industry Standard Process for Data Mining or simply ', '. It includes six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment. The diagram below shows the sequence of phases and reflects the iterative nature of the design process:', 'This approach is simple and pragmatic — and useful\xa0in explaining the scientific process behind the work of data scientists. This is the process we use in our project as well. Below, we’ll discuss how our understanding of social sentiment analysis has progressed from business understanding to data understanding to data preparation to data modeling, evaluation of the models, and, ultimately, a scalable implementation of machine learning.\xa0', 'Specifically, this process is applied in the following way:', 'To formulate a precise business problem that will be solved by the predictive model, it is critical to understand data sources and the structure of the data itself, and formulate a set of hypothesis of what range of business business “questions” can be answered predictively. This is an iterative process — initial analysis of the data leads to an initial formulation of the business problem and a hypothesis of how that problem can be solved. As the project matures, more is known about the data and the model’s predictive powers, and a more refined understanding of the business applications of predictive modeling emerges.', 'In our case, the team will begin by looking at samples of tweets related to movies to formulate a set of assumptions about what kind of models can be used, and what kind of business questions these models can answer. For example, the decision to segment the population of Twitter users into “power groups” based on the number of followers so that the business can distinguish the sentiments of highly influential individuals (with over 5,000 followers) from non-influential ones (fewer than 500) came pretty late in the project and altered the business understanding, and thus the way the models were used.\xa0', 'Study samples of the data to answer critical questions about this data’s structure; sources; volumes; frequency; fields that are critical, promising or irrelevant; security; accessibility, and so on. In our case, this means initially studying the structure of Twitter streams to learn enough to begin building data dictionaries and training data sets. As the project progresses, understanding of the data grows and so does the quality of the training data sets and the performance of the model.', 'For starters, we will assume that every tweet attributed to movie X can be measured on a numeric scale between positive and negative sentiment. The scale depends on the model employed. It will be either a probability (0;1) or a sentiment score (-∞;+∞). Our models will produce that measure for every tweet.', 'Data is never as “clean” and error-free as we would like it to be. Common offenses include', 'The goal of the data preparation stage is to identify data quality issues and write code that automatically detects data quality problems and corrects them', 'The process typically begins by choosing one or more models from a standard library of mathematical models that are deemed the most “promising” based on the hypotheses generated in the previous stages. Subsequently, systematic training and tuning leads to the gradual improvement of the models’ performance. This tends to be an iterative process. Based on the performance of the models, the team will likely need to go back and revisit the assumptions from the data understanding or even business understanding states. After some number of iterations, the models start producing increasingly more promising results until they eventually become good enough to be production-worthy.\xa0', 'Evaluation of the models’ performance and assessment of whether it is adequate to meet the business objectives. For example, a model that delivers 70% accuracy in predictions might be “good enough” for some applications but not for others. If the accuracy is deemed insufficient or there are still promising hypotheses on how the accuracy might be improved to deliver better business results, the team goes back to the data preparation / modeling stages. Once a model and its results have been validated and deemed acceptable by the business team, the data science team is ready to go further and productize the solution.', 'Now the job of the data scientists is complete and the software developers’ job begins. In order to implement the model at scale, it needs to change from a data scientist’s tool into production-ready code running on a scalable platform. Developers can then use technologies like Hadoop, Kafka, Spark Streaming, Cassandra, Tableau and others to capture data streams, run thousands of concurrent events through the model, persist all intermediate and final results to survive any failures, and ultimately deliver resulting insights to different business systems that will use them.', 'After the model is up and running, delivering the algorithmic discovery\xa0of business insights and application of those insights to marketing, sales and operations, the data science team continues to refine the model, identify more insights, and drive new business applications.\xa0', 'Armed with CRISP-DM methodology, we are ready to take you step by step through the process of creating predictive sentiment analytics system for real-time social movie\xa0reviews and attempt to tackle the following business problem:', 'Based on tweets from the \xa0English-speaking population of the United States related to selected new movie releases, can we identify patterns in the public’s sentiments towards these movies in real -time and track the progression of these sentiments over time?', 'Our quest begins with ', 'understanding part of our data science process.'], 'date': '\r\n\t\t\t\t\t\t\tNov 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'The basics of data science with a sentiment analysis example', 'article url': 'https://blog.griddynamics.com/lets-use-twitter-stream-sentiment-analysis-of-popular-movies-to-teach-the-rudiments-of-data-science/', 'first 160': ['There is a broad and fast-growing interest in data science and machine learning. It is fueled by an explosion in business applications that rely on automated detection of patterns and behaviors hidden in the data, that can be found by software and exploited to dramatically improve the way we market and sell products, optimize our inventory and supply chain, and detect fraud and support customers. \xa0In short, data science and machine learning improve how we make decisions in a wide range of situations based on patterns found in data.', 'For decades, mathematical modeling in business belonged to an obscure area at the intersection of business and IT. Now it is moving into the mainstream and the rush is on: Where do we find data scientists, how do we train them, and what tools do we give them? \xa0Is there a way we can scale analytics and data science to the point where they become a normal aspect of any software development project?', 'This series of blog posts is addressed to software engineers and technology managers who want to understand, in simple terms, how data science is used to solve common challenges in machine learning.\xa0', 'In thinking about the best ways to expose a large number of programmers to the basics of data science and machine learning, we took the same approach that helped introduce ', ' to millions of developers: the ', ', a teaching-oriented demo application that is intuitive enough that any developer can relate to its business goals, complex enough to represent real-world requirements, and \xa0simple enough to keep the developer from being overwhelmed by complexities found in real-world business applications.', '“Social Movie Reviews” is what we’re calling our “Pet Clinic for data science and machine learning,” and here is how we are going to use it to expose you to the world of data science:', 'This blog series is also a logical companion to our series of blog posts on ', ', which is a popular approach to building a computational platform for performing mathematical analysis and machine learning. We use our ', ' to provide a computational platform used in this tutorial on data sciences.', '\xa0'], 'date': '\r\n\t\t\t\t\t\t\tNov 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'The basics of data science with a sentiment analysis example', 'article url': 'https://blog.griddynamics.com/lets-use-twitter-stream-sentiment-analysis-of-popular-movies-to-teach-the-rudiments-of-data-science/', 'first 160': ['There is a broad and fast-growing interest in data science and machine learning. It is fueled by an explosion in business applications that rely on automated detection of patterns and behaviors hidden in the data, that can be found by software and exploited to dramatically improve the way we market and sell products, optimize our inventory and supply chain, and detect fraud and support customers. \xa0In short, data science and machine learning improve how we make decisions in a wide range of situations based on patterns found in data.', 'For decades, mathematical modeling in business belonged to an obscure area at the intersection of business and IT. Now it is moving into the mainstream and the rush is on: Where do we find data scientists, how do we train them, and what tools do we give them? \xa0Is there a way we can scale analytics and data science to the point where they become a normal aspect of any software development project?', 'This series of blog posts is addressed to software engineers and technology managers who want to understand, in simple terms, how data science is used to solve common challenges in machine learning.\xa0', 'In thinking about the best ways to expose a large number of programmers to the basics of data science and machine learning, we took the same approach that helped introduce ', ' to millions of developers: the ', ', a teaching-oriented demo application that is intuitive enough that any developer can relate to its business goals, complex enough to represent real-world requirements, and \xa0simple enough to keep the developer from being overwhelmed by complexities found in real-world business applications.', '“Social Movie Reviews” is what we’re calling our “Pet Clinic for data science and machine learning,” and here is how we are going to use it to expose you to the world of data science:', 'This blog series is also a logical companion to our series of blog posts on ', ', which is a popular approach to building a computational platform for performing mathematical analysis and machine learning. We use our ', ' to provide a computational platform used in this tutorial on data sciences.', '\xa0'], 'date': '\r\n\t\t\t\t\t\t\tNov 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tJoseph Gorelik', 'author_url': 'https://blog.griddynamics.com/author/joseph-gorelik/', 'tag': ['Data Science', 'Data science toolkit', 'Sentiment analysis of tweets']}, {'title': 'Expanding product discovery with ML-powered image similarity search', 'article url': 'https://blog.griddynamics.com/expanding-product-search-with-image-similarity/', 'first 160': ['With an online catalog of over 1M SKUs and a multi-billion dollar e-commerce store, things were going well for the online retailer. However, their search engine’s relevancy was not ideal, hurting their shoppers’ ability to discover the right products and lowering conversion rates.', 'The core problem was imperfect product attribution of catalog data. The attributes were coming from the e-commerce system in a form of text fields that contained a sizable number of errors or omissions. This time, the product management team decided to try something new - use product images as a data source to extract the attributes from the visual product features, such as color, style, height, shape, and more.', 'At the same exact time, a different idea took shape - perhaps it was also possible to use visual similarity between product images to offer customers an entirely new and exciting way to discover products similar to those they “like”. Upon further examination, it turned out that the underlying technology is nearly identical in both cases. The product management team brought in Grid Dynamics, as they wanted our ML experience to help them build an entirely new type of search engine, based on visual product similarity.', 'In this blog post, we’ll share how such an engine can be designed and trained to address visual similarity search and automated image-based catalog attribution using computer vision and machine learning.', 'Not all shoppers know exactly what they want when they shop. Many of them have a rough idea of what they are looking for, but are also interested in comparing items. In many domains, such as apparel or decoration, product look is a major factor in making a purchasing decision. Therefore, an accurate way of determining visual similarity is of great value to online retailers.', 'For product discovery, the main feature involves the ability to search for a similar-looking item in a catalog given a sample or an “anchor” item:', 'Here, you can see how the model can pick up the main characteristics of the sample dress and find similar-looking short striped dresses from the catalog. This feature can also be termed “visual recommendation” or “show me more like this”.', 'Once you have determined the visual similarity of your products, you can also unlock several other powerful features, such as “sort by similar”. This allows the customer to sort the category or the search result not only by common sorting criteria such as price or rating, but also by similarity to a particular “anchor” product, which can be explicitly selected by the customer with a “like” button on the UI:', 'As seen above, the category can then be re-sorted by visual similarity to the anchor product, with respect to applied keywords and filters. In a more sophisticated case, the “anchor” product can be implicitly derived from the customer behaviour, such as shopping cart content, session view history, add-to-bag, or order history.', 'Another interesting feature is that similarity can be sorted against multiple anchor products at once, essentially finding matches that have features of both items:', 'You can play with a sandbox demo ', ' that features a catalog of dresses, toys, laptops, and TVs.', "Now that we’re clear on the main use cases, let's take a look under the hood and see how it all works.", "As much as we like to think that our computers can “think” and “understand” us, in reality, that's not how they work. They are just exceptionally good at remembering things and crunching numbers very, very quickly. For a computer, a picture of a dress is just a set of neatly arranged bytes. Needless to say, searching for other images with similar bytes is very unlikely to yield similar-looking images.", 'Instead, we need to somehow convert those images into a different numerical representation that captures the essence of the image’s content and style. This conversion, known as vectorization, should work in such a way that similar-looking images will be mapped to similar vectors. Once images are converted into vectors, a wealth of algorithms are then generated to search for other similar vectors, thus finding similar images.', 'This approach to similarity search is known as a ', ' and is traditionally applied to text-based similarity. However, nothing prevents us from applying the same principles to other objects, as long as we can find a proper vectorization model.', 'The vector space model approach is simple and powerful but can be slow if applied one at a time to many single images. Applied directly to millions of products in a catalog, it can take a long time to search through all of them to find similarities to an anchor image. The main reason for this is ', '. All nearest neighbor search algorithms eventually need to calculate distances between the vectors, which becomes slower as vector dimensionality grows. As any meaningful representation of an image requires hundreds or even thousands of dimensions, the vector space model approach needs to be combined with other external processes to perform well in most practical use cases.', 'One common technique to speed up the nearest neighbor search is by using dimensionality reduction. Algorithms like ', ' can reduce the dimensionality of data while minimizing the loss of information from dropped dimensions. This makes it possible to reduce vector sizes from thousands to hundreds without a noticeable loss in search quality.', 'Another approach to speed up the process is to use a traditional search engine to get results before filtering. Using an ', ', a search engine can very quickly find good candidate matches for similar products, e.g. dresses. If the anchor image is a dress, this prevents the visual similarity search needing to be performed on each of the millions of products in the catalog. Instead, it can only be run on the products highlighted by the search engine results. This approach of combining a high-quality, but slower, similarity ranking with a low quality, but faster, similarity search is called search results re-ranking.', 'At a high level, the solution architecture is as follows:', 'The most important part of the visual similarity feature is the vectorization model. Since being introduced to the community in 2012, Convolutional Neural Networks (CNN) have taken the visual recognition industry by storm, continually pushing the envelope on tasks of image classification and object detection. There are a lot of different CNN architectures available, created by a host of different research groups. Description of CNN internals is beyond the scope of this post, but there are a lot of excellent ', '. This ', ' gives a good introduction to the internals of CNN.', 'The ', ' shows a nice comparison of popular CNN architectures.', 'For our visual similarity project, we wanted to use Convolutional Neural Networks to extract latent features of images, e.g. features that are very important for capturing image meanings. These can be used to compare images, yet are impossible to map to well-known “human” categories like length, color, or shape.', 'CNNs internally do exactly that - in the training process they “learn” which features are important for the task at hand, encoding this information into trained network weights. All we need to do is to forward propagate an image through the trained network and take neuron activations on some hidden layer as a vector encoding the image. The exact choice of the hidden layer or hidden layers to extract features from depends on the application.', 'The deeper we go in the hierarchy of layers from the input image, the more information about the content of the image we obtain, e.g. our vectors encode shapes and their arrangements. Shallow layers can be used to obtain encoding of style of the image, e.g. texture, palette, and other spatially distributed features.', 'The simplest way to get started with extracting image features is ', ', e.g. to use pre-trained network weights that are publically available. Those weights are typically obtained by training CNN towards image classification goals on a large public training dataset such as ', '. If your images are very different from the images available on imagenet, or you are a lucky owner of a large and well-attributed image catalog, then you will be able to re-train the model from scratch in order to obtain network weights finely tuned for your kind of images. Still, it is always a good idea to start with pre-trained weights to have a solid performance baseline to compare to.', 'In order to make an informed choice of the vector embedding approach, we used a simple search quality assessment method based on gathering a “golden set” of anchor images and quality assessments about how similar some other images are in the catalog to the anchor image. Once such a dataset is available, it is easy to score every vectorization implementation using a standard search quality scoring technique like ', '.', 'For the purposes of apparel search, the shape and arrangement of the image is more important, thus we chose the last fully connected “bottleneck” layer as a source of image vectors. For other applications, such as a fine art search, more style-biased embeddings are generally used.', 'We evaluated a number of possible embeddings and found that Inception V3 performed the best for our case. Even though Inception V4 is a more recent model, it didn’t perform as well as V3. One plausible explanation is that V3 produces larger embeddings, and is able to capture more latent features.', 'Once we optimized the vectorization process and obtained vectors for every image in the catalog, the next step was to select a similarity search algorithm. In other words, given a vector, we should be able to find K “closest” vectors in a multidimensional vector space. To define exactly what “closest vectors” means, we have to decide on a formula to calculate a distance between the vectors. There are many different ways to define vector distance, called ', '. One of the most well-known metrics is Euclidean distance, with other popular metrics being Cosine and Manhattan metrics, which correspondingly measure angle and per-coordinate distances between vectors. We used Euclidean distance as a baseline.', 'Once the vector distance is defined, we can use a brute force algorithm to calculate vector distance between the anchor vector and each of the other vectors in the catalog. We can quickly obtain the top K closest neighbors in an almost linear time from the size of the catalog, but the difficulty of calculation of the distance itself, growing proportionally to the dimensionality of the data, makes the search far more complex.', 'There are a number of algorithms (kd-tree and ball-tree) that try to address this problem by cleverly partitioning multi-dimensional space and presenting it as a tree structure at indexing time, thus avoiding the necessity to calculate distances to all of the vectors in a catalog at query time. The advantage of those algorithms is that they still produce exact results, e.g. functionally identical to brute force approach. However, thr efficiency of such data structures quickly diminishes with the number of dimensions in the image vectors, so for very large embeddings, they become a little more than fancy wrappers over brute force vector space search.', 'We ended up using ', ', which makes a tradeoff by obtaining an approximate nearest neighbors result, e.g. it can occasionally miss some actual nearest neighbors, sacrificing recall. Still, the performance gain is tremendous, and it was an acceptable trade-off for our application. You can always evaluate the level of mistakes created by approximate nearest neighbor search by comparing it with brute force results to keep algorithm recall under control.', 'As we can see, CNN-based vectorisation empowers us to tackle visual similarity of products. But why stop there? We can still look back at the product attributes and textual product information, and see if we can vectorize this data as well. Attribute data, being mostly categorical, can be vectorized using one-hot encoding, e.g. all possible values of an attribute are listed as a vector dimension and marked as 1 if a given product has such an attribute value.', 'For textual fields, we can use semantic encoding available from ', ' or ', ' models. Again, transfer learning can be used to get word embeddings obtained from training those networks on really large corpora, such as Wikipedia. Attribute vectors may play an important role in product comparisons and in situations when visual similarity does not mean much. An example of this is in electronics.', 'Resulting vectors can be used separately or concatenated to obtain a compound vector embedding of the product based on all data available about a product. This should allow similarity search to tap into all features of the image: hidden features from product images, a textual description, and explicit product attributes.', 'Product images and product data encoding powered by deep learning models unlocks many new customer facing features - including both the “more like this” search or sort by visually similar feature. Product vectors encoding visual features can boost product recommendations and be used to automatically ', '. All this makes image recognition capability an indispensable part of the e-commerce enterprise ecosystem.', 'If you are one of the many e-commerce companies with a large product catalog and are engaged in a never-ending fight for better product attribution, or if you are looking to give your customers new and exciting ways to discover your products, give us a call - we would be happy to help!'], 'date': '\r\n\t\t\t\t\t\t\tMay 30, 2018\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tTimofey Durakov', 'author_url': 'https://blog.griddynamics.com/author/timofey-durakov/', 'tag': ['Machine Learning and Artificial Intelligence', 'Search', 'Visual Search']}, {'title': 'From reference architecture to reference implementation: detailing the DevOps aspects of In-Stream Processing Service', 'article url': 'https://blog.griddynamics.com/from-reference-architecture-to-reference-implementation-detailing-the-devops-aspects-of-in-stream-processing-service/', 'first 160': ['In the previous four blog posts in this series we covered the reference architecture of a general purpose In-Stream Processing Service blueprint. To recap, here is a list of shortcuts to the blogs in that series:', 'In the next few posts we’ll present our ', ' of that blueprint, and open source all of its components so that anyone can deploy and run the entire service platform on AWS (Amazon Web Services) within a few hours by using our deployment and orchestration scripts.\xa0', 'This is the “DevOps” part of the story — making the platform operational on the dynamic cloud infrastructure for development, testing and production purposes. The main topics will concern scalability, availability, portability and automation of the platform’s deployment and operations on any public cloud.\xa0', 'We even developed a ', ' for real-time sentient analysis of twitter feeds for Social Movie Reviews that runs on our reference implementation out of the box. You can play with the interactive web application that lets you visualize public’s historic and real-time sentiments towards the latest movies, powered by our In-Stream Processing service here. We also wrote a series of blogs that explain the scientific process behind the work of the data scientists, shows every step in the process of developing the sentiment analytics application from the data scientist point of view, and illustrates how the machine learning models were trained, evaluated and tuned to perform the analytics. The series of blogs is collectively called “Data Science Kitchen: a hands-on primer on how data scientists create machine learning models, using Twitter stream sentiment analysis of social movie reviews as our teaching example.” ', ', which we strongly advise you to read\xa0— along with those that will come after it.', 'Now let’s jump into the details of the reference implementation, starting from a discussion of the technology stack used to automate the deployment and operational management.'], 'date': '\r\n\t\t\t\t\t\t\tNov 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'Deploying In-Stream Processing']}, {'title': 'From reference architecture to reference implementation: detailing the DevOps aspects of In-Stream Processing Service', 'article url': 'https://blog.griddynamics.com/from-reference-architecture-to-reference-implementation-detailing-the-devops-aspects-of-in-stream-processing-service/', 'first 160': ['In the previous four blog posts in this series we covered the reference architecture of a general purpose In-Stream Processing Service blueprint. To recap, here is a list of shortcuts to the blogs in that series:', 'In the next few posts we’ll present our ', ' of that blueprint, and open source all of its components so that anyone can deploy and run the entire service platform on AWS (Amazon Web Services) within a few hours by using our deployment and orchestration scripts.\xa0', 'This is the “DevOps” part of the story — making the platform operational on the dynamic cloud infrastructure for development, testing and production purposes. The main topics will concern scalability, availability, portability and automation of the platform’s deployment and operations on any public cloud.\xa0', 'We even developed a ', ' for real-time sentient analysis of twitter feeds for Social Movie Reviews that runs on our reference implementation out of the box. You can play with the interactive web application that lets you visualize public’s historic and real-time sentiments towards the latest movies, powered by our In-Stream Processing service here. We also wrote a series of blogs that explain the scientific process behind the work of the data scientists, shows every step in the process of developing the sentiment analytics application from the data scientist point of view, and illustrates how the machine learning models were trained, evaluated and tuned to perform the analytics. The series of blogs is collectively called “Data Science Kitchen: a hands-on primer on how data scientists create machine learning models, using Twitter stream sentiment analysis of social movie reviews as our teaching example.” ', ', which we strongly advise you to read\xa0— along with those that will come after it.', 'Now let’s jump into the details of the reference implementation, starting from a discussion of the technology stack used to automate the deployment and operational management.'], 'date': '\r\n\t\t\t\t\t\t\tNov 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAnton Ovchinnikov', 'author_url': 'https://blog.griddynamics.com/author/anton-ovchinnikov/', 'tag': ['Big Data', 'Deploying In-Stream Processing']}, {'title': 'How to build a DIY Magic Mirror out of commodity hardware and augmented reality APIs', 'article url': 'https://blog.griddynamics.com/build-diy-magic-mirror-out-of-commodity-hardware-and-ar-tech/', 'first 160': ['As augmented reality technology continues to advance, it opens up possibilities for a dazzling array of practical and innovative applications. One great example is a “Magic Mirror”. A Magic Mirror is a reflective TV screen linked to a camera and controlled by augmented reality software. The TV screen can behave like a regular mirror, but it can also enhance the reflection by showing video feeds from different angles, adding accessories like earrings purses or hats, applying makeup or changing the color of outfits.', 'See a demo of this technology below.', 'With their features, limited only by the imagination of marketing teams, Magic Mirrors are expected to replace traditional mirrors in dressing rooms in retail stores. They will help draw customers in and enrich the shopping experience, leading to increased engagement, loyalty and sales.', 'Retailers interested in a Magic Mirror system, can choose to purchase one out-of-the-box from a number of suppliers, or build their own using commodity components and developing a custom augmented reality software. In this blog post we’ll share our experience building a “Magic Mirror kit” out of commodity hardware components and open AR frameworks that is cheap, fast and can be enhanced to fit every retailer’s specific needs, as an alternative to purchasing a closed vendor solution.', 'With growing interest in Magic Mirrors, there are many companies that offer out-of-the-box Magic Mirror solutions, like ', '. Using these products can get expensive quickly. With price of $10K - $15K per mirror, a retailer with 1,000 stores and 2-4 dressing rooms per store, may be looking at a $2M - $4M price tag.', 'At the same time, the cost of the hardware is only around $2K - $3K depending on the configuration, or 20% of the total price. The remaining cost of the solution is in the software. So, the key question the customers should be asking is this: what will it take to build the custom software in-house relative to the overall cost of the out-of-the-box product?', 'It turns out that you can build a magic mirror relatively inexpensively from scratch. Using a simple mobile application and commercially available hardware, we were able to produce a simple magic mirror for a fraction of the cost in very little time using a small mobile team.', 'Before we go into more detail about how we developed this solution for the customer, let’s first look further into the features and some examples of magic mirrors available on the market.', 'The following gives a breakdown of the more common features and uses for magic mirrors. First we will review the basic features and then list some of the more advanced features possible using augmented reality (AR) technology.', 'A magic mirror can essentially be as small as a tablet. In this case, it is a camera, a mirror, and a computer in a single device. Tablet-based magic mirrors are already being used for makeup and hairstyling in hairdressing salons where they use augmented reality technology to for example apply virtual makeup to the customer.', 'Of course, any tablet can also be used at home. It’s also possible to cast the screen of a tablet or a smartphone to home TV with Chromecast or Apple TV appliance.  Additionally, there are some smart TVs, which could receive a video stream from mobile devices without any hardware in between.', '\n', '  ', 'There are also larger magic mirrors that utilize a semi-transparent two-way film. These appear identical to a traditional glass mirror until they are powered on. Once active, these mirrors have a “smart area”, which takes up a portion of the mirror’s surface and can be used to display a range of different visual elements such as text, video, or other types of multimedia.', '\n', ' ', 'In public areas such as a retailer outlet or shopping mall, magic mirrors can be much larger (in the range of 65” - 75”). These are intended for public shared access rather than personal use so generally do not allow the public any direct control over them. Because of this, they usually do not include any form of user interface. In some cases, they may recognize gestures or allow limited user control via their personal smartphone or mobile device.', '\n', '\n', 'With this understanding of the Magic Mirror market, let’s now take a good look at how to build the Magic Mirror in-house with commodity components. The analysis below is performed for the “public mirror”, although the same approach works for all other types.', 'The solution is based on three major components: (1) a TV with a camera, on a stand or inside a frame, that acts as a “mirror”; (2) a smartphone with a built-in camera that acts as a computing platform, as well as the “eye”, and sometimes can take user input to configure or control the mirror; and (3) connectivity between a TV and a smartphone, including the network and connectivity software like Chromecast or Apple TV.', 'The complete solution can be based on either Android or Apple platform technologies. We’ll start with Android stack and later explain how the same architecture can be adopted to iOS stack.', 'Now we need to integrate these pieces together, starting from a basic ability to get the video feed from the camera, manipulate the image and project it back to the TV screen. That’s done quite simply by building an app based on pure ', ' without any third-party libraries. We combine ', ' API for building modular applications with ', ' (API to display a content stream, along with ', ' from the ', ' package to be able to manipulate the video feed from the camera with a fine-grain control of image capture and post-processing at high frame rates.', "To seamlessly link the Android device and the monitor, we use a Google Chromecast adapter, which is the cheapest and simplest way to stream smartphone screen content to any TV/monitor with an HDMI input. The Chromecast dongle is plugged into the HDMI port and powered by a separate USB cable. If the monitor has a USB port, the dongle's power cable can be plugged directly into it.", 'To bridge the smartphone and the monitor using Chromecast:', 'For more detailed instructions, refer to this Google guide:', 'As we want to rotate the monitor to portrait mode, there are some changes that need to be made as the default settings for monitors are set to the landscape orientation. To make sure the smartphone and monitor views are synchronized, we need to cheat the native Android orientation and make some changes in our application. To do this, we declare ', ' orientation as ', ' but leave the phone in portrait orientation. ', 'Now the monitor works as if it were a mirror, i.e. the monitor renders the real view seen by the smartphone camera. Now to add a little more “magic” to the mirror. By recording several seconds of mirroring and setting the camera to use a delay, the person can turn around and then be able to view themselves. This allows them to turn a full 360 degrees and then wait a set number of seconds to be able to see themselves from any angle displayed on the screen.', 'The ', ' component is what we need to record the camera view as it allows us to save live video to external storage. It is a very simple process to replay recorded video as ', ' is a standard Android component. Then the only other component that we need is ', '.', 'Volla! We have a basic Magic Mirror that allows the consumer to walk up to it and spin around for 8 seconds, and then see the recording of these 8 seconds along with a real-time view with full 360-degree view. This “360-Mirror” is a complete system, extremely cost-effective and fully ready for installation at the retail store. It will take a small team of mobile developers a few days of work to put together.', 'The process using the iOS platform is very similar to the Android process outlined above.', 'All that needs to be done is to replace the Android hardware and software components with iOS ones. So this involves using an iPhone rather than an Android smartphone and switching the casting device from Chromecast to Apple TV.', 'From this point on, you have a platform that can be enhanced in various ways to add “augmented reality jazz” and features such as change in color of a piece of clothing worn, add an accessory, apply virtual makeup and change scenery to the beach or a nighttime party. We will not get into the details on such implementations in this blog. Suffice to say that the sky’s the limit and many dazzling features can be added over time using mobile development and specialized augmented reality APIs.', 'The main argument for building Magic Mirror in-house boils down to two factors:', 'Some retailers with small store footprint and lack on qualified development resources would undoubtedly be better off buying a commercial offering. At the same time, a large number of tier-1 retailers may be well-justified in building their own Magic Mirror along the lines described in this post.'], 'date': '\r\n\t\t\t\t\t\t\tFeb 22, 2018\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Sumtsov', 'author_url': 'https://blog.griddynamics.com/author/dmitry-sumtsov/', 'tag': ['Mobile']}, {'title': 'In-Stream Processing service blueprint', 'article url': 'https://blog.griddynamics.com/in-stream-processing-service-blueprint/', 'first 160': ['This Blueprint aims to address a number of architectural goals we’ve heard repeatedly from our customers, who are CTOs and Chief Architects of Big Data systems:', 'In the current version of the Blueprint, we pre-integrated a number of technologies (', ') that, together, make up an In-Stream Processing stack:', 'The Blueprint also addresses a number of operational concerns:', 'We start by describing a distributed, fault-tolerant computing system architecture that is highly available with respect to a failure of any node or functional component of the Blueprint. The diagram below shows the minimum cluster configuration that can be scaled up to meet actual business demands:', "If any single node fails, the system in the diagram above will still be fully functional. The diagram below shows which nodes could fail simultaneously without a negative impact on our In-Stream Processing service's functionality. There won’t be any performance degradation if the virtual machines are sized correctly to handle the complete workload in an emergency situation.", 'One point worth noting\xa0is that the absence of a single point of failure makes the system tolerant of many —\xa0but not all\xa0', '\xa0kinds of failures. For example, systems distributed across several data centers indirectly include communication subsystems that are not under the direct control of a Data Center or a Platform as a Service (PaaS) provider. The number of possible deployment configurations is so large that no single recipe can be universal enough to address all non-specific fault tolerance problems. That said, the presented architecture is able to ensure 99.999% reliability in most cases.', 'Another important aspect of reliability is the delivery semantic. In the strictest case every event must be processed by the streaming service once, and only once. An end-to-end ability to process all the events exactly once requires:', 'The idempotency requirement means the downstream system has to tolerate data duplication. See the diagram below:', 'Data transmission reliability to (communication point #1) and from our In-Stream Processing Service (communication point #3) requires certain behaviors from components external to the Blueprint. Specifically, the source and downstream systems should deliver and consume events in a synchronous way.', 'Apache Kafka and Spark Streaming ensure that data will be processed without losses once it is accepted for processing. Simply speaking, failures inside the Spark Streaming Application are automatically handled by re-processing the recent data. It is achieved by Apache Kafka’s capability to keep data for a configurable period of time up to dozens of days (communication point #2).', 'Getting an optimal configuration for Apache Kafka requires several important design decisions and careful trade offs\xa0between architectural concerns such as durability and latency, ease of scaling, and cost of overhead.', 'First, let’s start with durability concerns. Apache Kafka is run as a cluster of brokers. One cluster can contain many logical channels called queues or topics, each split into multiple partitions. Each partition is replicated across brokers according to a replication factor configured for durability. As a result, each broker is responsible for a number of assigned partitions and replicas. Coordination of re-assignments in case of failures is provided with ', '.', 'Data replication and persistence together deliver “pretty good” immunity from data loss. But as with all distributed systems, there is a trade off\xa0between latency and the system’s level of durability. Apache Kafka provides many configuration options, starting with asynchronous delivery of events from producers in batches and ending with synchronous publishing of every event blocking until the event is committed, i.e. when all in sync replicas for the partition have applied it to their logs.', 'Asynchronous delivery clearly will be the fastest. The producer will receive an immediate acknowledgment\xa0when data reaches the Kafka broker without waiting for replication and persistence. If something happens to the Kafka broker at that moment, the data will be lost. The other extreme is full persistence. The producer will receive an acknowledgment\xa0only after the submitted events are successfully replicated and physically stored. If something happens during these operations, the producer will re-submit the events. This will be the safest configuration option from a data loss standpoint, but it is the slowest mode.', 'The next important aspect of Apache Kafka configuration is choosing the optimal number of partitions. Since repartitioning the message queue is a manual process, it is highly disruptive to service operations because it requires downtime. Ideally, we shouldn’t need to do this more frequently than once a year; however, to maintain stable partitioning for this long requires solid forecasting skills.', 'In a nutshell, getting partitioning right is a combination of art and science directed at finding a balance between ease of scaling and the overhead cost of over-partitioning. More partitions bring higher overhead and impact memory requirements, ZooKeeper performance, and partition transitioning time during failover. On the other hand, since a partition is a unit of parallelization, the number of partitions should be sufficient to efficiently parallelize event handling by the message queue. Since a single Kafka partition must be consumed by a single sourcing container in a Spark Streaming application, the number of partitions also heavily affects overall throughput of event consumption by the sourcing containers. The advantages and drawbacks of over-partitioning are described in ', ' from the Apache Kafka designers.', 'The illustration above shows how, for efficient topic partitioning, we should estimate the number of partitions. It should be the minimum number sufficient to achieve:', 'The process of finding an optimal parallelization level for a Spark Streaming applications is, again, a mixture of art and science. It is an iterative process of application tuning with many forks and dependencies that is highly dependent on the nature of the application. Sadly, no “one-size-fits-all” methodology has emerged that can substitute for hands-on experience.', 'Finally, we will provide a few thoughts on hardware configuration for Kafka. Since Kafka is a highly-optimized message queue system designed to efficiently parallelize computing and I/O operations, it’s crucial to parallelize storage I/O operations since they are the slowest part of the system, especially with random data access. An important Apache Kafka advantage is its ability to keep the raw events stream for several days. For example, in case of 300K events per second, 150 bytes per event, the data size for seven days will be about 24 TB. Kafka provides per-event compression which can cut storage requirements, but compression impacts performance — and, in any case, the actual compression ratio depends on the events format and content.', 'ZooKeeper is a distributed coordination service used to achieve high availability for Apache Kafka partitions and brokers, as well as a resource manager for the Spark Streaming cluster. Apache ZooKeeper lets distributed systems follow services, which facilitates overall high availability: it provides guaranteed consistent storage for the state of a distributed system, client monitoring, and leader election.', 'Previous versions of the Spark API for Apache Kafka integration used ZooKeeper for managing consumer offsets, which almost always turned ZooKeeper into a performance bottleneck. Nowadays, with the introduction of a Direct Apache Kafka API in Spark, consumer offsets are best managed in checkpoints. In this case, one ZooKeeper ensemble can be shared between the Message Queue and the Stream Processing clusters with no compromise in performance or availability, while substantially reducing administration efforts and cloud resources.', 'A ZooKeeper ensemble consists of several, typically three to seven, identical nodes that each have dedicated high speed storage for transaction persistence. ZooKeeper works with small and simple data structures reflecting the cluster state. This data is very limited in size. Therefore RAM and consumed disk space are almost independent of the event flow rate and may be considered constant for all practical purposes.', 'ZooKeeper performance is sensitive to storage I/O performance. Considering that ZooKeeper is a crucial part of the Kafka ecosystem, it’s a good idea to run it on dedicated instances or at least to set up a dedicated storage mount point for its persistent data.', 'Here is a fault-tolerant Spark Streaming cluster design. It addresses three reliability aspects: zero data loss, no duplicates, and automatic failover for all component-level failures. The next diagram shows a five-component sub-system where each component must be highly available:', 'The next aspect of reliability is the assurance of zero data loss and no duplications. Typical message delivery semantics recognize three types of requirements:', 'In-Stream Processing applications are often required to support Exactly Once delivery semantics. While it is possible to achieve an end-to-end Exactly Once semantic for most In-Stream Processing applications, comprehensive discussion of this topic deserves a separate article. Here, we’ll briefly mention the main approaches, starting with the diagram below:', 'As previously mentioned, Kafka has the ability to replay the stream, which is important if there is a failure. A replay of recent events is automatically triggered in case of a failure inside the Stream Processing Application, whether it is caused by hardware or software. “Recent events” in this case means micro-batches that were emitted but not successfully processed. At Least Once delivery semantics are achieved by doing this. Exactly Once requires additional effort which either involves In-Stream events deduplication (i.e. looking up processed event identifiers from an external database) or by designing an idempotent method of insight consumption by downstream systems. The last option simply means the downstream system is tolerant of duplicates; i.e. two equal REST calls, one of which is duplicated, will not break consistency.', 'Cassandra is a massively scalable, highly available NoSQL database. A Cassandra cluster consists of peer nodes with no dedicated cluster manager function. That means any client may connect to any node and request any data, as shown on this diagram.', 'The minimum highly available Cassandra cluster consists of 3 (three) nodes, where replication factor is 3 (three), with 2 (two) read and write replicas. With fewer nodes, a node failure will block read or write operations if the database was configured to ensure data consistency.', 'Cassandra allows you to \xa0tune its durability and consistency levels by configuring the replication factor (RF) and the number of nodes to write (W) and read (R). In order for the data to be consistent, the following ', '.', 'Durability is achieved via data replication. The Virtual Node (VN) concept is essential for understanding how data replication works. As we have said, a regular Cassandra cluster consists of several homogenous nodes. The dataset is split into many segments, called Virtual Nodes. Each segment is replicated several times according to the configured replication factor, as seen in the diagram below:', 'Hardware failures, generally speaking, result in data inconsistency in replicas. Once a failed node comes back online, a ', ' is started that restores replica consistency. The repair process has automatic and manual options. If the failed node is considered permanently down, the administrator has to ', ' from the cluster.', 'Another aspect of durability is data persistence. Cassandra employs commit logs. Essentially, all writes to a commit log are cached like any other storage I/O operation. The data is truly persisted only after it is physically written to storage. Since such operations are time-consuming, the commit log is synced periodically — every so many milliseconds\xa0', '\xa0based on parameters specified in the configuration. Theoretically, the data not synced with physical storage might be lost due to problems such as a sudden power outage. In practice, data replication and power supply redundancy make this situation very unlikely.\xa0', 'Redis was originally written in 2009 as a simple, key-value, in-memory database. This made Redis very efficient for read operations with simple data structures. Since then, Redis has acquired many additional features including sharding, persistence, replication, and automatic failover. This has made it an ideal lookup datastore. Since lookup data volumes are usually rather small, we rarely see a necessity for data partitioning for scalability with Redis. The typical HA Redis deployment consists of at least two nodes. Availability is achieved with Master-Slave data replication. Redis includes an agent called Sentinel that monitors data nodes and performs the failover procedure when the Master node fails. Sentinel can be deployed in various ways; the ', ' information about the advantages and pitfalls of different approaches. We suggest deploying Sentinel to ZooKeeper nodes, as shown in the following diagram:', 'The number of running Sentinel instances must be greater than two since each Sentinel agent can start a failover procedure after coordination with a majority of agents.', 'Redis is very performant database and it’s very unlikely that the CPU will become a bottleneck, \xa0any more than memory will for the “lookup database.” However, if the CPU becomes a bottleneck, scalability is achieved via data sharding. Redis is a single-threaded application, so ', ' on the same VM. In sharding mode, Redis creates 16384 hash slots and distributes them between shards. Clients may still request data from any node. In case a request comes to a node that doesn’t have the requested data, it will be redirected to the proper node.', 'Durability is achieved with data persistence. Here, Redis provides two main options: a snapshot of all user data as an RDB file, or log all write operations in an AOF (Append-Only File). Both options have their own advantages and drawbacks, which are described in detail in the ', '. (In our experience we have found that the AOF method provides higher durability in exchange for slightly more maintenance effort).', 'Now that we have analyzed each component separately, it is time to display a single “big picture” diagram that shows the complete, end-to-end topology of a scalable, durable and fault-tolerant In-Stream Processing service design:', 'To summarize the capabilities of our In-Stream Processing service, these are the SLAs targeted by the design we have presented here:', 'Several important architectural and operational concerns are not addressed by this version of our Blueprint, such as:', 'The usual reasons for exclusion include:', 'If you have a question you’d like to ask, we’d be happy to share what we’ve learned. Please leave a question in the comments or ', '.'], 'date': '\r\n\t\t\t\t\t\t\tJul 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'In-Stream Processing service blueprint', 'article url': 'https://blog.griddynamics.com/in-stream-processing-service-blueprint/', 'first 160': ['This Blueprint aims to address a number of architectural goals we’ve heard repeatedly from our customers, who are CTOs and Chief Architects of Big Data systems:', 'In the current version of the Blueprint, we pre-integrated a number of technologies (', ') that, together, make up an In-Stream Processing stack:', 'The Blueprint also addresses a number of operational concerns:', 'We start by describing a distributed, fault-tolerant computing system architecture that is highly available with respect to a failure of any node or functional component of the Blueprint. The diagram below shows the minimum cluster configuration that can be scaled up to meet actual business demands:', "If any single node fails, the system in the diagram above will still be fully functional. The diagram below shows which nodes could fail simultaneously without a negative impact on our In-Stream Processing service's functionality. There won’t be any performance degradation if the virtual machines are sized correctly to handle the complete workload in an emergency situation.", 'One point worth noting\xa0is that the absence of a single point of failure makes the system tolerant of many —\xa0but not all\xa0', '\xa0kinds of failures. For example, systems distributed across several data centers indirectly include communication subsystems that are not under the direct control of a Data Center or a Platform as a Service (PaaS) provider. The number of possible deployment configurations is so large that no single recipe can be universal enough to address all non-specific fault tolerance problems. That said, the presented architecture is able to ensure 99.999% reliability in most cases.', 'Another important aspect of reliability is the delivery semantic. In the strictest case every event must be processed by the streaming service once, and only once. An end-to-end ability to process all the events exactly once requires:', 'The idempotency requirement means the downstream system has to tolerate data duplication. See the diagram below:', 'Data transmission reliability to (communication point #1) and from our In-Stream Processing Service (communication point #3) requires certain behaviors from components external to the Blueprint. Specifically, the source and downstream systems should deliver and consume events in a synchronous way.', 'Apache Kafka and Spark Streaming ensure that data will be processed without losses once it is accepted for processing. Simply speaking, failures inside the Spark Streaming Application are automatically handled by re-processing the recent data. It is achieved by Apache Kafka’s capability to keep data for a configurable period of time up to dozens of days (communication point #2).', 'Getting an optimal configuration for Apache Kafka requires several important design decisions and careful trade offs\xa0between architectural concerns such as durability and latency, ease of scaling, and cost of overhead.', 'First, let’s start with durability concerns. Apache Kafka is run as a cluster of brokers. One cluster can contain many logical channels called queues or topics, each split into multiple partitions. Each partition is replicated across brokers according to a replication factor configured for durability. As a result, each broker is responsible for a number of assigned partitions and replicas. Coordination of re-assignments in case of failures is provided with ', '.', 'Data replication and persistence together deliver “pretty good” immunity from data loss. But as with all distributed systems, there is a trade off\xa0between latency and the system’s level of durability. Apache Kafka provides many configuration options, starting with asynchronous delivery of events from producers in batches and ending with synchronous publishing of every event blocking until the event is committed, i.e. when all in sync replicas for the partition have applied it to their logs.', 'Asynchronous delivery clearly will be the fastest. The producer will receive an immediate acknowledgment\xa0when data reaches the Kafka broker without waiting for replication and persistence. If something happens to the Kafka broker at that moment, the data will be lost. The other extreme is full persistence. The producer will receive an acknowledgment\xa0only after the submitted events are successfully replicated and physically stored. If something happens during these operations, the producer will re-submit the events. This will be the safest configuration option from a data loss standpoint, but it is the slowest mode.', 'The next important aspect of Apache Kafka configuration is choosing the optimal number of partitions. Since repartitioning the message queue is a manual process, it is highly disruptive to service operations because it requires downtime. Ideally, we shouldn’t need to do this more frequently than once a year; however, to maintain stable partitioning for this long requires solid forecasting skills.', 'In a nutshell, getting partitioning right is a combination of art and science directed at finding a balance between ease of scaling and the overhead cost of over-partitioning. More partitions bring higher overhead and impact memory requirements, ZooKeeper performance, and partition transitioning time during failover. On the other hand, since a partition is a unit of parallelization, the number of partitions should be sufficient to efficiently parallelize event handling by the message queue. Since a single Kafka partition must be consumed by a single sourcing container in a Spark Streaming application, the number of partitions also heavily affects overall throughput of event consumption by the sourcing containers. The advantages and drawbacks of over-partitioning are described in ', ' from the Apache Kafka designers.', 'The illustration above shows how, for efficient topic partitioning, we should estimate the number of partitions. It should be the minimum number sufficient to achieve:', 'The process of finding an optimal parallelization level for a Spark Streaming applications is, again, a mixture of art and science. It is an iterative process of application tuning with many forks and dependencies that is highly dependent on the nature of the application. Sadly, no “one-size-fits-all” methodology has emerged that can substitute for hands-on experience.', 'Finally, we will provide a few thoughts on hardware configuration for Kafka. Since Kafka is a highly-optimized message queue system designed to efficiently parallelize computing and I/O operations, it’s crucial to parallelize storage I/O operations since they are the slowest part of the system, especially with random data access. An important Apache Kafka advantage is its ability to keep the raw events stream for several days. For example, in case of 300K events per second, 150 bytes per event, the data size for seven days will be about 24 TB. Kafka provides per-event compression which can cut storage requirements, but compression impacts performance — and, in any case, the actual compression ratio depends on the events format and content.', 'ZooKeeper is a distributed coordination service used to achieve high availability for Apache Kafka partitions and brokers, as well as a resource manager for the Spark Streaming cluster. Apache ZooKeeper lets distributed systems follow services, which facilitates overall high availability: it provides guaranteed consistent storage for the state of a distributed system, client monitoring, and leader election.', 'Previous versions of the Spark API for Apache Kafka integration used ZooKeeper for managing consumer offsets, which almost always turned ZooKeeper into a performance bottleneck. Nowadays, with the introduction of a Direct Apache Kafka API in Spark, consumer offsets are best managed in checkpoints. In this case, one ZooKeeper ensemble can be shared between the Message Queue and the Stream Processing clusters with no compromise in performance or availability, while substantially reducing administration efforts and cloud resources.', 'A ZooKeeper ensemble consists of several, typically three to seven, identical nodes that each have dedicated high speed storage for transaction persistence. ZooKeeper works with small and simple data structures reflecting the cluster state. This data is very limited in size. Therefore RAM and consumed disk space are almost independent of the event flow rate and may be considered constant for all practical purposes.', 'ZooKeeper performance is sensitive to storage I/O performance. Considering that ZooKeeper is a crucial part of the Kafka ecosystem, it’s a good idea to run it on dedicated instances or at least to set up a dedicated storage mount point for its persistent data.', 'Here is a fault-tolerant Spark Streaming cluster design. It addresses three reliability aspects: zero data loss, no duplicates, and automatic failover for all component-level failures. The next diagram shows a five-component sub-system where each component must be highly available:', 'The next aspect of reliability is the assurance of zero data loss and no duplications. Typical message delivery semantics recognize three types of requirements:', 'In-Stream Processing applications are often required to support Exactly Once delivery semantics. While it is possible to achieve an end-to-end Exactly Once semantic for most In-Stream Processing applications, comprehensive discussion of this topic deserves a separate article. Here, we’ll briefly mention the main approaches, starting with the diagram below:', 'As previously mentioned, Kafka has the ability to replay the stream, which is important if there is a failure. A replay of recent events is automatically triggered in case of a failure inside the Stream Processing Application, whether it is caused by hardware or software. “Recent events” in this case means micro-batches that were emitted but not successfully processed. At Least Once delivery semantics are achieved by doing this. Exactly Once requires additional effort which either involves In-Stream events deduplication (i.e. looking up processed event identifiers from an external database) or by designing an idempotent method of insight consumption by downstream systems. The last option simply means the downstream system is tolerant of duplicates; i.e. two equal REST calls, one of which is duplicated, will not break consistency.', 'Cassandra is a massively scalable, highly available NoSQL database. A Cassandra cluster consists of peer nodes with no dedicated cluster manager function. That means any client may connect to any node and request any data, as shown on this diagram.', 'The minimum highly available Cassandra cluster consists of 3 (three) nodes, where replication factor is 3 (three), with 2 (two) read and write replicas. With fewer nodes, a node failure will block read or write operations if the database was configured to ensure data consistency.', 'Cassandra allows you to \xa0tune its durability and consistency levels by configuring the replication factor (RF) and the number of nodes to write (W) and read (R). In order for the data to be consistent, the following ', '.', 'Durability is achieved via data replication. The Virtual Node (VN) concept is essential for understanding how data replication works. As we have said, a regular Cassandra cluster consists of several homogenous nodes. The dataset is split into many segments, called Virtual Nodes. Each segment is replicated several times according to the configured replication factor, as seen in the diagram below:', 'Hardware failures, generally speaking, result in data inconsistency in replicas. Once a failed node comes back online, a ', ' is started that restores replica consistency. The repair process has automatic and manual options. If the failed node is considered permanently down, the administrator has to ', ' from the cluster.', 'Another aspect of durability is data persistence. Cassandra employs commit logs. Essentially, all writes to a commit log are cached like any other storage I/O operation. The data is truly persisted only after it is physically written to storage. Since such operations are time-consuming, the commit log is synced periodically — every so many milliseconds\xa0', '\xa0based on parameters specified in the configuration. Theoretically, the data not synced with physical storage might be lost due to problems such as a sudden power outage. In practice, data replication and power supply redundancy make this situation very unlikely.\xa0', 'Redis was originally written in 2009 as a simple, key-value, in-memory database. This made Redis very efficient for read operations with simple data structures. Since then, Redis has acquired many additional features including sharding, persistence, replication, and automatic failover. This has made it an ideal lookup datastore. Since lookup data volumes are usually rather small, we rarely see a necessity for data partitioning for scalability with Redis. The typical HA Redis deployment consists of at least two nodes. Availability is achieved with Master-Slave data replication. Redis includes an agent called Sentinel that monitors data nodes and performs the failover procedure when the Master node fails. Sentinel can be deployed in various ways; the ', ' information about the advantages and pitfalls of different approaches. We suggest deploying Sentinel to ZooKeeper nodes, as shown in the following diagram:', 'The number of running Sentinel instances must be greater than two since each Sentinel agent can start a failover procedure after coordination with a majority of agents.', 'Redis is very performant database and it’s very unlikely that the CPU will become a bottleneck, \xa0any more than memory will for the “lookup database.” However, if the CPU becomes a bottleneck, scalability is achieved via data sharding. Redis is a single-threaded application, so ', ' on the same VM. In sharding mode, Redis creates 16384 hash slots and distributes them between shards. Clients may still request data from any node. In case a request comes to a node that doesn’t have the requested data, it will be redirected to the proper node.', 'Durability is achieved with data persistence. Here, Redis provides two main options: a snapshot of all user data as an RDB file, or log all write operations in an AOF (Append-Only File). Both options have their own advantages and drawbacks, which are described in detail in the ', '. (In our experience we have found that the AOF method provides higher durability in exchange for slightly more maintenance effort).', 'Now that we have analyzed each component separately, it is time to display a single “big picture” diagram that shows the complete, end-to-end topology of a scalable, durable and fault-tolerant In-Stream Processing service design:', 'To summarize the capabilities of our In-Stream Processing service, these are the SLAs targeted by the design we have presented here:', 'Several important architectural and operational concerns are not addressed by this version of our Blueprint, such as:', 'The usual reasons for exclusion include:', 'If you have a question you’d like to ask, we’d be happy to share what we’ve learned. Please leave a question in the comments or ', '.'], 'date': '\r\n\t\t\t\t\t\t\tJul 10, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAnton Ovchinnikov', 'author_url': 'https://blog.griddynamics.com/author/anton-ovchinnikov/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'How to replatform Endeca rules to Solr', 'article url': 'https://blog.griddynamics.com/how-to-replatform-endeca-rules-to-solr/', 'first 160': ['In the \xa0', ' we discussed the Endeca rules model and explained how to re-implement this model using Elasticsearch. We needed to implement ', ' to trigger our rules and we leveraged powerful ', ' feature in Elasticsearch which greatly simplified our implementation. In this blog post, we will discuss how to approach implementation of Endeca rules if you are running Solr.', 'Unfortunately, \xa0Solr currently does not have a percolator-like functionality. \xa0We believe it will be available soon because ', '. Meanwhile, we can employ an alternative approach to implement inverted search', 'based purely on Solr queries. \xa0We will use the same example we used in previous article for illustration.', 'Firstly, let’s recall the particular trigger types that we will have to implement:', ' the search phrase contains search terms sequentially in a strict order but may also contain other words before or after.', ' The rule is configured with search terms = “', '”. The search phrase “', '” will trigger this rule. At the same time, the search phrase “', '” will not trigger the rule.', ' the search phrase contains all search terms in any order with optional additional words in any position.', ' The rule is configured with search terms = “', '”. The search phrase “', '” will trigger this rule.', ' the rule will be triggered only and only when the search phrase is exactly equal to search terms. No additional words are allowed.', 'The rule is configured with search terms “', '”. Only the search phrase “', '” will trigger this rule, not any other.', 'We will use those triggers as an example and we will use default Solr configuration for simplicity. So, lets roll our sleeves and get some inverted search up&running!', 'First, after launching Solr we need to create the new core/collection for named rules. We can do it from the core admin page or from the terminal by executing .', ' command. ', 'We are going to use the same logical rule structure as in the previous post. The rule will be modeled as a parent document, with triggers represented as child documents. So how the example from the previous post will look in the case of Solr? ', 'In this post, we will extend our simple rule engine functionality with two essential features : \xa0rule collapsing and sorting. Rule collapsing refers to the situation when multiple rules of the same type fire, and we have to select the one with the highest priority (represented as lowest priority number).', 'Let’s start with basic rule structure. Note that we used *_i, *_s and *_t suffixes in order to map integer, string and text field types respectively.', 'Now, lets convert our sample rules into the Solr input structure:', 'After the indexing, we will have our core filled with our sample rules.', 'Since we have 3 different match modes, in order to build our inverted search query, we need to create a disjunction boolean query. We will show you the final result and then walk you through every part of the query.', 'Let’s use the keyword “how to cook” as an example. Below is a complete request how to match rules using the “how to cook” user keyword. ', 'As you can see this request correctly returns rule \xa0no.1 associated with a matchPhrase trigger configured on “', '”.', 'So, lets analyze all parts of this complex query', 'is a request to regular select RequestHandler', 'ToParentBlockJoinQuery is needed to match Rule (parent document) by it’s matched triggers (child documents)', 'This is the main query for matching triggers. As you can see, this query is a disjunction query with 3 clauses for 3 different match modes. The specific queries for each type are extracted to separate nested params exactQuery, phraseQuery and matchAllQuery', "MatchExact query, It is very straightforward - we just need to check if \xa0that keyword field content is exactly the same as the user's query. As we are only looking for exact match, un-tokenized string field is used.", 'MatchPhrase query. Here the query parser needs to cut all possible n-grams from the user search phrase. As we have a very short example keyword, \xa0we have only two n-grams “how to” and “to cook”. \xa0Using this approach, we are matching only those triggers which contain some subphrase of the user keyword.', 'MatchAll query is the trickiest one, leading to inverted search problem. We will discuss it separately to properly explain all the details', 'Collapse Filter query in order to fetch only no.1 rule of each type with the lowest priority', "Formally speaking, matchAll query means that we have to find such rules, where the tokens configured in the trigger are the subset of tokens from the user query. We don't know which tokens will match, but we know that the number of matched tokens should be exactly the same as the total number of tokens in the trigger. ", 'We conveniently store the number of tokens in the keyword in the field ', 'We will use S0lr function query framework to perform this precise matching. Function queries were designed for match scoring, but with some simple tricks we can use them for precise filtering as well:', 'We will unwind this query from inside out, so follow the numbers in the listing:', "That's it. Now we are able to perform inverted search and match our AllMatch triggers.", 'Lets consider some more examples:', 'The request for “order status” keyword, which correctly matches rule no. 2 associated with matchExact trigger configured on phrase “order status” goes as follows:', 'The request for “best oven for pizza” keyword, which correctly matches rule \xa0no. 3 associated with matchAll trigger configured on words set “oven best pizza” goes as follows:', 'We can also consider keyword “how to cook best pizza” which is matching both “how to” matchPhrase trigger and “oven best pizza” matchAll trigger, but because of collapsing filter query(fq) we are getting only rule no. 3 with the highest priority.', 'In this blog post, we discussed \xa0the trickiest part of Endeca rule migration, matchAll trigger implementation. Full fledged implementation should also include other aspects, such as:', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tApr 07, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Retail Search', 'Search']}, {'title': 'Overview of In-Stream Processing solutions on the market', 'article url': 'https://blog.griddynamics.com/overview-of-in-stream-processing-solutions-on-the-market/', 'first 160': ['Specifically, we focus on the following big questions asked from the point of view of the customer’s chief architect:', 'The choices are captured in the following decision flow diagram:', 'Overview of In-Stream Processing Solutions on the Market', 'Many vendors offer In-Stream Processing as a “feature” of a broader Big Data processing platform rather than as a separate service that is loosely coupled with their Big Data platform and, therefore, can be integrated with other Big Data platforms and services. For many customers, tight coupling of the In-Stream and\xa0Big Data processing platforms\xa0is not practical because technology decisions about Data Warehouses, Data Lakes and Batch Analytics are made at different times, by different organizations, based on different selection criteria than those used to choose an In-Stream Processing platform. Even if a comprehensive Big Data platform is already in place, the choice of a stream processing feature for that platform shouldn’t be predefined, since a standalone, self-sufficient In-Stream Processing product may fit actual and prospective business requirements much better.', 'Big Data applications are big drivers of cloud infrastructure adoption, so it should not surprise anyone that all major cloud providers are investing heavily in Big Data APIs in general, and streaming APIs in particular. Choosing a specific cloud vendor for streaming APIs has several compelling advantages, including speed of implementation, SaaS consumption and delivery model, and integration with other APIs of the cloud platform. The major concern, of course, is the implication of that choice: in all likelihood, getting out of that cloud platform later will not be practical without massive costs.', 'The choice to pick a specific cloud API should not be made lightly. If your company has already made a strategic commitment to a specific cloud and its APIs, it might be a moot point. The APIs of that cloud provider should be considered the default choice because, presumably, that’s why you chose that provider. However, if your company has not yet made such a commitment or has adopted a more balanced multi-cloud strategy, cloud portability is an essential consideration. The preferred choice would most likely be open source technologies or vendor products that can be deployed and run on any cloud with minimal operational implications.', 'Blueprints, sometimes called reference architectures, can be powerful accelerators and enablers for companies that have decided to build their own systems using open source technologies deployable on any cloud rather than to buy vendor products.', 'Such companies are Grid Dynamics’ traditional customers. They face a substantial battle to figure out:', 'Knowing how to make the right design choices and how to answer these and similar questions is our business. Grid Dynamics is an engineering services company specializing in Big Data in general, and In-Stream Processing in particular, using open source technologies and cloud environments.', 'Beside working on customer projects, we have a research lab where our architects work to identify repeatable business use cases that can be addressed with repeatable design patterns and work to turn these design patterns into reusable blueprints. These blueprints are our intellectual property, and we make them freely available to our user community.', 'When a blueprint matches the business use case closely, the time-to-market can be 30% to 50% faster than starting from scratch. That’s because a lot of design choices have already been made, tools pre-integrated, and environments pre-configured. Making modifications to a proven design is much faster than creating a brand-new design.', 'Grid Dynamics makes money by consulting on design modifications, providing implementation services, and managing the resulting systems according to SLAs. This works well for our customers, who can rely on us as a design, implementation, and managed services partner to supplement their in-house teams — using 100% open solutions developed in a fraction of the time and at a fraction of the cost of proprietary alternatives. Needless to say, this works well for us, too; we get to monetize our experience and research by providing value other vendors can’t. And if a customer chooses to use our blueprints without our help, we are still delighted, as that’s how we gain loyal friends in high places.', 'For all these reasons, we have created a blueprint called In-Stream Processing Service that will be described in detail '], 'date': '\r\n\t\t\t\t\t\t\tMay 07, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'Overview of In-Stream Processing solutions on the market', 'article url': 'https://blog.griddynamics.com/overview-of-in-stream-processing-solutions-on-the-market/', 'first 160': ['Specifically, we focus on the following big questions asked from the point of view of the customer’s chief architect:', 'The choices are captured in the following decision flow diagram:', 'Overview of In-Stream Processing Solutions on the Market', 'Many vendors offer In-Stream Processing as a “feature” of a broader Big Data processing platform rather than as a separate service that is loosely coupled with their Big Data platform and, therefore, can be integrated with other Big Data platforms and services. For many customers, tight coupling of the In-Stream and\xa0Big Data processing platforms\xa0is not practical because technology decisions about Data Warehouses, Data Lakes and Batch Analytics are made at different times, by different organizations, based on different selection criteria than those used to choose an In-Stream Processing platform. Even if a comprehensive Big Data platform is already in place, the choice of a stream processing feature for that platform shouldn’t be predefined, since a standalone, self-sufficient In-Stream Processing product may fit actual and prospective business requirements much better.', 'Big Data applications are big drivers of cloud infrastructure adoption, so it should not surprise anyone that all major cloud providers are investing heavily in Big Data APIs in general, and streaming APIs in particular. Choosing a specific cloud vendor for streaming APIs has several compelling advantages, including speed of implementation, SaaS consumption and delivery model, and integration with other APIs of the cloud platform. The major concern, of course, is the implication of that choice: in all likelihood, getting out of that cloud platform later will not be practical without massive costs.', 'The choice to pick a specific cloud API should not be made lightly. If your company has already made a strategic commitment to a specific cloud and its APIs, it might be a moot point. The APIs of that cloud provider should be considered the default choice because, presumably, that’s why you chose that provider. However, if your company has not yet made such a commitment or has adopted a more balanced multi-cloud strategy, cloud portability is an essential consideration. The preferred choice would most likely be open source technologies or vendor products that can be deployed and run on any cloud with minimal operational implications.', 'Blueprints, sometimes called reference architectures, can be powerful accelerators and enablers for companies that have decided to build their own systems using open source technologies deployable on any cloud rather than to buy vendor products.', 'Such companies are Grid Dynamics’ traditional customers. They face a substantial battle to figure out:', 'Knowing how to make the right design choices and how to answer these and similar questions is our business. Grid Dynamics is an engineering services company specializing in Big Data in general, and In-Stream Processing in particular, using open source technologies and cloud environments.', 'Beside working on customer projects, we have a research lab where our architects work to identify repeatable business use cases that can be addressed with repeatable design patterns and work to turn these design patterns into reusable blueprints. These blueprints are our intellectual property, and we make them freely available to our user community.', 'When a blueprint matches the business use case closely, the time-to-market can be 30% to 50% faster than starting from scratch. That’s because a lot of design choices have already been made, tools pre-integrated, and environments pre-configured. Making modifications to a proven design is much faster than creating a brand-new design.', 'Grid Dynamics makes money by consulting on design modifications, providing implementation services, and managing the resulting systems according to SLAs. This works well for our customers, who can rely on us as a design, implementation, and managed services partner to supplement their in-house teams — using 100% open solutions developed in a fraction of the time and at a fraction of the cost of proprietary alternatives. Needless to say, this works well for us, too; we get to monetize our experience and research by providing value other vendors can’t. And if a customer chooses to use our blueprints without our help, we are still delighted, as that’s how we gain loyal friends in high places.', 'For all these reasons, we have created a blueprint called In-Stream Processing Service that will be described in detail '], 'date': '\r\n\t\t\t\t\t\t\tMay 07, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAnton Ovchinnikov', 'author_url': 'https://blog.griddynamics.com/author/anton-ovchinnikov/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'What is In-Stream Processing?', 'article url': 'https://blog.griddynamics.com/what-is-in-stream-processing/', 'first 160': ['The term “In-Stream Processing” means that a) the data is coming into the processing engine as a continuous “stream” of events produced by some outside system or systems, and b) the processing engine works so fast that all decisions are made without stopping the data stream and storing the information first.', 'You can think of an In-Stream Processing engine as a “cyber plant” for event processing. Imagine that events are coming in at high speeds to the front docks of the cyber plant where they are captured, sorted into queues, and sent on to the assembly lines for processing. Inside, on the cyber conveyor, specialized software robots perform analytical computations and transformations that filter, match, count, aggregate, and reason about the events as they are passed down the line. Whenever something interesting is discovered or computed — such as unmasking a fraud or computing a dynamic price\xa0', ' the notification is sent immediately to an external business system to do something about it. At the end of the conveyor, processed events are shipped to a warehouse where other systems can access them for other forms of processing.', 'Since many applications of In-Stream Processing are analytical in nature, some call these systems In-Stream Analytics. Alternatively, people sometimes drop the prefix “in-” and simply call it Stream Analytics or Stream Processing. Finally, it is worth mentioning that In-Stream Processing technologies fall into a wide class of approaches for dealing with large volumes of events, called ', ', or CEP. Because In-Stream Processing is fast and aims to analyze data nearly instantaneously, it is sometimes described as Fast Data, a term that’s growing in use and popularity.', 'In-Stream Processing is only one rather specialized type of processing in a broader landscape of technologies, processes, tools and applications that are commonly referred to as Big Data.', 'In-Stream Processing typically happens on the front end of data acquisition, and serves a dual purpose of:', 'A Typical Relationship Between In-Stream and the Rest of the Big Data Infrastructure', 'In-Stream Processing cannot exist by itself; it is integrated with the rest of the Big Data infrastructure to deliver real-time processing capabilities and can be added to an existing Big Data infrastructure as a new Big Data Service.', 'Typical In-Stream Processing happily handles workloads such as these:', 'For fewer than 1,000 events per second, In-Stream Processing might be overkill; modern microservice architecture can do the job. For a sustained rate of more than 100,000 events per second, In-Stream Processing will more than likely still work, but will require a customized design to accommodate specific requirements and infrastructure choices.', 'For applications with latency requirements under 2 seconds, In-Stream Processing will not be a viable option because the data is handed off too many times between the source system, the In-Stream Processing engine, and the application that actually acts on the insight.\xa0', 'For applications that can wait 60 minutes or more, batch analytics systems like Hadoop probably offer a cheaper, simpler, and more powerful solution than In-Stream Processing', 'In-Stream Processing is rapidly gaining popularity and finding applications in various business domains. In future posts we’ll describe the anatomy of specific use cases in detail to illustrate how In-Stream Processing works. For now, here is a short list of well-known, proven applications of In-Stream Processing:', 'While business domains are quite diverse, their usage patterns are actually very similar and come down to:', '1. feeding events to the stream processing engine', '2. implementing processing logic; and', '3. delivering results to appropriate output systems that will act on the data insights developed by the processing engine', 'Common Usage Pattern for In-Stream Analytics'], 'date': '\r\n\t\t\t\t\t\t\tMay 05, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'What is In-Stream Processing?', 'article url': 'https://blog.griddynamics.com/what-is-in-stream-processing/', 'first 160': ['The term “In-Stream Processing” means that a) the data is coming into the processing engine as a continuous “stream” of events produced by some outside system or systems, and b) the processing engine works so fast that all decisions are made without stopping the data stream and storing the information first.', 'You can think of an In-Stream Processing engine as a “cyber plant” for event processing. Imagine that events are coming in at high speeds to the front docks of the cyber plant where they are captured, sorted into queues, and sent on to the assembly lines for processing. Inside, on the cyber conveyor, specialized software robots perform analytical computations and transformations that filter, match, count, aggregate, and reason about the events as they are passed down the line. Whenever something interesting is discovered or computed — such as unmasking a fraud or computing a dynamic price\xa0', ' the notification is sent immediately to an external business system to do something about it. At the end of the conveyor, processed events are shipped to a warehouse where other systems can access them for other forms of processing.', 'Since many applications of In-Stream Processing are analytical in nature, some call these systems In-Stream Analytics. Alternatively, people sometimes drop the prefix “in-” and simply call it Stream Analytics or Stream Processing. Finally, it is worth mentioning that In-Stream Processing technologies fall into a wide class of approaches for dealing with large volumes of events, called ', ', or CEP. Because In-Stream Processing is fast and aims to analyze data nearly instantaneously, it is sometimes described as Fast Data, a term that’s growing in use and popularity.', 'In-Stream Processing is only one rather specialized type of processing in a broader landscape of technologies, processes, tools and applications that are commonly referred to as Big Data.', 'In-Stream Processing typically happens on the front end of data acquisition, and serves a dual purpose of:', 'A Typical Relationship Between In-Stream and the Rest of the Big Data Infrastructure', 'In-Stream Processing cannot exist by itself; it is integrated with the rest of the Big Data infrastructure to deliver real-time processing capabilities and can be added to an existing Big Data infrastructure as a new Big Data Service.', 'Typical In-Stream Processing happily handles workloads such as these:', 'For fewer than 1,000 events per second, In-Stream Processing might be overkill; modern microservice architecture can do the job. For a sustained rate of more than 100,000 events per second, In-Stream Processing will more than likely still work, but will require a customized design to accommodate specific requirements and infrastructure choices.', 'For applications with latency requirements under 2 seconds, In-Stream Processing will not be a viable option because the data is handed off too many times between the source system, the In-Stream Processing engine, and the application that actually acts on the insight.\xa0', 'For applications that can wait 60 minutes or more, batch analytics systems like Hadoop probably offer a cheaper, simpler, and more powerful solution than In-Stream Processing', 'In-Stream Processing is rapidly gaining popularity and finding applications in various business domains. In future posts we’ll describe the anatomy of specific use cases in detail to illustrate how In-Stream Processing works. For now, here is a short list of well-known, proven applications of In-Stream Processing:', 'While business domains are quite diverse, their usage patterns are actually very similar and come down to:', '1. feeding events to the stream processing engine', '2. implementing processing logic; and', '3. delivering results to appropriate output systems that will act on the data insights developed by the processing engine', 'Common Usage Pattern for In-Stream Analytics'], 'date': '\r\n\t\t\t\t\t\t\tMay 05, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAnton Ovchinnikov', 'author_url': 'https://blog.griddynamics.com/author/anton-ovchinnikov/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'How In-Stream Processing works', 'article url': 'https://blog.griddynamics.com/how-in-stream-processing-works/', 'first 160': ['As we already know, In-Stream Processing is a service that takes events as input and produces results that are delivered to other systems.', 'Typical In-Stream Processing Architecture', 'The architecture consists of several components:', 'A message queue serves several purposes for In-Stream Processing: it smoothes out peak loads; provides persistent storage for all events; or it can allow several independent processing units to consume the same stream of events.', 'Since a message queue initially collects events from all raw data sources, it has to be performant, scalable, and fault tolerant. That’s why it commonly runs on several dedicated servers which form the message queue cluster. The main concepts essential for understanding how highly scalable message queues work are Topics, Partitions, Producers, and Consumers.', 'Message Queue Basic Concepts, Parallelization Approach', 'Events ordering is guaranteed only inside one partition. Therefore, correct partition design for the original event stream is vital for business applications where message ordering is important.', 'Another important aspect of queue capabilities is reliability. Message queues must remain available for producers and consumers despite server failures or network issues, with minimal risks of data loss. To achieve that, data in every partition is replicated to multiple nodes of a cluster and persists several times per minute; see the diagram below. The efficient architectural design of these features is extremely important to keep the message queues highly performant.', 'Message Queue Reliability', 'In case of a failure at the consumer side it might be necessary to re-process data that was already read from the queue. Therefore, the capability to replay the stream starting at some point in the past becomes an essential component of overall reliability in a stream processing service.', 'Event Re-Read Capability', 'An In-Stream Processing application can be represented as a sequence of transformations, as shown in the next diagram. Every individual transformation must be simple and fast. As these transformations are chained together in a pipeline, the resulting algorithms are powerful, as well as rapid.', 'A Stream Processing Application As a Sequence of Transformations', 'New processing steps can be added to existing pipelines over time to improve the algorithms rather easily, leading to a fast development cycle of stream applications and extendability of the stream processing service.', 'At the same time, the transformations must be efficiently parallelizable to run independently on different nodes in a cluster, leading to a massively scalable design.', 'To assure this efficient parallelization, stream developers operate with two logical instruments: ', ' and ', '.', 'Parellelization of a Streaming Application', 'Developers need to define the logical model of parallelization by breaking computations into steps that are known as embarrassingly parallel computations. The process is illustrated in the diagram above. Sometimes it is actually necessary to rearrange the stream data in different partitions for different containers. This can be done by ', '. However, developers, beware: re-partitioning is an expensive operation that slows the pipeline speed considerably and should be avoided or at least minimized if at all possible.', 'Once the model is defined, the application is written using APIs of a particular In-Stream Processing framework, usually in a high-level programming language such as Java, Scala or Python. The stream processing engine will do the rest.', 'While there are many different In-Stream Processing engines on the market, they mostly follow a very similar design and architecture. Typically, the streaming cluster consists of one highly available ', ' and many ', '.', 'Containers are allocated to nodes based on resource availability, so new containers may be launched on any available node. If a node fails, the Container Manager will start up more containers on available nodes and re-run any events that may have been lost.', 'It is very important that one stream processing cluster can run many streaming applications simultaneously. Basically, an applications is simply a set of containers for the Container Manager. More applications lead to a bigger set of containers being served by the Container Manager.', 'Executing Multiple Streaming Applications on the Same Cluster', 'Machine Learning involves “training” the algorithms, called ', ', on representative datasets to “learn” the correct computations. The quality of the models depends on the quality of the training datasets and suitability of the chosen models for the use case.', 'The general approach to machine learning involves three steps:', 'In-Stream Processing can use the trained models to discover insights. It is rarely used for the training process itself, as the majority of training algorithms do not perform well in the streaming architecture. There are some exceptions; for example “k-means” clustering in Spark streaming.', 'Time Series Analysis is an area of machine learning for which In-Stream Processing is a natural fit, since it is based on sliding windows over data series. A complication that must be considered carefully is data ordering, since streaming frameworks usually don’t guarantee ordering between partitions, and time series processing is typically sensitive to it.', 'In-Stream Machine Learning is a young, but highly promising, domain of computer science that is getting a lot of attention from the research community. It is likely that new machine learning algorithms will emerge that can be run efficiently by the stream processing engines. This would allow In-Stream systems to train the models at the same time as running them, on the same machinery, and\xa0improve them over time.', 'Data Ingestion is the process of bringing data into the system for further processing. Data Enrichment adds simple quality checks and data transformations such as translating an IP address into a geographical location or a User Agent HTTP header into the operating system and browser type used by a visitor while browsing a web site. Historically, data was first loaded into batch processing systems, then transformations were made. Nowadays, more and more designs unite Data Ingestion and Data Enrichment into a single In-Stream process, because a) enriched data can be used by other In-Stream applications; and b) end users of batch analysis systems see ready-for-usage data much faster.', 'The results of all preceding phases are either individual actionable insights picked out of the original stream or a whole data stream, transformed and enriched by the processing. As we have already discussed, the In-Stream Processing service is one component of a wider Big Data landscape. In the end, it produces data used by other systems. Here are several generic use cases that require different interfaces to deliver results of the In-Stream Processing service:', 'Common Downstream Systems Interfaces'], 'date': '\r\n\t\t\t\t\t\t\tMay 05, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictoria Livschitz', 'author_url': 'https://blog.griddynamics.com/author/victoria-livschitz/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'How In-Stream Processing works', 'article url': 'https://blog.griddynamics.com/how-in-stream-processing-works/', 'first 160': ['As we already know, In-Stream Processing is a service that takes events as input and produces results that are delivered to other systems.', 'Typical In-Stream Processing Architecture', 'The architecture consists of several components:', 'A message queue serves several purposes for In-Stream Processing: it smoothes out peak loads; provides persistent storage for all events; or it can allow several independent processing units to consume the same stream of events.', 'Since a message queue initially collects events from all raw data sources, it has to be performant, scalable, and fault tolerant. That’s why it commonly runs on several dedicated servers which form the message queue cluster. The main concepts essential for understanding how highly scalable message queues work are Topics, Partitions, Producers, and Consumers.', 'Message Queue Basic Concepts, Parallelization Approach', 'Events ordering is guaranteed only inside one partition. Therefore, correct partition design for the original event stream is vital for business applications where message ordering is important.', 'Another important aspect of queue capabilities is reliability. Message queues must remain available for producers and consumers despite server failures or network issues, with minimal risks of data loss. To achieve that, data in every partition is replicated to multiple nodes of a cluster and persists several times per minute; see the diagram below. The efficient architectural design of these features is extremely important to keep the message queues highly performant.', 'Message Queue Reliability', 'In case of a failure at the consumer side it might be necessary to re-process data that was already read from the queue. Therefore, the capability to replay the stream starting at some point in the past becomes an essential component of overall reliability in a stream processing service.', 'Event Re-Read Capability', 'An In-Stream Processing application can be represented as a sequence of transformations, as shown in the next diagram. Every individual transformation must be simple and fast. As these transformations are chained together in a pipeline, the resulting algorithms are powerful, as well as rapid.', 'A Stream Processing Application As a Sequence of Transformations', 'New processing steps can be added to existing pipelines over time to improve the algorithms rather easily, leading to a fast development cycle of stream applications and extendability of the stream processing service.', 'At the same time, the transformations must be efficiently parallelizable to run independently on different nodes in a cluster, leading to a massively scalable design.', 'To assure this efficient parallelization, stream developers operate with two logical instruments: ', ' and ', '.', 'Parellelization of a Streaming Application', 'Developers need to define the logical model of parallelization by breaking computations into steps that are known as embarrassingly parallel computations. The process is illustrated in the diagram above. Sometimes it is actually necessary to rearrange the stream data in different partitions for different containers. This can be done by ', '. However, developers, beware: re-partitioning is an expensive operation that slows the pipeline speed considerably and should be avoided or at least minimized if at all possible.', 'Once the model is defined, the application is written using APIs of a particular In-Stream Processing framework, usually in a high-level programming language such as Java, Scala or Python. The stream processing engine will do the rest.', 'While there are many different In-Stream Processing engines on the market, they mostly follow a very similar design and architecture. Typically, the streaming cluster consists of one highly available ', ' and many ', '.', 'Containers are allocated to nodes based on resource availability, so new containers may be launched on any available node. If a node fails, the Container Manager will start up more containers on available nodes and re-run any events that may have been lost.', 'It is very important that one stream processing cluster can run many streaming applications simultaneously. Basically, an applications is simply a set of containers for the Container Manager. More applications lead to a bigger set of containers being served by the Container Manager.', 'Executing Multiple Streaming Applications on the Same Cluster', 'Machine Learning involves “training” the algorithms, called ', ', on representative datasets to “learn” the correct computations. The quality of the models depends on the quality of the training datasets and suitability of the chosen models for the use case.', 'The general approach to machine learning involves three steps:', 'In-Stream Processing can use the trained models to discover insights. It is rarely used for the training process itself, as the majority of training algorithms do not perform well in the streaming architecture. There are some exceptions; for example “k-means” clustering in Spark streaming.', 'Time Series Analysis is an area of machine learning for which In-Stream Processing is a natural fit, since it is based on sliding windows over data series. A complication that must be considered carefully is data ordering, since streaming frameworks usually don’t guarantee ordering between partitions, and time series processing is typically sensitive to it.', 'In-Stream Machine Learning is a young, but highly promising, domain of computer science that is getting a lot of attention from the research community. It is likely that new machine learning algorithms will emerge that can be run efficiently by the stream processing engines. This would allow In-Stream systems to train the models at the same time as running them, on the same machinery, and\xa0improve them over time.', 'Data Ingestion is the process of bringing data into the system for further processing. Data Enrichment adds simple quality checks and data transformations such as translating an IP address into a geographical location or a User Agent HTTP header into the operating system and browser type used by a visitor while browsing a web site. Historically, data was first loaded into batch processing systems, then transformations were made. Nowadays, more and more designs unite Data Ingestion and Data Enrichment into a single In-Stream process, because a) enriched data can be used by other In-Stream applications; and b) end users of batch analysis systems see ready-for-usage data much faster.', 'The results of all preceding phases are either individual actionable insights picked out of the original stream or a whole data stream, transformed and enriched by the processing. As we have already discussed, the In-Stream Processing service is one component of a wider Big Data landscape. In the end, it produces data used by other systems. Here are several generic use cases that require different interfaces to deliver results of the In-Stream Processing service:', 'Common Downstream Systems Interfaces'], 'date': '\r\n\t\t\t\t\t\t\tMay 05, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAnton Ovchinnikov', 'author_url': 'https://blog.griddynamics.com/author/anton-ovchinnikov/', 'tag': ['Big Data', 'In-Stream Processing']}, {'title': 'Six design principles of Continuous Performance Testing', 'article url': 'https://blog.griddynamics.com/six-design-principles-of-continuous-performance-testing/', 'first 160': ['In the course of delivering many successful Continuous Performance Testing (CPT) implementations for enterprise customers, Grid Dynamics engineering teams have developed a number of basic design principles to guide their actions. Your requirements may be unique, but just as all custom race cars have a chassis, suspension, and wheels, all CPT implementations need to follow the six design principles we talk about in this post.', ' ', 'Organize your performance testing into stages. Start with “cheap” tests that can identify performance problems on small datasets early in the pipeline to provide developers with instant feedback when they commit; place tests that require massive data sets and infrastructure later in the pipeline. Integrate all test executions with CI infrastructure.', 'The API is your best friend when it comes to performance testing. Focus on testing of the performance of programmatic interfaces rather than human experience. An API is your single throat to choke, as an API can be hit from different angles to measure the system’s response to various scenarios.', ' Deciding what to measure is half the battle. Define metrics that represent business KPIs, the risk of system slowdowns or breakages, and the impact of the system’s performance on these potential problems. Identify performance thresholds that constitute acceptable or faulty performance results. Get the business side of the house to sign off on them before you go to work on your CPT implementation', '100% of the testing process should be performed by software, not humans. This includes provisioning test environments; deploying relevant middleware and application code; configuring the environment and setting up the right connections to 3rd party interfaces; loading the test data; running the tests; collecting metrics; and cleaning up after the test run. 100% automation is necessary to assure that the tests can be run continuously on each commit, build, and release candidate. A single manual step cripples the pipeline and defeats its purpose.', 'Generate reports that track performance metrics of each build against the targets, monitor regression between builds, and analyze performance trends over time. Apply modern visualization techniques to make the data readable and actionable. Deliver performance data to the right people using modern dashboard tools.', 'Some insights can only be seen as a part of the broader trend. Store all test run results in a persistent data store, so that various analytical techniques can be applied to mine the data for patterns and discover performance trends over time. Historic data about performance test results is also invaluable for troubleshooting production issues down the line by comparing the performance shown in testing to actual production experience.', 'Let’s discuss each principle in detail:', 'This is the central idea behind merging performance testing into the CI pipeline in the first place. In our previous ', ', we showed you the following visual diagram:', 'To make our performance tests both automated and efficient, we segregate different types of performance testing by the type of environment, test dataset, and type of test queries we want to run. Each test is then automated and integrated into the CI pipeline. “Cheap” tests, like joint queries executed on modest syntactic datasets, are run first, followed by more “expensive” ones with more data and more complex workflows performed later in the pipeline.', 'Typical staged performance tests involve:', 'When a group of tests is broken down like this, it is possible to write automated sequences\xa0that provision the environment, load the data, and execute that group of tests end-to-end, including the quick analysis of test results for pass/fail logic within the CI process.', 'When we consider the best approaches to test the system’s performance automatically, APIs are our best friends for two reasons:', 'Splitting the full system into a number of API calls gives us the ability to reduce test complexity and duration, increase stability, and simplify investigation of any issues we find.', 'Let’s take the “checkout” scenario as an example. Our virtual user is going through several steps: “go to shopping cart “> “select products” > “select quantity” > “apply offers” > “enter shipping details” > “enter payment details” > “confirm taxes and shipping costs” > “place order”. At every step there are several UI actions and multiple API calls, both sequential and asynchronous. For proper performance metrics collection we are measuring the response times of every UI action and API call, and the duration of each complete step in the scenario. This allows us to check each scenario step duration against our acceptance criteria, as well as review the impact of every action and call. ', 'We can use the same type of scenario for stress tests and endurance testing by adjusting the number of virtual users and the test duration.', 'All performance tests boil down to validating how your system will behave in a certain configuration under a certain load scenario, and at what points it will start slowing down —\xa0and eventually break down. You are basically performing automated risk management: am I at risk of my system breaking due to poor performance? The key to successful risk management starts with knowing your risks, such as:', 'These potential “risk areas” will become the targets of our performance tests.', 'Once you have identified the riskiest parts of the system, the next step is to define the critical thresholds that the business side finds acceptable —\xa0or not. It could be:', 'It is critically important to involve business analysts in documenting critical thresholds so that the tests can be engineered to recreate the right test conditions. Get your performance KPIs right in advance, and get your business people to buy into them. If the business risks can be formalized in these KPIs and their thresholds, the performance engineering team can usually design a test to validate them.', 'When we design our performance tests, we need to think about how we:', 'We need powerful frameworks and tools that can perform all these functions, and do this repeatedly, automatically, and reliably, as a part of the CI process. While this might seem like a tall order, we will present you a complete set of 100% free open source tools to achieve this goal.', 'The importance of complete test execution automation cannot be overstated. Any manual step in the process means that human beings must be involved in the process, and therefore the performance tests cannot be a part of automated CI pipelines.', 'When we design our performance tests, we need to think about how we will analyze our results. Some results are easier to interpret than others. For example, you may need to correlate the data from latency, transaction-per-second throughput, and CPU utilization in typical load scenarios to answer a question like, “How well does this test result predict the performance of our application in production?” Here are some common approaches we use to evaluate performance results:', 'Again, we need powerful tools for the analysis of the data. In later blog posts we will recommend the toolset that we know and love, that proved itself in many successful implementations.', 'Some insights can only be evaluated accurately as part of a broader trend. Store all test run results in a persistent data store, so that various analytical techniques can be applied to mine the data for patterns and discover performance trends over time. Historic data about performance test results is also invaluable for troubleshooting production issues down the line by comparing the performance shown in testing to the actual production experience.', 'This requires a robust backend designed specifically to store test results and conveniently access them for further analysis. In later blogs we will present you with our approach for implementing such capabilities.\xa0', 'In the rest of this blog series, we will cover specific tools and frameworks for achieving all six of these principles using open source technologies. ', '\xa0'], 'date': '\r\n\t\t\t\t\t\t\tOct 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tMikhail Klokov', 'author_url': 'https://blog.griddynamics.com/author/mikhail-klokov/', 'tag': ['Test Automation', 'Continuous Performance Testing']}, {'title': 'Six design principles of Continuous Performance Testing', 'article url': 'https://blog.griddynamics.com/six-design-principles-of-continuous-performance-testing/', 'first 160': ['In the course of delivering many successful Continuous Performance Testing (CPT) implementations for enterprise customers, Grid Dynamics engineering teams have developed a number of basic design principles to guide their actions. Your requirements may be unique, but just as all custom race cars have a chassis, suspension, and wheels, all CPT implementations need to follow the six design principles we talk about in this post.', ' ', 'Organize your performance testing into stages. Start with “cheap” tests that can identify performance problems on small datasets early in the pipeline to provide developers with instant feedback when they commit; place tests that require massive data sets and infrastructure later in the pipeline. Integrate all test executions with CI infrastructure.', 'The API is your best friend when it comes to performance testing. Focus on testing of the performance of programmatic interfaces rather than human experience. An API is your single throat to choke, as an API can be hit from different angles to measure the system’s response to various scenarios.', ' Deciding what to measure is half the battle. Define metrics that represent business KPIs, the risk of system slowdowns or breakages, and the impact of the system’s performance on these potential problems. Identify performance thresholds that constitute acceptable or faulty performance results. Get the business side of the house to sign off on them before you go to work on your CPT implementation', '100% of the testing process should be performed by software, not humans. This includes provisioning test environments; deploying relevant middleware and application code; configuring the environment and setting up the right connections to 3rd party interfaces; loading the test data; running the tests; collecting metrics; and cleaning up after the test run. 100% automation is necessary to assure that the tests can be run continuously on each commit, build, and release candidate. A single manual step cripples the pipeline and defeats its purpose.', 'Generate reports that track performance metrics of each build against the targets, monitor regression between builds, and analyze performance trends over time. Apply modern visualization techniques to make the data readable and actionable. Deliver performance data to the right people using modern dashboard tools.', 'Some insights can only be seen as a part of the broader trend. Store all test run results in a persistent data store, so that various analytical techniques can be applied to mine the data for patterns and discover performance trends over time. Historic data about performance test results is also invaluable for troubleshooting production issues down the line by comparing the performance shown in testing to actual production experience.', 'Let’s discuss each principle in detail:', 'This is the central idea behind merging performance testing into the CI pipeline in the first place. In our previous ', ', we showed you the following visual diagram:', 'To make our performance tests both automated and efficient, we segregate different types of performance testing by the type of environment, test dataset, and type of test queries we want to run. Each test is then automated and integrated into the CI pipeline. “Cheap” tests, like joint queries executed on modest syntactic datasets, are run first, followed by more “expensive” ones with more data and more complex workflows performed later in the pipeline.', 'Typical staged performance tests involve:', 'When a group of tests is broken down like this, it is possible to write automated sequences\xa0that provision the environment, load the data, and execute that group of tests end-to-end, including the quick analysis of test results for pass/fail logic within the CI process.', 'When we consider the best approaches to test the system’s performance automatically, APIs are our best friends for two reasons:', 'Splitting the full system into a number of API calls gives us the ability to reduce test complexity and duration, increase stability, and simplify investigation of any issues we find.', 'Let’s take the “checkout” scenario as an example. Our virtual user is going through several steps: “go to shopping cart “> “select products” > “select quantity” > “apply offers” > “enter shipping details” > “enter payment details” > “confirm taxes and shipping costs” > “place order”. At every step there are several UI actions and multiple API calls, both sequential and asynchronous. For proper performance metrics collection we are measuring the response times of every UI action and API call, and the duration of each complete step in the scenario. This allows us to check each scenario step duration against our acceptance criteria, as well as review the impact of every action and call. ', 'We can use the same type of scenario for stress tests and endurance testing by adjusting the number of virtual users and the test duration.', 'All performance tests boil down to validating how your system will behave in a certain configuration under a certain load scenario, and at what points it will start slowing down —\xa0and eventually break down. You are basically performing automated risk management: am I at risk of my system breaking due to poor performance? The key to successful risk management starts with knowing your risks, such as:', 'These potential “risk areas” will become the targets of our performance tests.', 'Once you have identified the riskiest parts of the system, the next step is to define the critical thresholds that the business side finds acceptable —\xa0or not. It could be:', 'It is critically important to involve business analysts in documenting critical thresholds so that the tests can be engineered to recreate the right test conditions. Get your performance KPIs right in advance, and get your business people to buy into them. If the business risks can be formalized in these KPIs and their thresholds, the performance engineering team can usually design a test to validate them.', 'When we design our performance tests, we need to think about how we:', 'We need powerful frameworks and tools that can perform all these functions, and do this repeatedly, automatically, and reliably, as a part of the CI process. While this might seem like a tall order, we will present you a complete set of 100% free open source tools to achieve this goal.', 'The importance of complete test execution automation cannot be overstated. Any manual step in the process means that human beings must be involved in the process, and therefore the performance tests cannot be a part of automated CI pipelines.', 'When we design our performance tests, we need to think about how we will analyze our results. Some results are easier to interpret than others. For example, you may need to correlate the data from latency, transaction-per-second throughput, and CPU utilization in typical load scenarios to answer a question like, “How well does this test result predict the performance of our application in production?” Here are some common approaches we use to evaluate performance results:', 'Again, we need powerful tools for the analysis of the data. In later blog posts we will recommend the toolset that we know and love, that proved itself in many successful implementations.', 'Some insights can only be evaluated accurately as part of a broader trend. Store all test run results in a persistent data store, so that various analytical techniques can be applied to mine the data for patterns and discover performance trends over time. Historic data about performance test results is also invaluable for troubleshooting production issues down the line by comparing the performance shown in testing to the actual production experience.', 'This requires a robust backend designed specifically to store test results and conveniently access them for further analysis. In later blogs we will present you with our approach for implementing such capabilities.\xa0', 'In the rest of this blog series, we will cover specific tools and frameworks for achieving all six of these principles using open source technologies. ', '\xa0'], 'date': '\r\n\t\t\t\t\t\t\tOct 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictor Samoylov', 'author_url': 'https://blog.griddynamics.com/author/victor-samoylov/', 'tag': ['Test Automation', 'Continuous Performance Testing']}, {'title': 'Six design principles of Continuous Performance Testing', 'article url': 'https://blog.griddynamics.com/six-design-principles-of-continuous-performance-testing/', 'first 160': ['In the course of delivering many successful Continuous Performance Testing (CPT) implementations for enterprise customers, Grid Dynamics engineering teams have developed a number of basic design principles to guide their actions. Your requirements may be unique, but just as all custom race cars have a chassis, suspension, and wheels, all CPT implementations need to follow the six design principles we talk about in this post.', ' ', 'Organize your performance testing into stages. Start with “cheap” tests that can identify performance problems on small datasets early in the pipeline to provide developers with instant feedback when they commit; place tests that require massive data sets and infrastructure later in the pipeline. Integrate all test executions with CI infrastructure.', 'The API is your best friend when it comes to performance testing. Focus on testing of the performance of programmatic interfaces rather than human experience. An API is your single throat to choke, as an API can be hit from different angles to measure the system’s response to various scenarios.', ' Deciding what to measure is half the battle. Define metrics that represent business KPIs, the risk of system slowdowns or breakages, and the impact of the system’s performance on these potential problems. Identify performance thresholds that constitute acceptable or faulty performance results. Get the business side of the house to sign off on them before you go to work on your CPT implementation', '100% of the testing process should be performed by software, not humans. This includes provisioning test environments; deploying relevant middleware and application code; configuring the environment and setting up the right connections to 3rd party interfaces; loading the test data; running the tests; collecting metrics; and cleaning up after the test run. 100% automation is necessary to assure that the tests can be run continuously on each commit, build, and release candidate. A single manual step cripples the pipeline and defeats its purpose.', 'Generate reports that track performance metrics of each build against the targets, monitor regression between builds, and analyze performance trends over time. Apply modern visualization techniques to make the data readable and actionable. Deliver performance data to the right people using modern dashboard tools.', 'Some insights can only be seen as a part of the broader trend. Store all test run results in a persistent data store, so that various analytical techniques can be applied to mine the data for patterns and discover performance trends over time. Historic data about performance test results is also invaluable for troubleshooting production issues down the line by comparing the performance shown in testing to actual production experience.', 'Let’s discuss each principle in detail:', 'This is the central idea behind merging performance testing into the CI pipeline in the first place. In our previous ', ', we showed you the following visual diagram:', 'To make our performance tests both automated and efficient, we segregate different types of performance testing by the type of environment, test dataset, and type of test queries we want to run. Each test is then automated and integrated into the CI pipeline. “Cheap” tests, like joint queries executed on modest syntactic datasets, are run first, followed by more “expensive” ones with more data and more complex workflows performed later in the pipeline.', 'Typical staged performance tests involve:', 'When a group of tests is broken down like this, it is possible to write automated sequences\xa0that provision the environment, load the data, and execute that group of tests end-to-end, including the quick analysis of test results for pass/fail logic within the CI process.', 'When we consider the best approaches to test the system’s performance automatically, APIs are our best friends for two reasons:', 'Splitting the full system into a number of API calls gives us the ability to reduce test complexity and duration, increase stability, and simplify investigation of any issues we find.', 'Let’s take the “checkout” scenario as an example. Our virtual user is going through several steps: “go to shopping cart “> “select products” > “select quantity” > “apply offers” > “enter shipping details” > “enter payment details” > “confirm taxes and shipping costs” > “place order”. At every step there are several UI actions and multiple API calls, both sequential and asynchronous. For proper performance metrics collection we are measuring the response times of every UI action and API call, and the duration of each complete step in the scenario. This allows us to check each scenario step duration against our acceptance criteria, as well as review the impact of every action and call. ', 'We can use the same type of scenario for stress tests and endurance testing by adjusting the number of virtual users and the test duration.', 'All performance tests boil down to validating how your system will behave in a certain configuration under a certain load scenario, and at what points it will start slowing down —\xa0and eventually break down. You are basically performing automated risk management: am I at risk of my system breaking due to poor performance? The key to successful risk management starts with knowing your risks, such as:', 'These potential “risk areas” will become the targets of our performance tests.', 'Once you have identified the riskiest parts of the system, the next step is to define the critical thresholds that the business side finds acceptable —\xa0or not. It could be:', 'It is critically important to involve business analysts in documenting critical thresholds so that the tests can be engineered to recreate the right test conditions. Get your performance KPIs right in advance, and get your business people to buy into them. If the business risks can be formalized in these KPIs and their thresholds, the performance engineering team can usually design a test to validate them.', 'When we design our performance tests, we need to think about how we:', 'We need powerful frameworks and tools that can perform all these functions, and do this repeatedly, automatically, and reliably, as a part of the CI process. While this might seem like a tall order, we will present you a complete set of 100% free open source tools to achieve this goal.', 'The importance of complete test execution automation cannot be overstated. Any manual step in the process means that human beings must be involved in the process, and therefore the performance tests cannot be a part of automated CI pipelines.', 'When we design our performance tests, we need to think about how we will analyze our results. Some results are easier to interpret than others. For example, you may need to correlate the data from latency, transaction-per-second throughput, and CPU utilization in typical load scenarios to answer a question like, “How well does this test result predict the performance of our application in production?” Here are some common approaches we use to evaluate performance results:', 'Again, we need powerful tools for the analysis of the data. In later blog posts we will recommend the toolset that we know and love, that proved itself in many successful implementations.', 'Some insights can only be evaluated accurately as part of a broader trend. Store all test run results in a persistent data store, so that various analytical techniques can be applied to mine the data for patterns and discover performance trends over time. Historic data about performance test results is also invaluable for troubleshooting production issues down the line by comparing the performance shown in testing to the actual production experience.', 'This requires a robust backend designed specifically to store test results and conveniently access them for further analysis. In later blogs we will present you with our approach for implementing such capabilities.\xa0', 'In the rest of this blog series, we will cover specific tools and frameworks for achieving all six of these principles using open source technologies. ', '\xa0'], 'date': '\r\n\t\t\t\t\t\t\tOct 11, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Latnikov', 'author_url': 'https://blog.griddynamics.com/author/dmitry-latnikov/', 'tag': ['Test Automation', 'Continuous Performance Testing']}, {'title': 'How to extend CI pipelines with Continuous Performance Testing', 'article url': 'https://blog.griddynamics.com/how-to-extend-continuous-integration-ci-pipelines-with-continuous-performance-testing-cpt/', 'first 160': ["In previous posts we've talked about why \xa0Continuous Performance Testing (CPT) must be an ", ".\xa0We've also discussed some of the\xa0", '. ', "Now it's time to get into the nitty-gritty of making CPT part of your CI pipeline. As you will quickly discover, this isn’t rocket science once you understand the goals and the process. ", 'The standard pipeline for application development can be logically split into two main sections:', '\xa01. \xa0A continuous CI loop that starts with every commit and progressively applied to most stable builds, and', '\xa02. Release-level testing and sign-off, which is executed against the release candidate before it is about to go into production.', 'A pre-release sign-off test happens infrequently and is heavily manual in nature. It involves deployment of a full test environment that typically includes various end-to-end (E2E) user scenarios combined with load, stress, and stability testing. This is a highly skilled job performed by the application engineers and is both explorative and experimental in nature; they must set up the environment, configure tools to measure performance and utilization elements, run tests, measure results, analyze the findings, modify scenarios, adjust the infrastructure, reconfigure their tools, and try again.\xa0', 'Any performance issues found at this stage result in a tough dilemma for the release management team: Should they let the release through with known performance issues, or send the release back to engineering and miss the ship date?', 'Both options are far from ideal. It’s far more efficient to evaluate the performance characteristics of each build during active development, at the same time other quality issues are discovered and corrected. Getting this done right —\xa0cost-effectively and automatically —\xa0is the central idea behind CPT.\xa0', 'This can be done by taking \xa0performance testing functions that can be automated and moving them into the CI pipeline, like this:', 'Effectively, we apply the principles of multi-stage regression test pipelines to performance testing. This is typically done by creating a series of rather simple tests that check different aspects of the system against performance objectives in a selected set of scenarios, and inserting these tests into appropriate phases of the CI pipeline as shown above.', 'While not all types of performance testing can be reduced to the types of tests suitable for CI pipelines, here are a few common ones that can:\xa0', 'All three types of tests lend themselves well to integration with different stages of the existing CI pipeline: performance smoke tests extend existing functional smoke tests; performance regression testing supplements other forms of regression testing; and performance E2E tests are best run side by side with integration testing.\xa0', 'In our next post, we’ll dive deeper into the specifics of successfully extending existing CI pipelines with Continuous Performance Testing.'], 'date': '\r\n\t\t\t\t\t\t\tSep 21, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Latnikov', 'author_url': 'https://blog.griddynamics.com/author/dmitry-latnikov/', 'tag': ['Test Automation', 'Continuous Performance Testing']}, {'title': 'How to extend CI pipelines with Continuous Performance Testing', 'article url': 'https://blog.griddynamics.com/how-to-extend-continuous-integration-ci-pipelines-with-continuous-performance-testing-cpt/', 'first 160': ["In previous posts we've talked about why \xa0Continuous Performance Testing (CPT) must be an ", ".\xa0We've also discussed some of the\xa0", '. ', "Now it's time to get into the nitty-gritty of making CPT part of your CI pipeline. As you will quickly discover, this isn’t rocket science once you understand the goals and the process. ", 'The standard pipeline for application development can be logically split into two main sections:', '\xa01. \xa0A continuous CI loop that starts with every commit and progressively applied to most stable builds, and', '\xa02. Release-level testing and sign-off, which is executed against the release candidate before it is about to go into production.', 'A pre-release sign-off test happens infrequently and is heavily manual in nature. It involves deployment of a full test environment that typically includes various end-to-end (E2E) user scenarios combined with load, stress, and stability testing. This is a highly skilled job performed by the application engineers and is both explorative and experimental in nature; they must set up the environment, configure tools to measure performance and utilization elements, run tests, measure results, analyze the findings, modify scenarios, adjust the infrastructure, reconfigure their tools, and try again.\xa0', 'Any performance issues found at this stage result in a tough dilemma for the release management team: Should they let the release through with known performance issues, or send the release back to engineering and miss the ship date?', 'Both options are far from ideal. It’s far more efficient to evaluate the performance characteristics of each build during active development, at the same time other quality issues are discovered and corrected. Getting this done right —\xa0cost-effectively and automatically —\xa0is the central idea behind CPT.\xa0', 'This can be done by taking \xa0performance testing functions that can be automated and moving them into the CI pipeline, like this:', 'Effectively, we apply the principles of multi-stage regression test pipelines to performance testing. This is typically done by creating a series of rather simple tests that check different aspects of the system against performance objectives in a selected set of scenarios, and inserting these tests into appropriate phases of the CI pipeline as shown above.', 'While not all types of performance testing can be reduced to the types of tests suitable for CI pipelines, here are a few common ones that can:\xa0', 'All three types of tests lend themselves well to integration with different stages of the existing CI pipeline: performance smoke tests extend existing functional smoke tests; performance regression testing supplements other forms of regression testing; and performance E2E tests are best run side by side with integration testing.\xa0', 'In our next post, we’ll dive deeper into the specifics of successfully extending existing CI pipelines with Continuous Performance Testing.'], 'date': '\r\n\t\t\t\t\t\t\tSep 21, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tMikhail Klokov', 'author_url': 'https://blog.griddynamics.com/author/mikhail-klokov/', 'tag': ['Test Automation', 'Continuous Performance Testing']}, {'title': 'Challenges of Continuous Performance Testing', 'article url': 'https://blog.griddynamics.com/why-isnt-continous-performance-testing-more-common/', 'first 160': ['The main reason website and application performance testing is not already continuous in many companies is clear: it’s hard to implement. Why? Let’s look at a few ', ' Continuous Performance Testing takes money. Production-like infrastructure and an engineering team of specialists to create, automate, and maintain test cases are two major expense categories. These costs all come out of a project’s budget. Organizations that don’t allow for these expenses before they start implementing CPT, or don’t know how to estimate them, can easily find themselves unable to complete their CPT projects.', ' Your current development team may be full of good application and test engineers who are nevertheless not skilled at performance testing. Performance testing is still a rare area of specialization that requires knowledge of multiple disciplines, including performance requirements and analysis; application architectures and design of all elements in the stack; performance profiling and tooling; test data management; design of stress test scenarios; and test automation execution — a skill mix that’s hard to find in the job market.', ' Teams find that it’s hard to allocate, configure, and maintain dedicated environments for performance testing, which must be similar to your production environments right down to configurations and data —and it’s hard to keep them updated, too. The effort involved scares many organizations away from going down this path.', ' Continuous performance testing is an advanced discipline related to automated software testing, integration and delivery. If the organization doesn’t yet have continuous CICD processes for development in place, it’s almost impossible to run continuous performance testing.', " As the old saying goes, “the squeaky wheel gets the grease.” When an application experiences frequent quality problems, fixing functionality issues is a top priority. Investments in unit tests, basic CI infrastructure, regression testing, and integration testing are typically prioritized over performance testing. \xa0It doesn't help the case for CPT that it is traditionally performed, almost as an afterthought, at the end of the development cycle. This is not an easy mindset to change.", 'All these challenges can be overcome. The good news is that\xa0Continuous Performance Testing is becoming increasingly more accessible and affordable, so more companies are putting it into practice. In subsequent blog posts, we will discuss practical approaches to overcoming the common challenges described above, and present specific recommendations on how to successfully implement CPT.'], 'date': '\r\n\t\t\t\t\t\t\tSep 05, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVictor Samoylov', 'author_url': 'https://blog.griddynamics.com/author/victor-samoylov/', 'tag': ['Test Automation', 'Continuous Performance Testing']}, {'title': 'Vector space retrieval model for e-commerce', 'article url': 'https://blog.griddynamics.com/vs-model-for-ecommerce/', 'first 160': ['Machine learning is an essential part of the modern search system. ML models are helping in many areas of search workflow, such as intent classifications, query parsing, facet selection, and, most importantly, results re-ranking. ', 'In this blog post, we will explore the benefits of expanding the coverage of machine learning models to the core part of the search workflow - document retrieval, and starts a series of posts to describe the implementation of the ideas covered in ', ' and ', '.', 'As with the majority of \xa0search systems, e-commerce search query frequency distribution forms a “head-torso-long tail” pattern:', 'For the head, many torso and some of the long-tail queries concept-oriented query parsing (a.k.a. concept search) is usually sufficient. Concept search tries to map semantically complete parts of the query to the appropriate attributes and descriptions within products. ', 'However, for the complex long-tail queries, concept search often falls short. Nowadays, a new way of product retrieval is emerging and quickly gaining popularity among search practitioners - vector retrieval. These days it is possible to create a deep learning model, which will vectorize both search query and catalog products into the same multi-dimensional vector space, which enables a new way of retrieval using nearest neighbor vector search algorithms. ', 'The focus of this blog post will be the deep learning model which will embed both queries and products into the joint vector space. ', 'The key to building an ML-based retrieval system is to have training data which provides samples of products relevant to queries. Ideally, we would like to have an explicit customer\'s "relevance judgment" on how relevant a particular product to her query. Obviously, this kind of judgment is next to impossible to obtain in the real-world. Next best thing we can do is to use customer behavior/clickstream data as a proxy to relevance judgment. For example, we can rank our products based on popularity and number of clicks and purchases: ', 'Search engine history and clickstream is not the only way to obtain relevance judgment data. \xa0Clickstream data is heavily affected by the quality of the current search engine. For example, if current search engine is not retrieving relevant products, or produces no results, there is no way for the customers to click on the relevant products and give us relevance signal. ', 'There are different techniques which can be used to augment the clickstream data:', 'When such information is aggregated with sufficient statistical significance, it can become a treasure trove of relevance judgment data. ', 'It is also possible to extend existing clickstream data using high-end general-purpose NLP models recommendations as a starting point for relevance judgment. BERT-like models, especially after fine-tuning on the domain data, are pretty useful for this. ', "Another useful approach is to synthesize queries generated from catalog data. This approach helps to fight the cold start problem when we don't have enough user engagement data for new product types or brands. In this approach, we combine product attributes to generate a query, and consider all products containing this combination of attributes as relevant with the score proportional to their popularity. ", 'Here are a few tips on query synthesis:', 'Before data is used for model training, it should go through several preprocessing steps. The goal of those steps is to better tokenize and normalize queries with respect to particular domain knowledge. ', 'Of course, theoretically, given large enough data, the ML model can figure out necessary data patterns by itself. However, in practice, it is always better to mix-in some of the domain knowledge early in the training pipeline to fight data scarcity and let the model converge faster. ', 'Preprocessing starts with ', ' and ', '. After that, text features are ready to be tokenized. However, we should avoid direct whitespace tokenization, as it often destroys the meaning of phrases. Smarter domain-specific tokenizers, which are aware of sizes, dimensions, age groups, colors can produce far better tokens for the model to consume. ', 'It is important to ', ' tokens, for example, to unify size units. ', ' and ', ' are also common practices in text features preprocessing. However, you need to be careful with stop word removal because, for example,', '"not/no/nor" are in NLTK standard stop word list and if you drop them the meaning of the concept may be lost. ', 'Normalization allows us to de-duplicate, group, and merge clickstream data. For example, if we have queries "dresses" and "dress", after normalization they both will become "dress" and clickstream data can be merged between these two queries. ', 'Once the data is normalized and cleaned up, it is time to encode it as vectors. We can use token and subtoken encoders to represent our tokens:', 'Both representations are one-hot encoded and both vectors are concatenated. ', 'Our retrieval model is inspired by ', '. To adopt the general approach from this paper to e-commerce, we have to take into account that e-commerce products are in fact semi-structured documents. We have product features: attributes, images, descriptions which carry different semantic weight. We want to encode those features separately and then merge them into the final product embedding vector which resides in the multi-dimensional vector space shared by products and search queries:', 'To achieve this kind of semantic encoding, we will employ the following architecture of the neural network, which will have separate inputs for catalog product and search terms, yet produce joint embeddings for them. ', 'Product features will go through different encoders, depending on their types:', 'Text features can be processed using a combination of the following methods:', 'Images can also be analyzed with a variety of encoders:', 'Feature embeddings are post-processed using several fully connected layers and concatenated together into a single embedding. ', 'Finally, search term encoding and product encoding are mapped into the same multi-dimensional vector space using fully connected layer with shared weights for product and query part.', 'We would like to train our model to embed products and queries into a vector space in such a way that queries and products relevant to those queries will reside in the same neighborhood. Mathematically, this means that those vectors should be close in terms of some ', ' measuring a distance in multi-dimensional vector space, such as ', '. ', 'We will use the ', ' training approach which is a popular loss choice when we want to train the model for similarity-like tasks, such as look-alike recommendations, face recognition, and other tasks where we try to cluster and search vector representations of our entities in a vector space. ', 'The main idea of the triplet loss training approach is to select a triplet of data points: ', ', ', ' and ', ' sample. Triplet loss function is rewarding for the anchor and positive sample to be closer to each other than for the anchor and negative sample by a specified margin. In our case of joint product-query embedding we will have a query as an anchor, relevant product as a positive sample, and an irrelevant product as a negative sample. ', 'Mathematically, the triplet loss formula will look as follows, for the case of a batch of N triplets:', '\n$$\n L = \\displaystyle\\sum_{i=1}^{N}[ \\| f_{i}^{a} - f_{i}^{p}\\| - \\| f _{i}^{a} - f _{i}^{n}\\| +\\alpha] _+\n$$\n    ', 'Here \xa0$f_a$- is a query vector acting as anchor, $f_p$- positive product vector and $f_n$- negative product vector, $\\alpha$ is a margin, empirically chosen constant , \xa0$||x||$ is an Euclidean norm, $[x]_+=max(0,x)$. ', 'Note, that we need a margin value to avoid collapsing of all embeddings into zero vectors. If all vectors will be 0 and we have no margin, loss function becomes zero as well, which we need to avoid. To do that, we are adding a margin value by which all the query-negative distances should exceed the query-positive distances. This loss function will produce a gradient that will pull anchor and positive together while pushing anchor and negative apart.', 'After many iterations of this simple step, we will achieve an optimal embedding for each query and product. ', 'One of the key questions in the triplet loss process is a proper sampling of the triplets. Our dataset so far only contains the query-positive pairs, so we need to augment each of these pair with a negative sample. ', 'The simplest approach is to use random sampling from the catalog. If the catalog is big enough, there is a fair chance that the sampled product will be a negative sample. There are a few challenges with this approach:', 'So, we need a different technique to mine better negatives. Let\'s consider the types of negatives we have in the catalog on the example of the "cocktail dress" query. We can retrieve a short black dress and a long evening gown. In this case, short black dress is more relevant than the long one. At the same time, in comparison with sweaters and washing machines, a long dress is way more relevant. This means that we should distinguish ', ' and ', '. For the initial part of the training out-of-class negatives are more important to help the model to distinguish the main product types to avoid producing truly bizarre results, while for the later stages of training in-class negatives are getting more important to help understand nuances.', 'It is a good idea to parametrize which portion of the in-class and out-class negative we are sampling and experiment with different distributions and ways to change this parameter during training. We can also modify a loss function to have different margins for in-class and out-of-class negative samples. Similar ideas can be found in person re-identification papers with the quadruplet loss approach. ', 'An additional important technique is ', ' \xa0The main idea is to selects triplets for back-propagation which contain "hardest" samples. This optimization technique can be used on the batch and epoch level: ', 'In practice, hard negatives mining significantly speeds up model training and improves the trained model quality.', 'The final step in the model training process is model evaluation. After the training is complete, the model have optimized its loss function, but it remains to be seen how this loss function corresponds to our goal of producing relevant results.', 'Following the classical approach, we divide the train and validation set during the preprocessing stage and use it separately. It is important to make separation on the query level to prevent having incomplete relevance data and most importantly, a data leak. ', 'We will calculate the loss for the validation set once per epoch, predominantly to check for model overfitting.', 'Most importantly, we will use a set of search quality metrics that can be calculated during the training process and measure the quality of the model based on relevance judgment from clickstream data.', 'Here is short list of the most useful metrics:', 'Keep in mind, that the retrieval metrics are the most important here as our retrieval model can be viewed as the first model in the search relevance chain. It should be mostly concerned with good product matching. Down the line, LTR or post-ranking models can take over and rerank top results and further improve NDCG and MRR metrics.', 'To perform a model evaluation using those metrics, we should have a search index based on the current model. All products should be vectorized using the current model and ANN search is performed for each evaluation query. Positive samples for a given query are used to calculate relevance metrics. The evaluation index has \xa0to be re-created at the end of each training epoch.', 'The high fidelity approach would be to create an index from all the products so that evaluation will closely resemble real-life scenarios. This is pretty costly, so a more practical solution is to evaluate the metrics using the index built on the validation set only. At the end of the training, full evaluation can be performed and compared with intermediate training results.', 'Model training should stop when we stop seeing meaningful improvements in evaluation metrics.', 'In this blog post, we described a neural network architecture and training procedure to create a high-quality relevance model for e-commerce vector search. ', 'However, the relevance model needs to be integrated into the search ecosystem and participate in both online and offline data flows. We will cover this topic in our next blog post in the series. ', 'Stay tuned and happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tSep 04, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Rusyaikin', 'author_url': 'https://blog.griddynamics.com/author/dmitry-rusyaikin/', 'tag': ['DSSM', 'vector retrieval', 'vector search', 'search models', 'bert', 'albert', 'electra', 'NDCG', 'relevancy']}, {'title': 'Vector space retrieval model for e-commerce', 'article url': 'https://blog.griddynamics.com/vs-model-for-ecommerce/', 'first 160': ['Machine learning is an essential part of the modern search system. ML models are helping in many areas of search workflow, such as intent classifications, query parsing, facet selection, and, most importantly, results re-ranking. ', 'In this blog post, we will explore the benefits of expanding the coverage of machine learning models to the core part of the search workflow - document retrieval, and starts a series of posts to describe the implementation of the ideas covered in ', ' and ', '.', 'As with the majority of \xa0search systems, e-commerce search query frequency distribution forms a “head-torso-long tail” pattern:', 'For the head, many torso and some of the long-tail queries concept-oriented query parsing (a.k.a. concept search) is usually sufficient. Concept search tries to map semantically complete parts of the query to the appropriate attributes and descriptions within products. ', 'However, for the complex long-tail queries, concept search often falls short. Nowadays, a new way of product retrieval is emerging and quickly gaining popularity among search practitioners - vector retrieval. These days it is possible to create a deep learning model, which will vectorize both search query and catalog products into the same multi-dimensional vector space, which enables a new way of retrieval using nearest neighbor vector search algorithms. ', 'The focus of this blog post will be the deep learning model which will embed both queries and products into the joint vector space. ', 'The key to building an ML-based retrieval system is to have training data which provides samples of products relevant to queries. Ideally, we would like to have an explicit customer\'s "relevance judgment" on how relevant a particular product to her query. Obviously, this kind of judgment is next to impossible to obtain in the real-world. Next best thing we can do is to use customer behavior/clickstream data as a proxy to relevance judgment. For example, we can rank our products based on popularity and number of clicks and purchases: ', 'Search engine history and clickstream is not the only way to obtain relevance judgment data. \xa0Clickstream data is heavily affected by the quality of the current search engine. For example, if current search engine is not retrieving relevant products, or produces no results, there is no way for the customers to click on the relevant products and give us relevance signal. ', 'There are different techniques which can be used to augment the clickstream data:', 'When such information is aggregated with sufficient statistical significance, it can become a treasure trove of relevance judgment data. ', 'It is also possible to extend existing clickstream data using high-end general-purpose NLP models recommendations as a starting point for relevance judgment. BERT-like models, especially after fine-tuning on the domain data, are pretty useful for this. ', "Another useful approach is to synthesize queries generated from catalog data. This approach helps to fight the cold start problem when we don't have enough user engagement data for new product types or brands. In this approach, we combine product attributes to generate a query, and consider all products containing this combination of attributes as relevant with the score proportional to their popularity. ", 'Here are a few tips on query synthesis:', 'Before data is used for model training, it should go through several preprocessing steps. The goal of those steps is to better tokenize and normalize queries with respect to particular domain knowledge. ', 'Of course, theoretically, given large enough data, the ML model can figure out necessary data patterns by itself. However, in practice, it is always better to mix-in some of the domain knowledge early in the training pipeline to fight data scarcity and let the model converge faster. ', 'Preprocessing starts with ', ' and ', '. After that, text features are ready to be tokenized. However, we should avoid direct whitespace tokenization, as it often destroys the meaning of phrases. Smarter domain-specific tokenizers, which are aware of sizes, dimensions, age groups, colors can produce far better tokens for the model to consume. ', 'It is important to ', ' tokens, for example, to unify size units. ', ' and ', ' are also common practices in text features preprocessing. However, you need to be careful with stop word removal because, for example,', '"not/no/nor" are in NLTK standard stop word list and if you drop them the meaning of the concept may be lost. ', 'Normalization allows us to de-duplicate, group, and merge clickstream data. For example, if we have queries "dresses" and "dress", after normalization they both will become "dress" and clickstream data can be merged between these two queries. ', 'Once the data is normalized and cleaned up, it is time to encode it as vectors. We can use token and subtoken encoders to represent our tokens:', 'Both representations are one-hot encoded and both vectors are concatenated. ', 'Our retrieval model is inspired by ', '. To adopt the general approach from this paper to e-commerce, we have to take into account that e-commerce products are in fact semi-structured documents. We have product features: attributes, images, descriptions which carry different semantic weight. We want to encode those features separately and then merge them into the final product embedding vector which resides in the multi-dimensional vector space shared by products and search queries:', 'To achieve this kind of semantic encoding, we will employ the following architecture of the neural network, which will have separate inputs for catalog product and search terms, yet produce joint embeddings for them. ', 'Product features will go through different encoders, depending on their types:', 'Text features can be processed using a combination of the following methods:', 'Images can also be analyzed with a variety of encoders:', 'Feature embeddings are post-processed using several fully connected layers and concatenated together into a single embedding. ', 'Finally, search term encoding and product encoding are mapped into the same multi-dimensional vector space using fully connected layer with shared weights for product and query part.', 'We would like to train our model to embed products and queries into a vector space in such a way that queries and products relevant to those queries will reside in the same neighborhood. Mathematically, this means that those vectors should be close in terms of some ', ' measuring a distance in multi-dimensional vector space, such as ', '. ', 'We will use the ', ' training approach which is a popular loss choice when we want to train the model for similarity-like tasks, such as look-alike recommendations, face recognition, and other tasks where we try to cluster and search vector representations of our entities in a vector space. ', 'The main idea of the triplet loss training approach is to select a triplet of data points: ', ', ', ' and ', ' sample. Triplet loss function is rewarding for the anchor and positive sample to be closer to each other than for the anchor and negative sample by a specified margin. In our case of joint product-query embedding we will have a query as an anchor, relevant product as a positive sample, and an irrelevant product as a negative sample. ', 'Mathematically, the triplet loss formula will look as follows, for the case of a batch of N triplets:', '\n$$\n L = \\displaystyle\\sum_{i=1}^{N}[ \\| f_{i}^{a} - f_{i}^{p}\\| - \\| f _{i}^{a} - f _{i}^{n}\\| +\\alpha] _+\n$$\n    ', 'Here \xa0$f_a$- is a query vector acting as anchor, $f_p$- positive product vector and $f_n$- negative product vector, $\\alpha$ is a margin, empirically chosen constant , \xa0$||x||$ is an Euclidean norm, $[x]_+=max(0,x)$. ', 'Note, that we need a margin value to avoid collapsing of all embeddings into zero vectors. If all vectors will be 0 and we have no margin, loss function becomes zero as well, which we need to avoid. To do that, we are adding a margin value by which all the query-negative distances should exceed the query-positive distances. This loss function will produce a gradient that will pull anchor and positive together while pushing anchor and negative apart.', 'After many iterations of this simple step, we will achieve an optimal embedding for each query and product. ', 'One of the key questions in the triplet loss process is a proper sampling of the triplets. Our dataset so far only contains the query-positive pairs, so we need to augment each of these pair with a negative sample. ', 'The simplest approach is to use random sampling from the catalog. If the catalog is big enough, there is a fair chance that the sampled product will be a negative sample. There are a few challenges with this approach:', 'So, we need a different technique to mine better negatives. Let\'s consider the types of negatives we have in the catalog on the example of the "cocktail dress" query. We can retrieve a short black dress and a long evening gown. In this case, short black dress is more relevant than the long one. At the same time, in comparison with sweaters and washing machines, a long dress is way more relevant. This means that we should distinguish ', ' and ', '. For the initial part of the training out-of-class negatives are more important to help the model to distinguish the main product types to avoid producing truly bizarre results, while for the later stages of training in-class negatives are getting more important to help understand nuances.', 'It is a good idea to parametrize which portion of the in-class and out-class negative we are sampling and experiment with different distributions and ways to change this parameter during training. We can also modify a loss function to have different margins for in-class and out-of-class negative samples. Similar ideas can be found in person re-identification papers with the quadruplet loss approach. ', 'An additional important technique is ', ' \xa0The main idea is to selects triplets for back-propagation which contain "hardest" samples. This optimization technique can be used on the batch and epoch level: ', 'In practice, hard negatives mining significantly speeds up model training and improves the trained model quality.', 'The final step in the model training process is model evaluation. After the training is complete, the model have optimized its loss function, but it remains to be seen how this loss function corresponds to our goal of producing relevant results.', 'Following the classical approach, we divide the train and validation set during the preprocessing stage and use it separately. It is important to make separation on the query level to prevent having incomplete relevance data and most importantly, a data leak. ', 'We will calculate the loss for the validation set once per epoch, predominantly to check for model overfitting.', 'Most importantly, we will use a set of search quality metrics that can be calculated during the training process and measure the quality of the model based on relevance judgment from clickstream data.', 'Here is short list of the most useful metrics:', 'Keep in mind, that the retrieval metrics are the most important here as our retrieval model can be viewed as the first model in the search relevance chain. It should be mostly concerned with good product matching. Down the line, LTR or post-ranking models can take over and rerank top results and further improve NDCG and MRR metrics.', 'To perform a model evaluation using those metrics, we should have a search index based on the current model. All products should be vectorized using the current model and ANN search is performed for each evaluation query. Positive samples for a given query are used to calculate relevance metrics. The evaluation index has \xa0to be re-created at the end of each training epoch.', 'The high fidelity approach would be to create an index from all the products so that evaluation will closely resemble real-life scenarios. This is pretty costly, so a more practical solution is to evaluate the metrics using the index built on the validation set only. At the end of the training, full evaluation can be performed and compared with intermediate training results.', 'Model training should stop when we stop seeing meaningful improvements in evaluation metrics.', 'In this blog post, we described a neural network architecture and training procedure to create a high-quality relevance model for e-commerce vector search. ', 'However, the relevance model needs to be integrated into the search ecosystem and participate in both online and offline data flows. We will cover this topic in our next blog post in the series. ', 'Stay tuned and happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tSep 04, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tMaria Dyuldina', 'author_url': 'https://blog.griddynamics.com/author/mdyuldina/', 'tag': ['DSSM', 'vector retrieval', 'vector search', 'search models', 'bert', 'albert', 'electra', 'NDCG', 'relevancy']}, {'title': 'Vector space retrieval model for e-commerce', 'article url': 'https://blog.griddynamics.com/vs-model-for-ecommerce/', 'first 160': ['Machine learning is an essential part of the modern search system. ML models are helping in many areas of search workflow, such as intent classifications, query parsing, facet selection, and, most importantly, results re-ranking. ', 'In this blog post, we will explore the benefits of expanding the coverage of machine learning models to the core part of the search workflow - document retrieval, and starts a series of posts to describe the implementation of the ideas covered in ', ' and ', '.', 'As with the majority of \xa0search systems, e-commerce search query frequency distribution forms a “head-torso-long tail” pattern:', 'For the head, many torso and some of the long-tail queries concept-oriented query parsing (a.k.a. concept search) is usually sufficient. Concept search tries to map semantically complete parts of the query to the appropriate attributes and descriptions within products. ', 'However, for the complex long-tail queries, concept search often falls short. Nowadays, a new way of product retrieval is emerging and quickly gaining popularity among search practitioners - vector retrieval. These days it is possible to create a deep learning model, which will vectorize both search query and catalog products into the same multi-dimensional vector space, which enables a new way of retrieval using nearest neighbor vector search algorithms. ', 'The focus of this blog post will be the deep learning model which will embed both queries and products into the joint vector space. ', 'The key to building an ML-based retrieval system is to have training data which provides samples of products relevant to queries. Ideally, we would like to have an explicit customer\'s "relevance judgment" on how relevant a particular product to her query. Obviously, this kind of judgment is next to impossible to obtain in the real-world. Next best thing we can do is to use customer behavior/clickstream data as a proxy to relevance judgment. For example, we can rank our products based on popularity and number of clicks and purchases: ', 'Search engine history and clickstream is not the only way to obtain relevance judgment data. \xa0Clickstream data is heavily affected by the quality of the current search engine. For example, if current search engine is not retrieving relevant products, or produces no results, there is no way for the customers to click on the relevant products and give us relevance signal. ', 'There are different techniques which can be used to augment the clickstream data:', 'When such information is aggregated with sufficient statistical significance, it can become a treasure trove of relevance judgment data. ', 'It is also possible to extend existing clickstream data using high-end general-purpose NLP models recommendations as a starting point for relevance judgment. BERT-like models, especially after fine-tuning on the domain data, are pretty useful for this. ', "Another useful approach is to synthesize queries generated from catalog data. This approach helps to fight the cold start problem when we don't have enough user engagement data for new product types or brands. In this approach, we combine product attributes to generate a query, and consider all products containing this combination of attributes as relevant with the score proportional to their popularity. ", 'Here are a few tips on query synthesis:', 'Before data is used for model training, it should go through several preprocessing steps. The goal of those steps is to better tokenize and normalize queries with respect to particular domain knowledge. ', 'Of course, theoretically, given large enough data, the ML model can figure out necessary data patterns by itself. However, in practice, it is always better to mix-in some of the domain knowledge early in the training pipeline to fight data scarcity and let the model converge faster. ', 'Preprocessing starts with ', ' and ', '. After that, text features are ready to be tokenized. However, we should avoid direct whitespace tokenization, as it often destroys the meaning of phrases. Smarter domain-specific tokenizers, which are aware of sizes, dimensions, age groups, colors can produce far better tokens for the model to consume. ', 'It is important to ', ' tokens, for example, to unify size units. ', ' and ', ' are also common practices in text features preprocessing. However, you need to be careful with stop word removal because, for example,', '"not/no/nor" are in NLTK standard stop word list and if you drop them the meaning of the concept may be lost. ', 'Normalization allows us to de-duplicate, group, and merge clickstream data. For example, if we have queries "dresses" and "dress", after normalization they both will become "dress" and clickstream data can be merged between these two queries. ', 'Once the data is normalized and cleaned up, it is time to encode it as vectors. We can use token and subtoken encoders to represent our tokens:', 'Both representations are one-hot encoded and both vectors are concatenated. ', 'Our retrieval model is inspired by ', '. To adopt the general approach from this paper to e-commerce, we have to take into account that e-commerce products are in fact semi-structured documents. We have product features: attributes, images, descriptions which carry different semantic weight. We want to encode those features separately and then merge them into the final product embedding vector which resides in the multi-dimensional vector space shared by products and search queries:', 'To achieve this kind of semantic encoding, we will employ the following architecture of the neural network, which will have separate inputs for catalog product and search terms, yet produce joint embeddings for them. ', 'Product features will go through different encoders, depending on their types:', 'Text features can be processed using a combination of the following methods:', 'Images can also be analyzed with a variety of encoders:', 'Feature embeddings are post-processed using several fully connected layers and concatenated together into a single embedding. ', 'Finally, search term encoding and product encoding are mapped into the same multi-dimensional vector space using fully connected layer with shared weights for product and query part.', 'We would like to train our model to embed products and queries into a vector space in such a way that queries and products relevant to those queries will reside in the same neighborhood. Mathematically, this means that those vectors should be close in terms of some ', ' measuring a distance in multi-dimensional vector space, such as ', '. ', 'We will use the ', ' training approach which is a popular loss choice when we want to train the model for similarity-like tasks, such as look-alike recommendations, face recognition, and other tasks where we try to cluster and search vector representations of our entities in a vector space. ', 'The main idea of the triplet loss training approach is to select a triplet of data points: ', ', ', ' and ', ' sample. Triplet loss function is rewarding for the anchor and positive sample to be closer to each other than for the anchor and negative sample by a specified margin. In our case of joint product-query embedding we will have a query as an anchor, relevant product as a positive sample, and an irrelevant product as a negative sample. ', 'Mathematically, the triplet loss formula will look as follows, for the case of a batch of N triplets:', '\n$$\n L = \\displaystyle\\sum_{i=1}^{N}[ \\| f_{i}^{a} - f_{i}^{p}\\| - \\| f _{i}^{a} - f _{i}^{n}\\| +\\alpha] _+\n$$\n    ', 'Here \xa0$f_a$- is a query vector acting as anchor, $f_p$- positive product vector and $f_n$- negative product vector, $\\alpha$ is a margin, empirically chosen constant , \xa0$||x||$ is an Euclidean norm, $[x]_+=max(0,x)$. ', 'Note, that we need a margin value to avoid collapsing of all embeddings into zero vectors. If all vectors will be 0 and we have no margin, loss function becomes zero as well, which we need to avoid. To do that, we are adding a margin value by which all the query-negative distances should exceed the query-positive distances. This loss function will produce a gradient that will pull anchor and positive together while pushing anchor and negative apart.', 'After many iterations of this simple step, we will achieve an optimal embedding for each query and product. ', 'One of the key questions in the triplet loss process is a proper sampling of the triplets. Our dataset so far only contains the query-positive pairs, so we need to augment each of these pair with a negative sample. ', 'The simplest approach is to use random sampling from the catalog. If the catalog is big enough, there is a fair chance that the sampled product will be a negative sample. There are a few challenges with this approach:', 'So, we need a different technique to mine better negatives. Let\'s consider the types of negatives we have in the catalog on the example of the "cocktail dress" query. We can retrieve a short black dress and a long evening gown. In this case, short black dress is more relevant than the long one. At the same time, in comparison with sweaters and washing machines, a long dress is way more relevant. This means that we should distinguish ', ' and ', '. For the initial part of the training out-of-class negatives are more important to help the model to distinguish the main product types to avoid producing truly bizarre results, while for the later stages of training in-class negatives are getting more important to help understand nuances.', 'It is a good idea to parametrize which portion of the in-class and out-class negative we are sampling and experiment with different distributions and ways to change this parameter during training. We can also modify a loss function to have different margins for in-class and out-of-class negative samples. Similar ideas can be found in person re-identification papers with the quadruplet loss approach. ', 'An additional important technique is ', ' \xa0The main idea is to selects triplets for back-propagation which contain "hardest" samples. This optimization technique can be used on the batch and epoch level: ', 'In practice, hard negatives mining significantly speeds up model training and improves the trained model quality.', 'The final step in the model training process is model evaluation. After the training is complete, the model have optimized its loss function, but it remains to be seen how this loss function corresponds to our goal of producing relevant results.', 'Following the classical approach, we divide the train and validation set during the preprocessing stage and use it separately. It is important to make separation on the query level to prevent having incomplete relevance data and most importantly, a data leak. ', 'We will calculate the loss for the validation set once per epoch, predominantly to check for model overfitting.', 'Most importantly, we will use a set of search quality metrics that can be calculated during the training process and measure the quality of the model based on relevance judgment from clickstream data.', 'Here is short list of the most useful metrics:', 'Keep in mind, that the retrieval metrics are the most important here as our retrieval model can be viewed as the first model in the search relevance chain. It should be mostly concerned with good product matching. Down the line, LTR or post-ranking models can take over and rerank top results and further improve NDCG and MRR metrics.', 'To perform a model evaluation using those metrics, we should have a search index based on the current model. All products should be vectorized using the current model and ANN search is performed for each evaluation query. Positive samples for a given query are used to calculate relevance metrics. The evaluation index has \xa0to be re-created at the end of each training epoch.', 'The high fidelity approach would be to create an index from all the products so that evaluation will closely resemble real-life scenarios. This is pretty costly, so a more practical solution is to evaluate the metrics using the index built on the validation set only. At the end of the training, full evaluation can be performed and compared with intermediate training results.', 'Model training should stop when we stop seeing meaningful improvements in evaluation metrics.', 'In this blog post, we described a neural network architecture and training procedure to create a high-quality relevance model for e-commerce vector search. ', 'However, the relevance model needs to be integrated into the search ecosystem and participate in both online and offline data flows. We will cover this topic in our next blog post in the series. ', 'Stay tuned and happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tSep 04, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['DSSM', 'vector retrieval', 'vector search', 'search models', 'bert', 'albert', 'electra', 'NDCG', 'relevancy']}, {'title': 'Finding a needle in a haystack: \nbuilding a question answering system for online store', 'article url': 'https://blog.griddynamics.com/question-answering-system-using-bert/', 'first 160': ['NLP technologies are rapidly improving, changing our user experience, and increasing the efficiency of working with text data. For instance, web search and language translation innovations changed our world, and now Deep Learning enters more and more areas. While writing these sentences, the editor corrects my grammar, suggests synonyms, analyzes the tone of the text, autocompletes words, and even whole sentences depending on the context.', 'Search is one of the main tools for everyday tasks, and it is also quickly evolving in recent years. We move from word matching to deep understanding of queries, which also changes our habits, and we start typing questions in the search boxes instead of simple keywords. Google already answers questions with instant cards and recently started to highlight the answers on a particular web page when you open it from the search results. The same works even for YouTube: the search engine can redirect you to a specific part of the video to answer your questions.', 'We call such systems ', '. QA systems help to find information more efficiently in many cases, and go beyond usual search, answering questions directly instead of searching for content similar to the query.', 'Besides web search, there are many areas where people work with domain-specific documents and need efficient tools to deal with business, medial and legal documents. Recently, COVID-19 crisis significantly increased the interest in QA systems. There are hundreds of research papers, reports, and other medical documents published every day for which we need efficient information retrieval tools, and QA systems provide an excellent addition to the classic search engines here. Another good use for QA systems is a conversational system. The dialog with digital assistant contains a lot of questions and answers, and here modern QA systems can help to replace hundreds of manually configured intents with a single model. We believe that QA is just the next logical step for information retrieval systems that will be widely integrated in the nearest future.', 'In this blog post, we describe our experience of building such systems and adapting them to a specific domain, especially when we don’t have resources to label a lot of data. We also explore how we can leverage unlabeled data, and how a knowledge distillation may help here. Finally, we touch on the performance aspects of the solutions based on popular Transformer models like BERT.', 'In our project, we are going to build QA system to find answers in customer’s Q&A and reviews for a particular type of products: photo & video cameras. You can find a similar system at amazon.com:', 'For relatively complex and technical products, like cameras, such a QA system is very useful because many questions arise during product discovery and selection. Customers study the market by comparing different options across many criteria depending on their use cases and preferences, and the information from customer Q&A and reviews is especially important. For popular cameras, you may see up to a thousand reviews.', 'Amazon uses a classic keyword search algorithm to find something relevant, yet you still need to sift through the results to find the relevant response. It does not work well for all questions, because what you want is a direct answer, not just the text similar to your question, e.g., keyword search falls short here. ', 'Below, you can see a few examples of our question answering system, in which go beyond simple keyword matching:', 'Semantic Question Answering', 'We believe that such QA systems can be of much more use in this and similar scenarios. ', 'Let’s look at the typical architecture of QA systems, models, and how we can improve the quality of available pre-trained models adapting to our photo & video cameras domain.', "One of the simplest forms of QA systems we'll talk about in this post is a ", ', when the task is to find a relatively short answer to a question in an unstructured text.', 'Of course, there are many additional tasks, such as finding both short and long answers, answering yes/no questions, response generation, multi-modal QA, etc. Conversational systems also add additional complexity, as you need to consider the dialog context. ', 'This post will focus on the most common case when we need to extract a short answer from unstructured text. The nice thing about MRC is that it covers many types of questions out of the box and doesn’t require structured data. The only requirement is that the answer should exist in the text. Even when you have semi-structured information such as product specifications, you still can easily convert it to plain text and use the QA model with it.', 'When it comes to machine reading comprehension, one cannot fail to mention the ', ' (SQuAD). There are plenty of QA or MRC datasets available (', ', ', ', ', ', etc.), but SQuAD is one of the most used. We will not go into details in this blog post, but here are several facts about SQuAD:', 'Examples from SQuAD 2.0 dataset', 'Unanswerable questions (that were added to SQuAD 2.0) require the model to determine whether an answer exists in the document. Further, we will refer to that task as the ', '. It turned out that this is a rather challenging task, as it requires a deeper understanding of the context. As stated in the ', ': “a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0”. ', 'SQuAD dataset has several limitations:', 'The main problem of almost all available QA datasets (SQuAD, NaturalQuestions, and others) is that they contain many easy examples and similar questions between the train and test data. Also, the models trained on one big QA dataset don’t perform so well on another. The high score that we see at the leaderboards near “human performance” results doesn’t mean that the models are learning reading comprehension.', 'We used SQuAD 2.0 to train a baseline model and help with some of our experiments. Despite its limitations, SQuAD is a well-structured, clean dataset, and still a good benchmark. It showed pretty good results on out-of-domain data, and there are many pre-trained models and various research papers around it.', 'The attention mechanism has become one of the key components in solving the MRC problem. In 2017, Stanford Attentive Reader used BiLSTM + Attention to achieve 79.4 F1 on SQuAD 1.1, then ', ' used the idea that attention should flow both ways — from the context to the question and from the question to the context. There were many other solutions based on attention mechanisms, but the last few years’ key breakthrough was achieved with Transformer architectures like BERT. Currently, the latest models solve this pretty complex task really well.', 'Of course, even the best models are not truly intelligent, you can easily find examples where they fail, and there are also limitations of the datasets described earlier. But, as an instrument for question answering tasks, these models already have a good quality, and they can surprise in some cases.', 'The task that involves finding an answer in multiple documents is often referred to as open-domain question answering. There are two main approaches to such systems: retrieval-only and using MRC models. The MRC-based usually includes several stages:', 'Such an approach has several disadvantages. First of all, you have several models, so no matter how well MRC works, it still depends on the retrieval stage results. Another significant limitation is the MRC performance. For example, BERT-base models may be slow even for ten documents per request. On the other hand, smaller models may not be good enough, but it depends on the desired quality and your requirements. ', 'Another approach is to reduce all those components to a dense-retrieval task when we vectorize documents and query separately (i.e., we don’t use query-document attention), and then use kNN search. Even though we discussed that research has moved towards applying the attention mechanism between query and document, solutions without it still can produce good results, especially when we want to find long answers (sentences). Recently, we started to build search engines this way, you can find more information ', '. This post will focus on the multi-stage approach, especially on how we can adapt the MRC model to our domain.', 'Despite task complexity, question answering models usually have a simple architecture on top of the Transformers like BERT. Essentially, you just need to add classifiers to predict which tokens are the start and the end of the answer. The model’s input is constructed from the query and the document, where a separation token and segment embeddings help the model differentiate a question from a document. ', 'If predictions of the start and the end token point to CLS token, then the answer does not exist. In practice, such an approach doesn’t always work very well. You can find various solutions where a separate classifier or a more complex architecture is used to improve the “no answer” classification. In our case, we also faced the problem when one version of the model is better at the answer extraction task, while another at “no answer” classification. If you are not familiar with Transformer architecture, concepts like attention, CLS token, segment embeddings, we recommend reading more about ', '. ', 'Autoregressive and sequence-to-sequence models like ', ' and ', ' also can be applied for MRC, but that is beyond the scope of our story.', 'Most of BERT-like models have limitations of max input of 512 tokens, but in our case, customer reviews can be longer than 2000 tokens. To process longer documents, we can split it into multiple instances using overlapping windows of tokens (see example below). ', 'Sequence length limitation is not the only reason why we want to split the document into smaller parts. Since attention calculations have quadratic complexity by the number of tokens, processing the document of 512 tokens is usually very slow. In practice, the window size of 128...384 tokens contains enough surrounding context to extract the answer correctly in most cases. It is computationally beneficial to process several smaller documents instead of a single big document in many cases. ', 'So there is a trade-off between window size, stride, batch size, and precision, where you should find the parameters which will fit your data and performance requirements. Transformer architectures, like ', ' and ', ', deal with long documents more efficiently, but they are usually faster only for documents with a size starting from ~2000 tokens.', 'Whether you will use a pre-train model or train your own, you still need to collect the data — a model evaluation dataset. Collecting MRC dataset is not an easy task. To prepare a good model, you need good samples, for instance, tricky examples for “no answer” cases. Additionally, not only you should label the answers, but also prepare questions if you don’t have them (compare this to tasks like classification and NER). Preparing questions can be extra challenging for exotic domains, which may require domain experts. In our case, the field is not that specific, yet many questions can be written and understood only for people with some photography experience. We decided to label a part of the test data ourselves (without services like MTurk) to understand the process’s challenges better.', 'Luckily, our case was not that hard because we already have customer’s Q&A, so we don’t need to write questions. However, we can not just use Q&A data to train/test the model, and then use it for everything else in our domain. The first reason is that customer reviews are very different from Q&A data in terms of content and document length. Another problem is that many Q&A pairs have answers biased to its question, and not only because of lexical & syntax similarity.', 'To collect better samples, we did the following:', 'During labeling, we faced the following challenges:', 'We also built our tool to label the data faster because we didn’t find any convenient tool on the market \xa0(except ', ' which can be adapted to MRC task via NER labels).', 'Here are several examples of questions from our dataset:', 'Can we generate such questions using some rules or deep learning models like GPT or T5? The answer is “yes, and no”. You can find several works describing such an approach, but we didn’t see any convincing results. We believe that it is not feasible to generate really good questions automatically, and such questions will be different from real-life examples. ', 'In general, even synthetic questions may help improve your model as an additional source of data for training, and such an approach can also be used for data augmentation. To generate the question either by rules or generative models, you first need to extract possible answers (e.g., using NER models).', 'Helpfully, there are plenty of models pre-trained on SQuAD 2.0 with different architectures and sizes at the 🤗', ' . Let’s check how well they perform in our domain (using our labeled 1000 examples):', 'You can see a clear correlation between the metrics and the model size. That is typical for the Transformers world — bigger models show better results. You can also see two types of F1 and EM (exact match) metrics in the table. Since “no answer” classification is a pretty challenging task, we are often faced with the problem that some models are good at the “no answer” task (', '), while others at the answer extraction (', '). So, it makes sense to keep an eye on both objectives separately. For the answer extraction task, we should track both F1 and EM metrics as well: even though EM is more precise, it’s difficult to label all possible variations of the correct answer. That means that EM will be underestimating the model performance. On the other hand, F1 is prone to overestimate the model performance in some cases (see examples below).', "Although the results are far from ideal and noticeably lower than the SQuAD dataset results, the largest model shows pretty good results out of the box. However, it's so slow that we can't use it in production even with GPU acceleration. RoBERTa-base and ALBERT-base models don’t perform that well, and they are also not fast enough for our case, mostly because we work with long sequences (more than 256 tokens), and we would like to process ten such documents per user request in our pipeline. Still, employing various optimizations and GPU-accelerated servers *-base models can be suitable for many cases, including ours. Let’s try to improve the models quality for our domain.", 'Does domain adaptation matter for QA tasks? After all, the model trained on the SQuAD dataset already answers many questions correctly. If we look at how we usually answer the questions like “', '”, “', '”, “', '”, it often depends on various domain-independent knowledge about the language, especially in case of syntactic and lexical similarity between a question and an answer. There are questions for which you can find an answer without understanding some words in it. But in reality, not all documents and answers are similar to a question in such a way, so domain-specific lexical knowledge is still also necessary for many questions, and the SQuAD dataset also doesn’t cover all the cases. Thus, we still need to enrich a language model with our domain knowledge and fit the model to our questions and documents.', 'In modern NLP, we use ', ' techniques to adapt the language models (LM) to a target domain and task. There are usually two main steps: ', ' on unlabeled data and then ', ' for target tasks using a labeled dataset. The LM pre-training step helps a lot with ', ' when you can achieve good results using fewer labeled samples. But even if your LM has enough knowledge after pre-training, you still need to add task-specific layers on top of it and adapt them to a target task as well, and here you still need enough labeled data (that is one of the limitations to achieve even better sample-efficiency during fine-tuning ', '). ', 'Let’s investigate how we can adapt the model to our photo & video cameras domain using unlabeled data, and what is possible to achieve with a small labeled dataset. For our project, we had ~1GB of raw text data related to the photo & video cameras domain, and ~120K unlabeled Q&A pairs.', "First, let's check if pre-training LM on in-domain data will help us. In our case, we had ~1GB of raw text, which is not enough to train the LM from scratch, but could be enough to take a pre-trained language model and continue pre-training it on domain-specific data. After we adapt LM using a masked language modeling objective ", ', we need to fine-tune it for the MRC task after that, that’s where we need labeled data. ', 'We can still use the SQuAD dataset to fine-tune task-specific layers, but since this dataset does not overlap with our domain, it probably can not outperform existing models trained on it. Even more, training on the SQuAD dataset may lead to a ', ' effect (of what we learned during pre-training). However, several studies have shown that the knowledge gained at the lower layers of the Transformer does not change dramatically during the fine-tuning on an MRC task (other tricks like discriminative fine-tuning and gradual unfreezing also can help here). ', 'We also have our small test dataset (1000 labeled examples). The dataset is still tiny to fine-tune task-specific layers, so additional fine-tuning on SQuAD may help here, and then we can fine-tune the model on our test dataset using cross-validation. Let’s perform several experiments with RoBERTa-base.', 'As we can see, additional LM pre-training improves the model. Now let’s take these models to fine-tune them using cross-validation (1000 examples, mean of 4 folds shown in the table):', "Although we cannot directly compare the models’ performance on our full test dataset with the mean of the folds, it still shows us that we can improve our model even with a small dataset. And we see again that additional LM pre-training using in-domain data helps in our case, even with fine-tuning on SQuAD after that. We also tried cross-validation without the SQuAD dataset, but the model didn't show good results as expected. But even in those experiments, the model with additional LM pre-training was better.", "Using the same approach as for creating the test dataset, we collected additional ~30K question-document pairs from customer’s Q&A and reviews for which we don’t have labels. One of the obvious ideas is to use a pseudo-labeling technique, which didn't give us noticeable improvements. But the less obvious idea is to apply a knowledge distillation to our problem. ", ' is usually used as a model compression technique when we have a model, but it is not fast or small enough. Knowledge distillation is a technique in which we train a smaller model (', ') using output probabilities from our primary larger model (', '), so a student starts to imitate its teacher’s behavior. Rather than training only with a cross-entropy loss over the hard targets (ground-truth labels), we also transfer the knowledge from the teacher to the student with a cross-entropy over probabilities of the teacher. The loss based on comparing output distributions is much richer than from only hard targets. The idea is that by using such “soft” labels, we can transfer some “dark knowledge” from the teacher model. Additionally, learning from “soft” labels prevents the model from being too sure about its prediction, similarly to a label smoothing technique.', 'Knowledge Distillation is a beautiful technique that works surprisingly well and is especially useful with Transformers. Bigger models often show better results, but it’s hard to put such big models to production. And knowledge distillation is one of the main techniques to help here.', "Now, let’s understand how we can apply distillation for our case. As we saw earlier, ALBERT-xxlarge showed the best results on our dataset, so it will be a teacher model. We don’t have labels for our 30K examples, but we can just remove part of the loss which uses labels. Sure, we will inherit more teacher mistakes during knowledge distillation without ground-through labels, but we don’t have models better than our teacher at the moment, so even to get similar results on a smaller model would be useful for us. Let’s try to distill the knowledge to our RoBERTa-base fine-tuned on in-domain data (we didn't use ALBERT-base because there were no fast tokenizers and ONNX support for ALBERT architecture at that time).", 'As you can see, we got RoBERTa-base with F1/EM close to its teacher ALBERT-xxlarge, which is also much slower than our RoBERTa, and we didn’t use any labeled data for training. Of course, it doesn’t mean we don’t need labeled data anymore, the score is still far from ideal, and we also inherited teacher mistakes, so adding labeled data may improve this training procedure.', 'We can also think about distillation as an additional pre-training step to achieve better sample-efficiency when using labeled data. Below, you can see that a knowledge distillation helps in our cross-validation experiments as well.', 'The better teacher you have, the better student you can probably train. So, one of the main directions can be improving the teacher model. First of all, distillation training procedure allows us to effectively use an ensemble of teachers and distill them to a single smaller model. Another exciting technique is the ', ' when we use the same model architecture for both student and teacher. Self-distillation allows us to train a student model which will perform better than its teacher. It may seem strange, but because a student updates its weights learning from the data that the teacher didn’t see, this can lead to better performance of the student (of comparable size) on the data from that distribution. Our experiments also reproduced this behavior when we applied self-distillation for ALBERT-xxlarge and then used it as one of our teachers for further knowledge distillation to RoBERTa-base.', 'Knowledge distillation as pre-training using unlabeled data is not a novel idea, and it already showed good results in both computer vision (', ', ', ') and NLP (', ').', 'Worth to say that even though we didn’t label any data for distillation, we still had questions, which is not always the case. But as we mentioned earlier, we didn’t use the original Q&A pairs. Instead, we collected these pairs from independent Q&As and reviews using the pre-trained USE-QA model. In such a way, we can approach the further model improvements in production. After we start collecting real questions asked by users interacting with our system, we can find candidate documents for these questions in the same way, and use this dataset for knowledge distillation without labeling many examples.', 'RoBERTa-base model was our choice in terms of a trade-off between quality and speed, but it is possible to distill the knowledge even into smaller models like DistilRoBERTa. As we said earlier, RoBERTa-base is still relatively slow, so we applied various optimizations (most of them are well-known), which allowed us to achieve more than 3.5x acceleration. Here is the list of things which worked well for us:', 'Besides knowledge distillation, there are other popular techniques, for instance, pruning. For computationally efficient ', ', better to prune entire attention heads or even layers. Yet pruning is more beneficial when your goal is to decrease the model’s size, not to accelerate it. The pruning could also be easily applied after model training by analyzing which weights are less important. In our case, we were able to prune ~15% of attention heads without a significant drop in accuracy. ', 'There are also various task-specific model optimizations. For instance, ', ' which is mostly applied for a classification task, and ', ' for a document ranking model. The second one requires pretty simple modifications to pre-trained BERT or RoBERTa architecture, and can significantly accelerate the documents re-ranking model in the question-answering pipeline.', 'We also tried to apply that to our MRC model, where we removed attention (before fine-tuning) between question and document for 1-5 layers without a significant drop in accuracy, but it gave us only ~1.15x acceleration, and only for documents longer than 256 tokens.', "Generally, we can achieve a low latency, but high throughput for long documents is difficult to achieve even with DistilRoBERTa. Moreover, in our case, we need to apply the MRC model to 5-10 documents per user request. That's why retrieval and re-ranking models are also important. If they allow us to retrieve smaller chunks of a document more accurately, we will feed the MRC model with a smaller number of shorter candidates. ", 'However, for high load systems, it makes sense to consider less complex models and retrieval-only approaches (without the MRC model). You can also implement two types of question answering systems - slower and more accurate vs. faster and less accurate. That will allow you to balance the load between them or use a "slower" model in near real-time mode. You can immediately return the answer provided by the faster QA system, then calculate the answer using more complex models in the background, and provide high-quality answers to similar questions next time.', 'In this blog post, we described how the machine reading comprehension can be used for building question answering systems over a set of documents. But the same model can benefit other systems in our photo & video cameras domain. ', 'The first application we discovered is that we can use MRC as a universal information extractor. Why can only users ask questions to our model? We can predefine a set of questions to extract the information for other tasks automatically. One such example is the automatic extraction of the pros and cons of particular products from customer reviews. When a user opens a product page, we can show such information extracted by our MRC model in a separate block.', 'Another native place for MRC is a conversational system, where we can replace a bunch of NLU models with a single MRC model. It can be especially effective for product information questions and FAQs. A less obvious application is using MRC as a NER model, asking the questions to extract information from user utterances (see example below). Such an approach may not be efficient when you have many types of entities, and doesn’t work so well for some entities as the classic NER model, but still a pretty interesting example of zero-shot NER.', 'Speaking about photos & cameras, and other similar products, not only Q&A and review may have useful information. Nobody reads user manuals any more: when facing with the issue, we tend to google the manual and find the answer there. So, user manuals can be also put into a question answering system along with conversational system to provide a user-friendly access to user manual data.', 'In this blog post, we took a dive into question answering systems and, in particular, machine reading comprehension models based on Transformer architecture such as BERT. Nowadays, Transformers allow us to build advanced NLP-powered applications and achieve good results with fewer labeled examples. And even though many of these models are huge, modern GPUs and techniques like knowledge distillation allow us to use them in production.', 'Worth to say that besides the MRC model, there is one more critical component that selects relevant documents for MRC. Even though we didn’t touch that topic in this blog post, document ranking is as crucial as the MRC model. No matter how good the reader model, the system will not return good answers if you feed it with not relevant documents.', 'Besides the classic multi-stage (ranker + reader) approach to question answering systems, fully retrieval-based methods are a strong alternative that could provide competitive results for some cases and could be more suitable for high-load applications. And we also successfully use such approaches to build ', '.', 'We believe that question answering models could be a great addition to many information retrieval systems and benefit many areas where people work with domain-specific documents and need efficient tools to work with them. We are watching with curiosity in which new places these systems will show themselves, and if you see such places in your domain, we will be glad to discuss it with you.', 'Transfer Learning and Transformers:', 'Question Answering overview:', 'Analyzing QA datasets and what Transformers learn from them:', 'Pruning, distillation, quantization:', 'Speedup BERT re-ranking models:', 'Open Domain QA:', 'Frameworks to train MRC models:'], 'date': '\r\n\t\t\t\t\t\t\tOct 15, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tOleg Smirnov', 'author_url': 'https://blog.griddynamics.com/author/osmirnov/', 'tag': ['natural language processing', 'NLP', 'question answering', 'machine reading comprehension', 'MRC', 'SQuAD', 'transfer learning', 'Transformers', 'bert', 'albert', 'roberta', 'knowledge distillation']}, {'title': 'Finding a needle in a haystack: \nbuilding a question answering system for online store', 'article url': 'https://blog.griddynamics.com/question-answering-system-using-bert/', 'first 160': ['NLP technologies are rapidly improving, changing our user experience, and increasing the efficiency of working with text data. For instance, web search and language translation innovations changed our world, and now Deep Learning enters more and more areas. While writing these sentences, the editor corrects my grammar, suggests synonyms, analyzes the tone of the text, autocompletes words, and even whole sentences depending on the context.', 'Search is one of the main tools for everyday tasks, and it is also quickly evolving in recent years. We move from word matching to deep understanding of queries, which also changes our habits, and we start typing questions in the search boxes instead of simple keywords. Google already answers questions with instant cards and recently started to highlight the answers on a particular web page when you open it from the search results. The same works even for YouTube: the search engine can redirect you to a specific part of the video to answer your questions.', 'We call such systems ', '. QA systems help to find information more efficiently in many cases, and go beyond usual search, answering questions directly instead of searching for content similar to the query.', 'Besides web search, there are many areas where people work with domain-specific documents and need efficient tools to deal with business, medial and legal documents. Recently, COVID-19 crisis significantly increased the interest in QA systems. There are hundreds of research papers, reports, and other medical documents published every day for which we need efficient information retrieval tools, and QA systems provide an excellent addition to the classic search engines here. Another good use for QA systems is a conversational system. The dialog with digital assistant contains a lot of questions and answers, and here modern QA systems can help to replace hundreds of manually configured intents with a single model. We believe that QA is just the next logical step for information retrieval systems that will be widely integrated in the nearest future.', 'In this blog post, we describe our experience of building such systems and adapting them to a specific domain, especially when we don’t have resources to label a lot of data. We also explore how we can leverage unlabeled data, and how a knowledge distillation may help here. Finally, we touch on the performance aspects of the solutions based on popular Transformer models like BERT.', 'In our project, we are going to build QA system to find answers in customer’s Q&A and reviews for a particular type of products: photo & video cameras. You can find a similar system at amazon.com:', 'For relatively complex and technical products, like cameras, such a QA system is very useful because many questions arise during product discovery and selection. Customers study the market by comparing different options across many criteria depending on their use cases and preferences, and the information from customer Q&A and reviews is especially important. For popular cameras, you may see up to a thousand reviews.', 'Amazon uses a classic keyword search algorithm to find something relevant, yet you still need to sift through the results to find the relevant response. It does not work well for all questions, because what you want is a direct answer, not just the text similar to your question, e.g., keyword search falls short here. ', 'Below, you can see a few examples of our question answering system, in which go beyond simple keyword matching:', 'Semantic Question Answering', 'We believe that such QA systems can be of much more use in this and similar scenarios. ', 'Let’s look at the typical architecture of QA systems, models, and how we can improve the quality of available pre-trained models adapting to our photo & video cameras domain.', "One of the simplest forms of QA systems we'll talk about in this post is a ", ', when the task is to find a relatively short answer to a question in an unstructured text.', 'Of course, there are many additional tasks, such as finding both short and long answers, answering yes/no questions, response generation, multi-modal QA, etc. Conversational systems also add additional complexity, as you need to consider the dialog context. ', 'This post will focus on the most common case when we need to extract a short answer from unstructured text. The nice thing about MRC is that it covers many types of questions out of the box and doesn’t require structured data. The only requirement is that the answer should exist in the text. Even when you have semi-structured information such as product specifications, you still can easily convert it to plain text and use the QA model with it.', 'When it comes to machine reading comprehension, one cannot fail to mention the ', ' (SQuAD). There are plenty of QA or MRC datasets available (', ', ', ', ', ', etc.), but SQuAD is one of the most used. We will not go into details in this blog post, but here are several facts about SQuAD:', 'Examples from SQuAD 2.0 dataset', 'Unanswerable questions (that were added to SQuAD 2.0) require the model to determine whether an answer exists in the document. Further, we will refer to that task as the ', '. It turned out that this is a rather challenging task, as it requires a deeper understanding of the context. As stated in the ', ': “a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0”. ', 'SQuAD dataset has several limitations:', 'The main problem of almost all available QA datasets (SQuAD, NaturalQuestions, and others) is that they contain many easy examples and similar questions between the train and test data. Also, the models trained on one big QA dataset don’t perform so well on another. The high score that we see at the leaderboards near “human performance” results doesn’t mean that the models are learning reading comprehension.', 'We used SQuAD 2.0 to train a baseline model and help with some of our experiments. Despite its limitations, SQuAD is a well-structured, clean dataset, and still a good benchmark. It showed pretty good results on out-of-domain data, and there are many pre-trained models and various research papers around it.', 'The attention mechanism has become one of the key components in solving the MRC problem. In 2017, Stanford Attentive Reader used BiLSTM + Attention to achieve 79.4 F1 on SQuAD 1.1, then ', ' used the idea that attention should flow both ways — from the context to the question and from the question to the context. There were many other solutions based on attention mechanisms, but the last few years’ key breakthrough was achieved with Transformer architectures like BERT. Currently, the latest models solve this pretty complex task really well.', 'Of course, even the best models are not truly intelligent, you can easily find examples where they fail, and there are also limitations of the datasets described earlier. But, as an instrument for question answering tasks, these models already have a good quality, and they can surprise in some cases.', 'The task that involves finding an answer in multiple documents is often referred to as open-domain question answering. There are two main approaches to such systems: retrieval-only and using MRC models. The MRC-based usually includes several stages:', 'Such an approach has several disadvantages. First of all, you have several models, so no matter how well MRC works, it still depends on the retrieval stage results. Another significant limitation is the MRC performance. For example, BERT-base models may be slow even for ten documents per request. On the other hand, smaller models may not be good enough, but it depends on the desired quality and your requirements. ', 'Another approach is to reduce all those components to a dense-retrieval task when we vectorize documents and query separately (i.e., we don’t use query-document attention), and then use kNN search. Even though we discussed that research has moved towards applying the attention mechanism between query and document, solutions without it still can produce good results, especially when we want to find long answers (sentences). Recently, we started to build search engines this way, you can find more information ', '. This post will focus on the multi-stage approach, especially on how we can adapt the MRC model to our domain.', 'Despite task complexity, question answering models usually have a simple architecture on top of the Transformers like BERT. Essentially, you just need to add classifiers to predict which tokens are the start and the end of the answer. The model’s input is constructed from the query and the document, where a separation token and segment embeddings help the model differentiate a question from a document. ', 'If predictions of the start and the end token point to CLS token, then the answer does not exist. In practice, such an approach doesn’t always work very well. You can find various solutions where a separate classifier or a more complex architecture is used to improve the “no answer” classification. In our case, we also faced the problem when one version of the model is better at the answer extraction task, while another at “no answer” classification. If you are not familiar with Transformer architecture, concepts like attention, CLS token, segment embeddings, we recommend reading more about ', '. ', 'Autoregressive and sequence-to-sequence models like ', ' and ', ' also can be applied for MRC, but that is beyond the scope of our story.', 'Most of BERT-like models have limitations of max input of 512 tokens, but in our case, customer reviews can be longer than 2000 tokens. To process longer documents, we can split it into multiple instances using overlapping windows of tokens (see example below). ', 'Sequence length limitation is not the only reason why we want to split the document into smaller parts. Since attention calculations have quadratic complexity by the number of tokens, processing the document of 512 tokens is usually very slow. In practice, the window size of 128...384 tokens contains enough surrounding context to extract the answer correctly in most cases. It is computationally beneficial to process several smaller documents instead of a single big document in many cases. ', 'So there is a trade-off between window size, stride, batch size, and precision, where you should find the parameters which will fit your data and performance requirements. Transformer architectures, like ', ' and ', ', deal with long documents more efficiently, but they are usually faster only for documents with a size starting from ~2000 tokens.', 'Whether you will use a pre-train model or train your own, you still need to collect the data — a model evaluation dataset. Collecting MRC dataset is not an easy task. To prepare a good model, you need good samples, for instance, tricky examples for “no answer” cases. Additionally, not only you should label the answers, but also prepare questions if you don’t have them (compare this to tasks like classification and NER). Preparing questions can be extra challenging for exotic domains, which may require domain experts. In our case, the field is not that specific, yet many questions can be written and understood only for people with some photography experience. We decided to label a part of the test data ourselves (without services like MTurk) to understand the process’s challenges better.', 'Luckily, our case was not that hard because we already have customer’s Q&A, so we don’t need to write questions. However, we can not just use Q&A data to train/test the model, and then use it for everything else in our domain. The first reason is that customer reviews are very different from Q&A data in terms of content and document length. Another problem is that many Q&A pairs have answers biased to its question, and not only because of lexical & syntax similarity.', 'To collect better samples, we did the following:', 'During labeling, we faced the following challenges:', 'We also built our tool to label the data faster because we didn’t find any convenient tool on the market \xa0(except ', ' which can be adapted to MRC task via NER labels).', 'Here are several examples of questions from our dataset:', 'Can we generate such questions using some rules or deep learning models like GPT or T5? The answer is “yes, and no”. You can find several works describing such an approach, but we didn’t see any convincing results. We believe that it is not feasible to generate really good questions automatically, and such questions will be different from real-life examples. ', 'In general, even synthetic questions may help improve your model as an additional source of data for training, and such an approach can also be used for data augmentation. To generate the question either by rules or generative models, you first need to extract possible answers (e.g., using NER models).', 'Helpfully, there are plenty of models pre-trained on SQuAD 2.0 with different architectures and sizes at the 🤗', ' . Let’s check how well they perform in our domain (using our labeled 1000 examples):', 'You can see a clear correlation between the metrics and the model size. That is typical for the Transformers world — bigger models show better results. You can also see two types of F1 and EM (exact match) metrics in the table. Since “no answer” classification is a pretty challenging task, we are often faced with the problem that some models are good at the “no answer” task (', '), while others at the answer extraction (', '). So, it makes sense to keep an eye on both objectives separately. For the answer extraction task, we should track both F1 and EM metrics as well: even though EM is more precise, it’s difficult to label all possible variations of the correct answer. That means that EM will be underestimating the model performance. On the other hand, F1 is prone to overestimate the model performance in some cases (see examples below).', "Although the results are far from ideal and noticeably lower than the SQuAD dataset results, the largest model shows pretty good results out of the box. However, it's so slow that we can't use it in production even with GPU acceleration. RoBERTa-base and ALBERT-base models don’t perform that well, and they are also not fast enough for our case, mostly because we work with long sequences (more than 256 tokens), and we would like to process ten such documents per user request in our pipeline. Still, employing various optimizations and GPU-accelerated servers *-base models can be suitable for many cases, including ours. Let’s try to improve the models quality for our domain.", 'Does domain adaptation matter for QA tasks? After all, the model trained on the SQuAD dataset already answers many questions correctly. If we look at how we usually answer the questions like “', '”, “', '”, “', '”, it often depends on various domain-independent knowledge about the language, especially in case of syntactic and lexical similarity between a question and an answer. There are questions for which you can find an answer without understanding some words in it. But in reality, not all documents and answers are similar to a question in such a way, so domain-specific lexical knowledge is still also necessary for many questions, and the SQuAD dataset also doesn’t cover all the cases. Thus, we still need to enrich a language model with our domain knowledge and fit the model to our questions and documents.', 'In modern NLP, we use ', ' techniques to adapt the language models (LM) to a target domain and task. There are usually two main steps: ', ' on unlabeled data and then ', ' for target tasks using a labeled dataset. The LM pre-training step helps a lot with ', ' when you can achieve good results using fewer labeled samples. But even if your LM has enough knowledge after pre-training, you still need to add task-specific layers on top of it and adapt them to a target task as well, and here you still need enough labeled data (that is one of the limitations to achieve even better sample-efficiency during fine-tuning ', '). ', 'Let’s investigate how we can adapt the model to our photo & video cameras domain using unlabeled data, and what is possible to achieve with a small labeled dataset. For our project, we had ~1GB of raw text data related to the photo & video cameras domain, and ~120K unlabeled Q&A pairs.', "First, let's check if pre-training LM on in-domain data will help us. In our case, we had ~1GB of raw text, which is not enough to train the LM from scratch, but could be enough to take a pre-trained language model and continue pre-training it on domain-specific data. After we adapt LM using a masked language modeling objective ", ', we need to fine-tune it for the MRC task after that, that’s where we need labeled data. ', 'We can still use the SQuAD dataset to fine-tune task-specific layers, but since this dataset does not overlap with our domain, it probably can not outperform existing models trained on it. Even more, training on the SQuAD dataset may lead to a ', ' effect (of what we learned during pre-training). However, several studies have shown that the knowledge gained at the lower layers of the Transformer does not change dramatically during the fine-tuning on an MRC task (other tricks like discriminative fine-tuning and gradual unfreezing also can help here). ', 'We also have our small test dataset (1000 labeled examples). The dataset is still tiny to fine-tune task-specific layers, so additional fine-tuning on SQuAD may help here, and then we can fine-tune the model on our test dataset using cross-validation. Let’s perform several experiments with RoBERTa-base.', 'As we can see, additional LM pre-training improves the model. Now let’s take these models to fine-tune them using cross-validation (1000 examples, mean of 4 folds shown in the table):', "Although we cannot directly compare the models’ performance on our full test dataset with the mean of the folds, it still shows us that we can improve our model even with a small dataset. And we see again that additional LM pre-training using in-domain data helps in our case, even with fine-tuning on SQuAD after that. We also tried cross-validation without the SQuAD dataset, but the model didn't show good results as expected. But even in those experiments, the model with additional LM pre-training was better.", "Using the same approach as for creating the test dataset, we collected additional ~30K question-document pairs from customer’s Q&A and reviews for which we don’t have labels. One of the obvious ideas is to use a pseudo-labeling technique, which didn't give us noticeable improvements. But the less obvious idea is to apply a knowledge distillation to our problem. ", ' is usually used as a model compression technique when we have a model, but it is not fast or small enough. Knowledge distillation is a technique in which we train a smaller model (', ') using output probabilities from our primary larger model (', '), so a student starts to imitate its teacher’s behavior. Rather than training only with a cross-entropy loss over the hard targets (ground-truth labels), we also transfer the knowledge from the teacher to the student with a cross-entropy over probabilities of the teacher. The loss based on comparing output distributions is much richer than from only hard targets. The idea is that by using such “soft” labels, we can transfer some “dark knowledge” from the teacher model. Additionally, learning from “soft” labels prevents the model from being too sure about its prediction, similarly to a label smoothing technique.', 'Knowledge Distillation is a beautiful technique that works surprisingly well and is especially useful with Transformers. Bigger models often show better results, but it’s hard to put such big models to production. And knowledge distillation is one of the main techniques to help here.', "Now, let’s understand how we can apply distillation for our case. As we saw earlier, ALBERT-xxlarge showed the best results on our dataset, so it will be a teacher model. We don’t have labels for our 30K examples, but we can just remove part of the loss which uses labels. Sure, we will inherit more teacher mistakes during knowledge distillation without ground-through labels, but we don’t have models better than our teacher at the moment, so even to get similar results on a smaller model would be useful for us. Let’s try to distill the knowledge to our RoBERTa-base fine-tuned on in-domain data (we didn't use ALBERT-base because there were no fast tokenizers and ONNX support for ALBERT architecture at that time).", 'As you can see, we got RoBERTa-base with F1/EM close to its teacher ALBERT-xxlarge, which is also much slower than our RoBERTa, and we didn’t use any labeled data for training. Of course, it doesn’t mean we don’t need labeled data anymore, the score is still far from ideal, and we also inherited teacher mistakes, so adding labeled data may improve this training procedure.', 'We can also think about distillation as an additional pre-training step to achieve better sample-efficiency when using labeled data. Below, you can see that a knowledge distillation helps in our cross-validation experiments as well.', 'The better teacher you have, the better student you can probably train. So, one of the main directions can be improving the teacher model. First of all, distillation training procedure allows us to effectively use an ensemble of teachers and distill them to a single smaller model. Another exciting technique is the ', ' when we use the same model architecture for both student and teacher. Self-distillation allows us to train a student model which will perform better than its teacher. It may seem strange, but because a student updates its weights learning from the data that the teacher didn’t see, this can lead to better performance of the student (of comparable size) on the data from that distribution. Our experiments also reproduced this behavior when we applied self-distillation for ALBERT-xxlarge and then used it as one of our teachers for further knowledge distillation to RoBERTa-base.', 'Knowledge distillation as pre-training using unlabeled data is not a novel idea, and it already showed good results in both computer vision (', ', ', ') and NLP (', ').', 'Worth to say that even though we didn’t label any data for distillation, we still had questions, which is not always the case. But as we mentioned earlier, we didn’t use the original Q&A pairs. Instead, we collected these pairs from independent Q&As and reviews using the pre-trained USE-QA model. In such a way, we can approach the further model improvements in production. After we start collecting real questions asked by users interacting with our system, we can find candidate documents for these questions in the same way, and use this dataset for knowledge distillation without labeling many examples.', 'RoBERTa-base model was our choice in terms of a trade-off between quality and speed, but it is possible to distill the knowledge even into smaller models like DistilRoBERTa. As we said earlier, RoBERTa-base is still relatively slow, so we applied various optimizations (most of them are well-known), which allowed us to achieve more than 3.5x acceleration. Here is the list of things which worked well for us:', 'Besides knowledge distillation, there are other popular techniques, for instance, pruning. For computationally efficient ', ', better to prune entire attention heads or even layers. Yet pruning is more beneficial when your goal is to decrease the model’s size, not to accelerate it. The pruning could also be easily applied after model training by analyzing which weights are less important. In our case, we were able to prune ~15% of attention heads without a significant drop in accuracy. ', 'There are also various task-specific model optimizations. For instance, ', ' which is mostly applied for a classification task, and ', ' for a document ranking model. The second one requires pretty simple modifications to pre-trained BERT or RoBERTa architecture, and can significantly accelerate the documents re-ranking model in the question-answering pipeline.', 'We also tried to apply that to our MRC model, where we removed attention (before fine-tuning) between question and document for 1-5 layers without a significant drop in accuracy, but it gave us only ~1.15x acceleration, and only for documents longer than 256 tokens.', "Generally, we can achieve a low latency, but high throughput for long documents is difficult to achieve even with DistilRoBERTa. Moreover, in our case, we need to apply the MRC model to 5-10 documents per user request. That's why retrieval and re-ranking models are also important. If they allow us to retrieve smaller chunks of a document more accurately, we will feed the MRC model with a smaller number of shorter candidates. ", 'However, for high load systems, it makes sense to consider less complex models and retrieval-only approaches (without the MRC model). You can also implement two types of question answering systems - slower and more accurate vs. faster and less accurate. That will allow you to balance the load between them or use a "slower" model in near real-time mode. You can immediately return the answer provided by the faster QA system, then calculate the answer using more complex models in the background, and provide high-quality answers to similar questions next time.', 'In this blog post, we described how the machine reading comprehension can be used for building question answering systems over a set of documents. But the same model can benefit other systems in our photo & video cameras domain. ', 'The first application we discovered is that we can use MRC as a universal information extractor. Why can only users ask questions to our model? We can predefine a set of questions to extract the information for other tasks automatically. One such example is the automatic extraction of the pros and cons of particular products from customer reviews. When a user opens a product page, we can show such information extracted by our MRC model in a separate block.', 'Another native place for MRC is a conversational system, where we can replace a bunch of NLU models with a single MRC model. It can be especially effective for product information questions and FAQs. A less obvious application is using MRC as a NER model, asking the questions to extract information from user utterances (see example below). Such an approach may not be efficient when you have many types of entities, and doesn’t work so well for some entities as the classic NER model, but still a pretty interesting example of zero-shot NER.', 'Speaking about photos & cameras, and other similar products, not only Q&A and review may have useful information. Nobody reads user manuals any more: when facing with the issue, we tend to google the manual and find the answer there. So, user manuals can be also put into a question answering system along with conversational system to provide a user-friendly access to user manual data.', 'In this blog post, we took a dive into question answering systems and, in particular, machine reading comprehension models based on Transformer architecture such as BERT. Nowadays, Transformers allow us to build advanced NLP-powered applications and achieve good results with fewer labeled examples. And even though many of these models are huge, modern GPUs and techniques like knowledge distillation allow us to use them in production.', 'Worth to say that besides the MRC model, there is one more critical component that selects relevant documents for MRC. Even though we didn’t touch that topic in this blog post, document ranking is as crucial as the MRC model. No matter how good the reader model, the system will not return good answers if you feed it with not relevant documents.', 'Besides the classic multi-stage (ranker + reader) approach to question answering systems, fully retrieval-based methods are a strong alternative that could provide competitive results for some cases and could be more suitable for high-load applications. And we also successfully use such approaches to build ', '.', 'We believe that question answering models could be a great addition to many information retrieval systems and benefit many areas where people work with domain-specific documents and need efficient tools to work with them. We are watching with curiosity in which new places these systems will show themselves, and if you see such places in your domain, we will be glad to discuss it with you.', 'Transfer Learning and Transformers:', 'Question Answering overview:', 'Analyzing QA datasets and what Transformers learn from them:', 'Pruning, distillation, quantization:', 'Speedup BERT re-ranking models:', 'Open Domain QA:', 'Frameworks to train MRC models:'], 'date': '\r\n\t\t\t\t\t\t\tOct 15, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['natural language processing', 'NLP', 'question answering', 'machine reading comprehension', 'MRC', 'SQuAD', 'transfer learning', 'Transformers', 'bert', 'albert', 'roberta', 'knowledge distillation']}, {'title': 'How to use suffix arrays to combat common limitations of full-text search', 'article url': 'https://blog.griddynamics.com/using-suffix-arrays-to-fix-limitations-of-full-text-search/', 'first 160': ["Most full-text search engines don't handle this problem well due to how they index documents, leading to poor performance and inefficient use of of patent officers' time. In this article we will discuss the problem and present a solution in the context of Lucene-based engines", 'What is it about long, full-text, compound data that search engines find hard to cope with? Well, most search engines, including those based on Apache Lucene, operate in a manner analogical to the back-of-the-book index: data is listed alphabetically. As such, they can handle search according to the prefix only. This causes major problems with processing phrases that are long and contain compound data, like in the case of patent files and filings.', 'The standard way of running a search on this type of query uses a method called ', '. This requires the introduction of what is called a ', ' — the asterisk\xa0', " to replace any unknown parts of the phrase. Unfortunately, this method doesn't work well in Lucene because of how data is indexed. It results in sluggish processing times of at least several seconds, ", ' longer, depending on the index size and the number of entries.', 'Typically, people rarely use leading asterisks when searching, and waiting for several seconds in most cases seems fine, so this issue may not seem hugely problematic. But, when a new patent filing needs to be checked against thousands of existing entries, all of which have long, compound, technical names, the delay can be costly. Searching through thousands of complex records can lead to major delays, mistakes, and high inefficiency in the day-to-day work of patent officers.', 'Here are some examples to give you an idea of what types of elements can be part of such a query:', 'Notice that we’ve used asterisks to indicate the wildcard on each side of the search phrase. This means that the part of the phrase we’re using as a keyword is located somewhere in the middle of the indexed entry.', 'There are a few workarounds to the leading asterisk method. For example:', 'In some cases, this functionality is just left as it is, because of time limitations during programming. This means that there is no solution implemented, despite the awareness of the issue and the lack of well-functioning alternatives. As a result, users are forced to perform a wildcard query based on the Finite State Tranducer (FST), and face possible performance issues.', 'None of the above solutions truly solve the problem, especially the last one, which ignores it altogether. But, as we know, fast processing of complex phrases is a must for certain queries and business cases, like for the patent office example described above.', 'To cope with this problem, Grid Dynamics has successfully implemented a suffix array data structure for one of our clients. We set out to implement a universal solution that fixes the leading asterisk search performance problem out of the box. Let’s get into how it works.', 'We need to begin by explaining what a suffix array is. In general, a suffix array for any string set is a sorted list of all the suffixes for that set. In this context, a suffix is broadly understood as any element that comes after the first element.', 'As an example, let’s build a suffix array for one string – ', '.', "We will begin by appending a null symbol to the end of the strings. For this example, we're going to use the “$” symbol to signify null. We do this so that the search understands where a given suffix in the array ends. Therefore, we can define the following suffixes:", 'Now, we need to assign a number to each suffix.', 'Let’s sort these suffixes in alphabetical order. Remember, Lucene-based search works like an index in the back of a book.', 'Lastly, to aid the storage optimization process, we will save order of the suffixes.', 'In the chart below, the "Position in array" row reflects the number of the position in alphabetical order (as seen in the chart above). The "Number of suffix" row refers to our original numbering, seen in the first chart. We end up with this result:', '\xa0', 'To implement this solution, we\'re going to follow the methodology described above for each term in the index. This means we need to index all unique terms in all document. It entails concatenating all the terms together in a big string, and separating them with the null symbol (once again, for clarity, we\'re going to use the "$" sign). The result is a big suffix array that contains each term in the index.', 'The question remains: how will a suffix array help us find all the words containing a given string in the middle of the word? Let’s look at the following example to gain a better understanding.', "Let's suppose we have a few terms: port, sort, test, vest, rest. This would be the concatenation of all the strings: port$sort$test$rest$. And the suffix array would look like this:", 'If we query a partial term with a leading asterisk, we can find all the suffixes that start with that part. As long as all the suffixes are sorted, this can be easily accomplished with a ', ' of logarithmic complexity. Our search will show all the ', ' from the index that match our given wildcard, not just their suffixes.', 'Let’s see how this works in practice. Given the data above, we will query the partial term "or" surrounded by asterisks. Our query will look like this: ', '.', 'In order to do this, we need to perform a binary search in this array, to find the lower bound:', 'To find the upper bound:', 'The suffix array causes this search to yield results at a blazing fast\xa0speed, \xa0as the search now looks for the position of an indexed term in the array, rather than for the phrase itself.', 'It’s important to note that this solution requires some additional resources. Generally, a symbol in the Lucene index is represented by one byte. The suffix array approach uses this byte in the string with all the words, and also uses integer data type (', ') to represent the suffix starting with the given letter. ', ' is represented by four bytes, so the suffix array stores at least five bytes per each letter for every unique term in the index. The overall overhead for the construction of the suffix array can be difficult to calculate, as there are a lot of factors to consider.', 'One way to get around this issue is by using a ', ', which helps eliminate some RAM limitations by reducing the size of the suffix array. Additionally, constructing a suffix array takes a lot of time, so we use a special thread pool to do it.', 'In order to demonstrate the effectiveness of this solution, we ran a series of tests to compare the processing speed against the classical FST wildcard methodology. For the technical comparisons, we used dump files from Wikipedia. We stored only title and full-text fields, and used only the full-text field for querying. The queries consisted of two or more letters with asterisks at the beginning and the end of the queried term. The cache was never in use, to prevent the responses from caching.', 'The Solr was built based on trunk (7.0.0-SNAPSHOT plus the suffix array patch). We set Xmx to 8G. All the testing was done on i5-4300M, 64bit Fedora 25 (Linux core - 4.9.14). The results were measured in milliseconds (ms).', 'Here are the results:', '40k documents from Wikipedia with the total index size of 600 mb. All measures were completed after the initial run (e.g. warming up the JVM).', 'Random queries of size 3:', '\xa0', 'Random queries of size 4:', '\xa0', '400k documents with the total index size of 2.78 Gb. All measures were completed after the initial run.', 'Random queries of size 3:', '\xa0', 'Random queries of size 4:', '\xa0', 'Full queries of size 3 (all combinations from “aaa” to “zzz”):', '\xa0', '2 separate threads, doing queries of size 3:', '\xa0', '3 separate threads, doing queries of size 3:', '\xa0', 'As you can see, the implementation of suffix arrays had a major impact on the query processing speed, with response times significantly dropping down - up to 482 ms on average.', 'In this post we explained how implementing a suffix arrays can be workaround for speeding up compound string queries for a Lucene-based search engine. This approach has many real-life applications and has already been successfully implemented by Grid Dynamics for some of our clients. We have also submitted our patch to Lucene JIRA ', ' and received extremely valuable feedback from the community.', 'As always, all our technology blueprints are open. If you have issues with similar use cases, feel free to use this patch. It’s especially helpful in tests where your query is at least 3 symbols long - you will get extremely fast responses.', 'Don’t forget to subscribe to our blog to get the latest open-source blueprints for search, QA, real-time analytics, ', ' and more. And feel free to comment below.'], 'date': '\r\n\t\t\t\t\t\t\tJun 27, 2017\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tYakov Sirotkin', 'author_url': 'https://blog.griddynamics.com/author/yakov-sirotkin/', 'tag': ['Search']}, {'title': 'How to implement autocomplete search for large-scale e-commerce catalogs', 'article url': 'https://blog.griddynamics.com/implement-autocomplete-search-for-large-e-commerce-catalogs/', 'first 160': ['A customer of ours, one of the largest omni-channel retailers in the US, was having issues with product discovery. Their e-commerce site had a massive catalog with hundreds of thousands of SKUs, a modern e-commerce backend and a powerful search engine, yet the conversion was less than stellar. Frustrated customers often couldn’t find what they were looking for, even when the retailer had the goods.', 'The issue was broadly related to poor autocomplete features, which made product discoverability unnecessarily complicated. For example, these were common issues that plagued the user experience:', 'The search engine itself didn’t have sufficiently sophisticated autocomplete functionality built-in, failing to address such problems as:', 'The customer was faced a dilemma between two unappealing options: to commission an expensive and technologically complex heart surgery on the existing search engine, or replace that search engine altogether with another one that had autocomplete features.', 'Luckily, there was a third choice: to provide autocomplete functionality as an add-on service, that would integrate with their existing search engine and e-commerce back-end. This is one of our company’s specialities, developed over the years as a result of many generations of  large-scale implementations of e-commerce search engines for big retailers. Here is how we did this. You can also see a demo of this solution below.', 'Our approach is to create an autocomplete service that extends a retailer’s search bar with new features that are search engine-agnostic and do not require any customizations with the backend or search engine index. Furthermore, the quality of these autocomplete features matches that of those found in current state-of-the-art search catalogs offered by Google and Amazon.', 'To build this sophisticated suggestion catalog service, we combined two powerful ideas:', 'The first technique, catalog-based suggestion generation, functions to transform unnatural and often “synthetic-like” suggestions generated from a catalog, into more relevant and applicable suggestions. Catalog indexes can grow to be very large with large numbers of SKUs and additional product attributes. Therefore, to tune relevancy, we simplify and edit the product attributes to remove non-business related attributes that likely would not have been searched for in the first place. While this process may limit the number of product attributes, it is advantageous because it can be applied to any product catalog independent of the presence of user query statistics.', 'The second technique, most frequent user query-based suggestion generation, utilizes a more data-driven approach. This technique bypasses the manipulation of large catalog indexes and maintains a high number of product attributes, but is restricted by its requirement of the collection and possession of most frequent user query statistics. Additionally, this technique employs the search engine directly during suggestion index generation, whereas the catalog technique does not.', "Both techniques outlined above present their own advantages and disadvantages in terms of the quality of the generated suggestion sets. We have opted to implement both of these techniques to maximize the solution's user experience.", 'In order to address the relevancy of catalog-based suggestions independent of user query statistics, this technique processes data using a multi-step algorithm and produces index data on generated suggestions.', 'The solution is based on the following steps:', 'Finally, all of these suggestions are indexed into Solr using a structure such as the one presented below:', 'The “input” field stores the original text of a generated suggestion, “length” stores a number of words inside the suggestion, and the others match the description above.', 'For the input field, we don’t have to use EdgeNGramFilterFactory or other filters that split the phrase into a list of shingles anymore. A simple field config like the one posted below is sufficient:', 'In order to provide suggestions with only 1-2 words added to the original user input, we perform grouping on the “length field” and sort groups by length increase. Inside groups can be sorted into suggestions by their importance and ranked according to specific needs. It is important to note that for inside groups, the prefix input needs to be matched manually by building a special query with a PrefixQuery match on the last entered word.', 'Grouping the resulting suggestions allows the elimination of duplicates. Business-rule filtering eliminates indistinguishable and nonsensical suggestions. And the fact that we build our index on the actual catalog ensures that there will be no dead ends.', 'As a result, we eliminate all scenarios of naive catalog-based suggestions, while still supporting all the necessary business rules. This requires a lot of effort on large catalogs so the fact that our solution scales to very large catalog sizes is vital.', 'Our solution also includes a second technique that uses data to improve the quality of suggestions. This technique may be restricted by the requirement that statistics for user query frequencies have been separated into linguistic groups, but is still worth adopting. This technique is implemented by grouping all semantically equal suggestions (e.g. “prom dresses” should be given equal weight as “dress for prom”), with a normalized number of user queries for the text (rank value). If the collection of raw statistics isn’t trivial, then groupings like this may require additional complex data analyses, like tagging, however in most cases native search engine features are sufficient.', 'This technique is based on the following steps:', 'Finally, all of these suggestions are indexed into Solr using a structure like that presented below:', 'As you can see, the resulting suggestion structure is nearly the same as for the catalog-based suggestion technique. However, due to the nature and sheer volume of data used, this technique provides a superior ranking system. Once the suggestions are indexed into a search engine, they can be queried using the same algorithm described for in this article for the catalog-based technique.', 'In combining these two techniques, we were able to successfully create an autocomplete feature for the search engine that used both user statistics and a cleaned index to generate more relevant results. Even though we did need to work with an extremely large index and a large volume of user query data, we were able to build the solution without altering the search engine or messing with the customers backend, simplifying the project and lowering its cost.', 'Going the extra mile and using two techniques to create highly relevant search suggestions paid off enormously. After our solution went into production, the customer conversion rate went up by 5% overall and 9% on mobile devices. This was especially promising as it showed that a Solr based search engine using only the basic index based approach, would not solve the problem as it wouldn’t provide the same quality of search suggestions. Overall, our extra features saved the customer money on the implementation and improved their user experience.', 'The challenges we outlined in this article that our customer experienced in terms of, struggling to provide an adequate search experience and experiencing low online conversion rates, are not exclusive to just this one retailer. Competition in the digital retail space has become increasingly more intense, requiring every digital retailer to continually innovate to keep pace with the ever-rising bar of customer expectations. Furthermore, increasing catalog sizes requires more advanced product discovery to continue to allow retailers to provide a complete omni-channel experience. Like our customer, many large retailers are facing this rise in standards and are expected to have the whole breath of their inventory available in their online catalog. This means that they will need to be improving their search features, namely begin implementing autocomplete.', 'The struggles that large retailers face with attempting to implement autocomplete search functions can be solved with the same solution that worked for our customer. Our solution is an agnostic search engine add-on and therefore, is compatible with any existing search engine such as Solr, Endeca, Elasticsearch and many more. Furthermore, our solution doesn’t conflict with the backend and is equipped to handle even the largest volume e-commerce catalogs. This solution can achieve Google and Amazon quality autocomplete with just two key steps, catalog index customization and integration of the digital store’s search bar. Grid Dynamics is a specialist at these kinds of autocomplete implementations and our team is happy to assist any retailers struggling with this technology, just drop us a line ', '.'], 'date': '\r\n\t\t\t\t\t\t\tFeb 14, 2018\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVladislav Trofimov', 'author_url': 'https://blog.griddynamics.com/author/vladislav-trofimov/', 'tag': ['Search']}, {'title': 'Open Banking as a Driver of Digital Transformation in the EU Banking Industry', 'article url': 'https://blog.griddynamics.com/open-banking-as-a-driver-of-digital-transformation-in-the-eu-banking-industry/', 'first 160': ['In 2015, the European Parliament adopted a reformed Payment Service Directive, also known as ', ", which can be considered as the starting point of “open banking”. The new rules aimed to promote the development and use of innovative online and mobile payments through opening consumer banking data up to third party providers (TPPs) in a secure way. In other words, banks and building societies allow access and control of customers' personal and financial data to third-party financial service providers such as retail businesses, telecommunications providers, payment services, financial account aggregators, and others.", 'By providing access to the banking data of consumers, this allows TPPs to develop and offer a range of new financial products and services for the benefit of consumers. The ultimate result of the changes is that the world of financial services will be opened up to more than just banks, allowing fintech and other innovative upstarts to create new solutions based on the data that was previously the exclusive domain of banks.', 'While the new regulations and the open banking paradigm gives clear benefit to TPPs, what opportunities does it offer to clients and the banks? Firstly, open banking allows clients to centralize their financial management into a single point for all their accounts. Through open banking clients can also get access to a far broader set of capabilities than banks can provide on their own. It could be a personalized quote for a loan based on a detailed history of income and spending, a debt management tool with overdraft alerts, recommendations for better products with lower interest rates, or a tool that gives recommendations to save money based on analysis of their fixed payments and variable spend.', 'Clearly, customers may be concerned about sharing their account details and spending history with TPPs, which is why the open banking legislation has a strong focus on prioritizing customer data protection. Implementation of the open banking changes is strictly regulated and customers are asked to provide an explicit consent to any company wishing to access their data. All data is encrypted and its usage is tracked.', 'Banks mainly benefit from collaboration with third-party services in improving customer engagement by providing a better and smoother experience, as well as building and selling more innovative financial products. The changes that open banking brings provide a very promising opportunity for the financial services industry. According to the ', ' conducted by Tink, before 2020 many financial executives in Europe approached open banking from a purely compliance perspective — in particular to meet the set requirements of PSD2. But as we begin a new decade, open banking investments have become central to the digital transformation of the industry itself.', 'However, implementation of the PSD2 regulation compliance and open banking concepts do require major investments by the banking industry with a median estimated investment between 50-100 million euros. To better understand where the required complexity comes from, let’s take a closer look at the PSD2 and open banking requirements. ', 'In the open banking approach financial data is shared by banks through standard-compliant APIs. The data can be shared with TPPs of two classes: Payment Initiation Service Providers (PISPs) and Account Information Service Providers (AISPs). PISPs initiate payments on behalf of a user dealing \xa0with the actual transfer of money in the form of Peer-to-Peer (P2P) transfers or bill payments, while AISPs can only \xa0analyze a user’s spending habits.', 'PSD2 requires the restriction of access to personal and financial data by third parties until the explicit consent of the client is obtained in a secure manner. Before the open banking approach, TPPs typically used screen scraping to gain access to customer financial data. That approach obviously had a high risk of fraud because a customer had to share their account credentials with a third party application. The open banking approach implements a principle of communication between Account Servicing Payment Service Providers (ASPSPs), users, and TTPs based on consent and trust. This very important part of the standards is known in PSD2 as the Strong Customer Authentication (SCA) requirement, which requires two-step authentication for getting access to account information or executing payments.', 'The European Business Association (EBA) has published the final draft of the Regulatory Technical Standards (RTS) for requirements on SCA and common and secure communication under PSD2. It is important to note that the RTS doesn’t actually provide an explicit set of established rules. Instead, various standardization bodies in coordination with the EBA have created a set of binding frameworks and options based around market standards. These effectively act as a guideline for implementation. For banks, this can be seen as both a positive and a negative. It is advantageous in that it allows for a greater variety of technological innovation pathways to be explored. However, it also means that banks and financial institutions have to work out the specific design and form of workflows and items themselves. For many banks and TPPs, especially smaller institutions, this is considered to be quite an onerous undertaking. Further details and use cases are still to be finalized and will be determined soon.', 'Another challenge is that the EBA doesn’t provide a concrete definition of PSD2 interfaces. In addition, \xa0the open banking API standards in the EU (as well as world-wide) could be specific for different countries and even for groups of banks - any organization that participates in the open banking implementation is regulated by a country-specific authority. It has explicitly left this up to the market and emerging bodies. Even the guidelines for PSD2 APIs are provided by a variety of different standardization bodies. For example, financial institutions in Germany use the NextGenPSD2 XS2A framework of the Berlin Group as the main standard, STET is applied in France, and in the UK the UK Open Banking Standard is used, which is overseen by the ', ' (CMA) and ', ' (FCA).', 'As a result, implementation of open banking touches a variety of concerns such as handling SCA and exceptions, customer consent management, TPP transactions monitoring, customer authorization options, and managing certificates issued to TTPs. On top of that comes uncertainty surrounding the ongoing evolution of the PSD2 and RTS standards, which requires that open banking solutions are ready for future changes.', 'Taking into account the challenges outlined above, how exactly will banks and financial institutions in Europe approach the digital transformation of their services towards PSD2 compliance and open banking? The main industry players have all taken different paths, but one of the fundamental decisions that each institution must make is how to integrate existing systems into the new concepts.', 'There are various strategies that can be adopted in regards to integrating into existing systems. One is simply to upgrade the existing system. This strategy is good if the bank’s information system already supports many of the required capabilities meaning the open banking standards can be adopted into the system with only a minimal amount of upgrading required. However, it is unlikely this approach will work for legacy information systems. It is a better fit for institutions running custom built solutions or a hybrid (custom and proprietary) solution with a high level of control over implementation.', 'Another widely adopted approach is to introduce a middle layer that implements open banking capabilities and integrates with the existing core systems at the backend. This is usually the safest and quickest way towards PSD2 and open banking compliance because it provides a controlled and isolated impact on the legacy system.', 'This strategy allows for the onboarding process of the new capabilities to occur in one go or to be introduced incrementally depending on the “build vs buy” implementation strategy. The market offers a large variety of options for either approach. The “build” strategy is supported for instance by RedHat within the “', '” approach, cloud services providers like “', '”, or API management solutions like “', '”. The choice of available “buy” options is large as well and is represented by specialized open banking platforms that typically not only satisfy the regulatory requirements but also enable greater monetization potential via services in the future. It is worth mentioning popular open banking platforms like ', ', ', ', and ', '.', 'Some organizations may take the ongoing global adoption of the open banking paradigm as a trigger for comprehensive modernization of their information systems and services by building a brand new solution (custom or hybrid) in parallel to the legacy one.', 'This strategy gives great flexibility in the choice of the architecture such as custom microservices architecture, innovations, modern technologies, and approaches. This kind of digital transformation allows building a modern and cost-efficient solution by combining open-source technologies, managed services, and Commercial-off-the-Shelf (COTS) products. There is also a good opportunity to collaborate with TPPs for making joint improvements in business processes.', 'Grid Dynamics, being a ', ' in digital transformation for the past decade, notes many similarities between the transformation of information systems in banks towards the open banking paradigm and the rise of services platforms in other industries. This can be applied to both the technology shift as well as to the unlocking of new business opportunities. Providing customers with a modern user experience through responsive web UI and native mobile apps; building cost-efficient distributed scalable backends in the cloud; driving insights from data with machine learning and artificial intelligence; building a high-performance engineering culture, DevOps, and automation to change and innovate faster and cheaper. Does this all sound familiar? These measures all work exceptionally well at elevating businesses as well as delivering enhanced opportunities to the banking industry.', 'So given our extensive history working on digital transformations, what is our viewpoint on the introduction of the new open banking regulations? It is important to stress the word “agility” as a fundamental driver of the decision making on the implementation strategy. PSD2 regulations with defined tight deadlines have already placed a large amount of stress on banks across the EU. Becoming compliant with these regulations has already cost banks millions of euros. However, that is not the end of the story. The growing utilization of financial services through TPPs will dramatically increase the load on banks’ backend systems and the cashflow will grow as well as the amount of data flowing through the bank systems.', 'Being “agile” means making the systems and business processes adjustable and flexible enough to cost- and time-efficiently adapt to changes whether it is a new regulation requirement, a new opportunity, or a growing number of customers and transactions. From that point of view, building a custom modern banking system provides the highest degree of agility. However, it is unlikely to be the optimal case for many organizations taking into account its overall scale and complexity and its time-to-market, which \xa0may negate all its benefits.', 'The opposite approach of building a business on a COTS banking product solves many “early days” problems. When properly considered, it can provide a great boost to business though long term it may become a containing frame for business development and eventually lead to stagnation and falling behind the industry mainstream. We have dealt with such situations ', '. However, not all businesses properly recognize or mitigate the risks involved by implementing the necessary digital transformation steps in the right time frame.', 'The best of two opposing options, as is often the case, lies somewhere in the middle. Incrementally modernizing the banking information system in a hybrid way by combining pieces of custom developed components, some legacy systems, and highly customizable commercial products is usually the optimal approach. Of course, building the right enterprise and solution architecture must be prioritized as it \xa0supports the agile paradigm, defines and outlines the proper building blocks, and enables the introduction and development of new business capabilities. Nowadays it is typical to move along this route involving strategic partners, who accelerate the modernization process by bringing either pieces of a ready solution, or certain technical or domain expertise. Being such a partner for many Fortune 500 companies, we see the strong benefit of establishing a strategic, so called “', '” partnership as it goes beyond the acceleration of digital transformation to growing new business capabilities.', 'PSD2 and the open banking paradigm represent a tremendous shift in one of the most conservative customer-facing industries. PSD2 is intended to make it easier for companies to offer different and innovative services and as it stands in 2020, this has already been a success. In addition, banks have recognized opportunities in this trend that go beyond compliance and have started investing more in the acceleration of digital transformation and modernization of their IT infrastructure and services. Taking into account the ', ' on traditional banking services, these efforts and the building of new capabilities may prove to be crucial for business development and long term success.', 'However, implementation of the new capabilities comes with many challenges, which traditional banking IT systems may not have faced previously. This includes scaling systems to handle the increased external traffic, managing public APIs, tracking access to customer data, collecting and analyzing large amounts of streaming data, and integrating with third-party fintech services, etc. That is where establishing strategic partnerships with companies that are capable of making a contribution in technology or business development may dramatically boost innovation and transformation.', 'The scale and flexibility of the banks’ information systems will dictate how they will approach these challenges. Regardless of whether it is an adoption of a new commercial product, modernization of an existing solution, or building a brand new platform, it is important to validate the approach from the perspective of future changes. It will help to reduce time-to-market and payback periods further and in doing so help to deliver important competitive advantages.'], 'date': '\r\n\t\t\t\t\t\t\tSep 24, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tIgor Kononov', 'author_url': 'https://blog.griddynamics.com/author/igor/', 'tag': ['open banking', 'Digital transformation in banking', 'PSD2 implementation', 'Solution architecture for open banking', 'third-party banking', 'PSD2', 'Payment Services Directive', 'how open bank work', 'what is open banking']}, {'title': 'WSO2 vs GraphQL: overview and comparison of open source API management products', 'article url': 'https://blog.griddynamics.com/wso2-vs-graphql-overview-and-comparison-of-open-source-api-management-products/', 'first 160': ['In recent years, many organizations have provided public access to their digital ecosystems via APIs. This has transformed APIs from being a tool, into being a product. At the same time, wide adoption of microservices has led to an increased number of internal APIs used by organizations as well. ', 'The effect of these changes is that since 2005, the number of APIs has grown from a mere curiosity to an overwhelming trend. This is reflected by the ', ' public-API catalogue, which reports that the total API count has grown from just a handful in 2005 to over 22,000 today as demonstrated in the figure below.', 'With such strong growth of both public and internal APIs, there is a clear need for special tools and techniques to manage the full life cycle of APIs and provide a better experience for API clients. This is where API management solutions play an important role. They can offer a variety of features however, according to Forrester’s definition, they generally include at least three core components: an API user portal, a business admin portal, and an API Gateway.', 'The main goal of this research was to evaluate several popular open-source API management solutions. These evaluations were done with respect to Grid Dynamics’ own requirements and were aimed at allowing us to then decide if we could also recommend them to our clients.', 'In the “', '” report, there is a comparison of many popular API management solutions. The report named WSO2 as one of the leading solutions and as most of its products are open-source we have therefore included an assessment of WSO2.', 'In addition, we have chosen to evaluate the GraphQL stack, which is an emerging data query and manipulation language. GraphQL currently has ', ', who have started development of their APIs using this new architecture style. Some of our clients have also evaluated GraphQL for their use cases, so we already know that there is strong real world demand for this architecture style.', ' is a solution for end-to-end API management in the cloud, on-premise, or in hybrid environments. In our research we’ve focused on the latest version of WSO2 API Manager, which is 3.0.0. and was released several months ago. It consists of six main components: API Publisher (admin portal), API Developer Portal (user portal), API Gateway, Key Manager, Traffic Manager and API Analytics.', 'Even though WSO2 API Manager is an open source product, it offers several pricing options: an open-source version, subscription-based (with an ability to update products with bug-fixes), and a cloud version. We’ve focused here on the open-source version.', 'GraphQL is an open-source query language for APIs that includes a server-side runtime for executing queries over types and fields. These are defined in schemas. It’s commonly \xa0compared to REST, as both of these architectures provide ways to develop web APIs.', 'The GraphQL API is represented as a single endpoint, which is served over HTTP. Clients have the ability to define the exact structure of the data they need in a request query, so the amount of data transferred between the client and GraphQL server is lower compared to REST. The response is usually returned in a JSON format.', 'GraphQL has a specification for a query language and an execution language that describes capabilities and requirements of data-models for client server apps. It includes many ', ' written for different languages, so users can choose the one that best suits their needs.', 'We’ve used Apollo as an example of one of the popular GraphQL specification implementations. This product is maintained by the dedicated Apollo team.', 'Apollo Platform includes a production-ready GraphQL server (Apollo Server), schema management and monitoring tool (Apollo Graph Manager) and client (Apollo Client). It also includes ', ', which is a special configuration of Apollo Server designed to compose distributed GraphQL schemas.', 'Apollo Server is a spec-compliant JavaScript GraphQL server that can be used by any GraphQL client. It’s main goal is to validate and execute queries by using a type system that is defined in schema.', ' is a cloud service that helps to manage, validate, and provide some security features on data-graph. Apollo Server pushes schema and analytics data into Graph Manager when this feature is enabled on the server. Graph Manager offers some basic free features and requires a subscription for others so please check their ', ' for more details.', 'In order to check the main API management solution capabilities, we need to integrate them with some backend services. In our case we did our research based on the following architecture:', 'Here we have API Manager deployed in a separate K8S cluster. In the case of Apollo GraphQL Server, there is also the option to deploy it easily with Google App Engine. We tried both approaches.', 'API Manager solution is integrated with the backend services. In our case, we wanted to check several options within the integration including:', 'The above backend services expose REST, SOAP, and WebSocket APIs. These APIs provide the ability to receive, update, and delete carts-related information based on user ID and item ID request parameters.', 'We also deployed Prometheus to check if we can collect and analyze any analytics data from the API Manager. And we’ve used Auth0 as an Identity Provider in order to check if API Manager can be integrated with a third-party service to support authorization/authentication.', 'WSO2 API Manager can be deployed in a containerized environment. There is a WSO2 docker registry where WSO2 applications images are kept. At the time of writing this article, WSO2 had one ', 'for the 3.0.0 version, with the deployment split between WSO2 API Manager, API Analytics, and MySQL backend pods. However, we will most likely require a higher level of service isolation on production, so the configuration may need to be adjusted.', 'We used Apollo Server as a gateway to our backend services. Deployment was done using Google App Engine.', 'Initially, we had to use apollo-server-express as Apollo Server:', 'Create app.yaml, which is required by Google App Engine:', 'Set required ', ' field in ', ' file:', 'In order to build the GraphQL API, several basic steps should be performed:', 'The schema is a major component of the graph API. It’s a blueprint that describes all of the data types and their relations. It also describes ways in which the clients can fetch data (queries) and how clients can modify data (mutations).', 'A simple model for our REST service would be as follows:', 'Apollo Server is supposed to fetch data from any data source such as REST endpoints, gRPC services, databases, etc. DataSource is a class that’s responsible for fetching data from any backend. It also includes logic for caching and deduplication. There are some common datasource implementations in Apollo libraries for example, clients may use apollo-datasource-rest library, which provides RESTDataSource. For our example, we extended it to implement REST calls to our backend:', 'Resolver is responsible for fetching data (using data sources). It provides the instructions for turning GraphQL operations into data.', 'We’ve defined several features that will help us perform our research and understand what is supported in the products that we’ve chosen for evaluation:', 'In addition, there are some useful facts related to specific API Manager solutions, which we will go into in more detail after evaluating the items in the above list.', 'Now let’s start with our first requirement for API Manager and check if WSO2 and Apollo Platform can help us with its implementation.', 'In using the term “user portal”, we are referring to some kind of UI that enables users to find available APIs and evaluate and subscribe to them. While the term “admin portal” also refers to a UI, in this case we are referring to the ability to publish APIs and specify different limitations such as rate limits or security checks.', 'WSO2 uses its own terminology for user and admin portals. For its user portal, WSO2 API Manager provides Developer Portal, which allows API creators to host and advertise their APIs while allowing API consumers to self register, discover, evaluate, subscribe to, and consume APIs.', 'As an admin portal, WSO2 offers API Publisher, which allows API creators to develop, publish, scale, monetize, and version APIs.', 'Apollo Platform has fewer capabilities here. It provides only limited admin portal functionality in Apollo Graph Manager such as GraphQL schema overview, usage info, etc. It’s important to note that it isn’t safe to use as a user portal, as currently all members in an organization have full permissions set. This means that all users have the ability to remove schema for example, which isn’t a secure solution for a user portal.', 'With regard to user and admin portals, WSO2 API Manager has rich functionality. It contains both Publisher and Developer Portal, each with their own range of capabilities. Apollo GraphQL only contains Graph Manager, which is responsible only for several capabilities of admin portal however, there is no portal for API clients.', 'Here we are analyzing if API Manager solutions are able to integrate with different types of backend APIs. We expect to see support for the most popular protocols, i.e. REST, SOAP, and WebSockets.', 'WSO2 API Manager supports the following types of backend API - REST, SOAP, WebSocket, and GraphQL. In the case of WebSocket, STOMP is not supported. All types of APIs can be restricted for access by roles in Publisher and Developer Portal.', 'WSO2 API Publisher provides two variants for creating APIs:', 'The second variant is preferable as there is no need to manually describe all available backend resources. API Specification can be loaded from the provided URL as well as directly from the schema file.', 'It is notable that WSO2 API Manager provides two options during SOAP API creation - ', ' and ', '. In the case of ', ' WSO2 exposes the SOAP API and will route all requests to the SOAP backend. The ', ' option allows for the exposure of legacy SOAP backends as the REST endpoints.', 'As we already mentioned, corresponding data sources should be used in order to fetch data from the backend. For the REST backend, Apollo provides apollo-datasource-rest library, clients just need to extend the already implemented RESTDataSource.', 'For the SOAP backend, there is no official library. Clients can try to use a third-party implementation, ', ' is an example.', 'As for the websockets, in general GraphQL supports websocket protocols via subscriptions. However, ', ' points out that in the case of clients using Graph Manager, subscriptions should be disabled in Apollo.', 'As you can see, WSO2 API Manager provides a larger range of backend API implementation support and expanded capabilities such as support of API specification. For GraphQL only the REST backend is supported out-of-the-box. For all other APIs, the dev team will need to write their own libraries or find third-party alternatives.', 'In this part of the research we wanted to check if current API Manager solutions support FaaS components as a backend service.', 'In WSO2 API Manager documentation there is no information about FaaS as a backend service support. However, there are some workaround solutions that can solve this problem. This involves creating a separate backend API that can invoke the lambda function and publish it. Alternatively, it is possible to create a custom Message Mediation sequence in WSO2 API Manager and set this sequence to request flow for the definite API in Publisher.', 'In the case where a GraphQL datasource needs to call a cloud function, there is no special logic for that process. Instead, you need to write a datasource that can send the HTTP request to the trigger function. For some cloud providers, tools exist for better integration for example when clients use AWS Cloud with AppSync, they can use ', ' as a datasource.', 'Regarding FaaS (Serverless) components, there is no out-of-the-box support for this kind of service in either API Manager solutions. However, there are some workarounds that can be used to effectively address this issue.', 'In this part of the research, we wanted to understand what the options were for the integration of API Manager with backend services. For example, do we need to define each endpoint location manually or is there some form of automation tool that can help to locate available services?', 'When a new API is created in WSO2 API Publisher you need to define the backend endpoint (or endpoints) of the corresponding services. In WSO2 there are several ways in which this can be performed:', 'As we discussed previously, the definition of services should be implemented directly in the code. Therefore, there is the option of using a real service host/port or Ingress controller name, with the decision being based on the nature of the deployment infrastructure.', 'Both API management solutions offer the ability to define services via real endpoint addresses or by using Ingress Controller service discovery capabilities. Additionally, the latest version of WSO2 provides the ability to automatically publish new APIs by utilising an integration with Kubernetes Operators.', 'For this part of the research, we were looking to understand what kind of contract validations were supported by API Manager solutions. For example, if API Manager checks if new versions of the API have broken backwards compatibility.', 'WSO2 API Manager provides the ability to validate API schema. Validation occurs at the time of schema import. This validation is relevant only to the specification rules (e.g., the OpenAPI definition is validated in accordance with the OAS 3.0 specification), and not related to the changes in the API during new version creation. All versions of the API are independent of each other. Regarding version control, ', ' manager is very useful as it helps to control the state of different API versions.', ' is to avoid API versioning, which we usually use for regular REST-based APIs. The main point here is that clients read only those fields, which they have defined in a request, rather than the entire schema. In this case, most changes such as adding new fields won’t break clients.', 'However, some changes in schema such as field renaming or removal or a change in a field’s nullability will result in a breaking change in the GraphQL API as well. Therefore, there needs to be a mechanism in place to identify those breaking changes and notify the dev team about potential issues. Apollo Platform suggests using Apollo Graph Manager to track schema changes and report if something new may break clients.', 'Graph Manager generates differences between local schema and the one most recently registered. Note however that Graph Manager uses Graph Manager Analytics (which requires a paid subscription) to determine whether new changes affect queries/mutations that were executed on graph recently (the time window is customizable). It doesn’t compare two schemas entirely. So if a field wasn’t requested for some time, there is a chance that it’s changes won’t be reported by Graph Manager. Graph Manager then returns non-zero exit code if breaking changes were found. This means that this check can easily be integrated with CI.', 'WSO2 API Manager doesn’t compare previous and new versions of APIs. Therefore it won’t report if a new version contains breaking changes. However, it has the ability to validate API schema in accordance with some common rules when the API is created.', 'GraphQL doesn’t use versioning but Apollo Platform provides a mechanism to analyze schema changes and report if something is broken. This feature must be integrated with \xa0Apollo Graph Manager, which requires a paid subscription.', 'In our research, use of the term “adapter service” refers to the functionality or possibility of getting a combined response from several backend services. It also refers to the ability to apply transformations so that we have only one public facing endpoint and can shield clients from excessive backend complexity.', 'In WSO2 API Manager there are several options for implementing an adapter service:', 'It means that there is only one way to create the Adapter Service directly in API Manager - by using the custom Message Mediation sequence. In this case, there is also a need to create separate APIs in Publisher and add the created flows in the Message Mediation panel. This Adapter API is versioned, updated, and maintained as a common API.', 'With regards to Message Mediation, it is impossible to get information from xml in the Publisher UI. This means that you won’t be able to properly understand the flow of the Adapter from the UI. In addition, it’s important to note that Message Mediation will be applied to ALL resources from the API. All requests to the current API (regardless of the kind of resource) will follow this mediation sequence.', 'One of the main advantages of GraphQL is that clients can compose any request query that they need with all the required fields. GraphQL then fetches data from the underlying services and composes one response to the client. There is no need to write any special adapter service to compose the backend services into a single API.', 'In Apollo GraphQ, the Adapter Service functionality works naturally, as it is one of the core features of this technology. As for WSO2, it isn’t a convenient solution to write the adapter in xml files, or even using Java classes. It may also be problematic to debug it in case of any issues. So for WSO2, it appears to be more suitable to write a separate service, which will then act as an adapter.', 'We expected that the API Manager solution would provide useful API usage audit information such as API usage statistics, API response time, API usage by users, API subscriptions, etc. We also wanted to check how easy it would be to integrate the API Manager with a third-party monitoring system, which in our case is ', '.', 'In the WSO2 eco-system there is an ', ' application that is responsible for usage auditing. It includes various groups of statistics including API Publisher statistics (general information such as the number of API calls, total API count, top API creators, etc.), API Developer Portal statistics (number of application calls, faulty calls per application, etc.) and Admin Portal statistics (availability of APIs). Almost all metrics are related to the different statistics of API calls. WSO2 API Manager offers an ability to configure alerts based on collected statistics.', 'WSO2 API Manager additionally provides audit logs. They are enabled for user actions in Publisher, Developer Portal, and Carbon (sign-in, sign-up, API and application modification, etc.). Management Console can also be useful for monitoring purposes through the use of JVM Metrics (CPU, Memory, etc.).', 'What about integration with third-party monitoring systems? As discussed above, we assessed the capability for ', '. There are two steps that need to be undertaken to achieve this purpose. The first step is running Prometheus JMX exporter as a java agent for WSO2 API Manager. By default, all WSO2 products expose JMX MBeans during the startup. The related service URL should be used as a target that exposes JMX MBeans to the Prometheus server. The second step is the definition of the JMX exporter endpoint in Prometheus config.', 'The largest component of the metrics that can be found in Prometheus using API Manager are related to API Manager as a Java application. There are also some types of metrics related to API calls however, API Manager Analytics is more suitable for providing a wider range of other metrics.', 'Apollo Graph Manager provides information on performed queries, requests, and errors. To view these metrics Apollo Server needs to be configured to connect to Graph Manager. It is possible to view some common performance metrics including request rate and request latency but it’s important to note that the GraphQL API is represented as a single endpoint, so all those metrics will be shown for the entire endpoint. It’s also important to emphasize that Apollo Graph Manager is a paid product, and many of its major functionalities are available only under a subscription.', 'In cases where the client needs detailed information on a specific query execution, Apollo Graph Manager can use operation traces to provide ', ' such as:', 'It is also possible to segment metrics by clients. To achieve this, each request should contain corresponding HTTP headers. This helps to analyze data more granularly. In particular it allows you to better understand if a high failure rate for an operation is tied to a specific client version.', 'Regarding integrations with third-party monitoring systems, it’s possible to publish basic Node JS app metrics. Using Prometheus as an example, this can be done using the express-prometheus-middleware library. Then metrics like GraphQL server CPU usage, or average request duration can be enabled on Prometheus dashboards. As a result, the combination of Graph Manager and third-party monitoring systems provides a good outcome.', 'Both API Manager solutions provide the ability to execute usage audit monitoring for a range of graphs and metrics. However, there are some key differences with their respective approaches. WSO2 API Manager divides these metrics into separate APIs, while Apollo GraphQL bases their metrics on whole schema. It is possible to make integrations with third-party monitoring systems for both solutions. It is also important to note that the metrics provided via Apollo Graph Manager require a paid subscription.', 'We wanted to understand the capabilities of securing APIs using authentication/role-based authorization processes. We also wanted to check if it was possible to integrate with a third-party authentication server, which is in our case was Auth0.', 'WSO2 API Manager offers several types of application level security - OAUTH2, Basic, and Api Key. Specification of these types happens in API Publisher. There is the option to choose from several types of security level. In Developer Portal you can choose which type of access token will be supported by the application, OAUTH or JWT. The following grant types are supported in Developer Portal - Refresh Token, SAML2, Password, Client Credentials, IWA-NTLM, and JWT.', 'In WSO2 API Manager it is possible to define the scope for separate API resources. Scope presents a collection of user roles, which permit access to the specific resource. In this case, generation of the access token should be done by detailing specific roles. If there is a need to change the scope for some kind of resource in the API that is already being consumed in the application, the ', ' (relates to Key Manager cache).', 'When integrating WSO2 API Manager with a third party authentication server, ', ' should be used. It is an open-source identity and access management product that works as separate server. This product takes the role of Key Manager for API Manager.', 'What about ', '? Only the 5.9.0. version of Identity Server can be used with the 3.0.0. version of API Manager. In our case, we needed to use the prepackaged WSO2 Identity Server as a Key Manager (version 5.9.0.). It comes with the necessary configurations already installed that are needed to connect with WSO2 API Manager and is different to the default version of WSO2 Identity Server 5.9.0.', 'It’s necessary to set ', ' for API Manager and configure databases for API Manager and Identity Server (WSO2AM_DB and WSO2SHARED_DB should be common for both instances). A description of API Manager Gateway should also be added to Identity Server and the URL defined for Identity Server in the API Manager configuration.', 'The next step is to directly integrate with the third-party authentication server (in our case - Auth0 server). This requires the following steps:', 'What was the final outcome of this process? We integrated WSO2 API Manager with the Auth0 server using Identity Server (Authorization code was used as a Grant Type). However, we encountered a problem. This workflow works only with OAUTH token as a token type for application in Developer Portal. When we try to use the JWT token type, we always receive an ', ' error without any further description. Unfortunately, we weren’t able to find information on how to fix this problem in WSO2 documentation.', 'Apollo authentication and authorization should be implemented directly in the code. Therefore we wanted to check what needed to be done to integrate Apollo Server with Auth0, authenticate clients based on their credentials using OpenID Connect protocol, and authorize access to API based on roles.', 'GraphQL documentation says that we can use ', ' to add an authentication layer between the GraphQL client and server. The access token should be passed by the client as a specific request header. It can then be validated in the middleware and the authentication data will be set in the request object to pass to the next layers. ', 'Below is an example of the code where we have added token validation (it’s performed by Auth0). If the check passes, then we decode it and place it into context. In the case where the validation isn’t successful, then clients will get a 401 Error. Context is created for each request, so it’s safe to keep such sensitive information there. The context object can be accessed later in code for additional checks:', 'As for the authorization, GraphQL documentation says that there are only two places where authorization logic can be placed:', 'In the case where a user doesn’t have permission to view the resource, the status code of the response will be 200. Information about errors will be placed in the response body.', 'It’s not recommended to use resolvers in GraphQL best practices. Instead, it is suggested to use it only for prototyping and all related logic should be moved to the business layer. ', 'The Apollo documentation outlines several additional authorization techniques:', 'As you can see, there are several potential options available and GraphQL and Apollo only outline suggested best practices. It is up to the developers to select the option that best suits them.', 'WSO2 API Manager includes different built-in capabilities to secure the API with authentication/role-based authorization. It’s also possible to integrate WSO2 API Manager with third-party authentication servers. In the case of Apollo GraphQL, authentication and authorization logic should be implemented by developers directly in the code.', 'There needs to be a way to enforce agreement on API usage. One of the parameters is rate limits for the number of API calls that users can make. Those limitations can be set on different levels and with different rules.', 'In WSO2 API Manager there are four defined levels of throttling policy configurations:', 'WSO2 API Manager provides the option to create custom throttling policies in Admin Portal.', 'With the GraphQL API, it can be a difficult task to configure rate limits on any level other than the entire GraphQL endpoint. As there is no official GraphQL or Apollo library to perform this rate limiting, the dev team should look at third-party projects such as \xa0', ', or implement their own frameworks. In cases where rate limits need to be added more granularly, at the resolvers level for example, then ', ' can be used.', 'WSO2 suggests different levels at which rate-limits can be configured and this functionality is provided out-of-the-box. In contrast, Apollo doesn’t have built-in capabilities for rate limiting so the dev team should use third-party libraries, or alternatively write their own.', 'This part of the research is related only to WSO2 API Manager and describes one of its features.', 'In WSO2 API Manager version 3.0.0., a new feature was released - ', '. You can now deploy an API for individual services using Kubernetes. This API will act as the microgateway for related services, providing security, throttling, tracing, logging, and analytics capabilities. Using Operator for APIs you can also publish those APIs to the developer portal automatically, there is no need to create the APIs manually.', 'As with any other Kubernetes cluster, WSO2 API Manager can work with Istio. You can manually enable/install Istio in the WSO2 API Manager cluster to manage traffic routing, limitations, and security. However, WSO2 API Manager has native integration with Istio service mesh, meaning there is a separate ', '.', 'In this type of deployment, Istio provides data plane and control plane capabilities, while WSO2 API Manager provides management plane capabilities to manage microservices. When a new API is published via API Publisher Portal, the Portal pushes all related policies to the Envoy proxy via the Pilot and Mixer. The new API will then be available in WSO2 API Developer Portal, as usual. However, now clients access APIs not through the WSO2 API Gateway but by using the Istio Ingress gateway instead. It’s important to note that this approach may soon be changed however as there are discussions in the Istio community about moving away from the Mixer and Mixer plugin concept. As a result, WSO2 is currently working on an ', '.', 'The standard API Manager deployment is detailed at the start of this article. In this next section, we will assess Apollo GraphQL deployment in more detail as there are some interesting deployment characteristics that only relate to Apollo GraphQL.', 'Let’s look at deployment of Apollo Server as a serverless, autoscale GraphQL API to the cloud. Apollo Platform has support for both ', ' and ', '. As for Google Functions, there are also several deployment options:', 'For almost all options (with the exception of serverless-http), we’ll need to replace the standard Express server with a cloud-provider specific one. In this case, we’ll need to think about how to migrate logic that was done in Express middleware.', 'In comparison with standalone deployment types like Google App Engine or Google Functions, container orchestration with service mesh can provide some additional features that are not available in Apollo out-of-the-box.', 'For example, Apollo server doesn’t have built-in capabilities for circuit breaking in its data sources. However, Istio provides traffic management features that can be helpful upon deployment and production use such as circuit breaking and service timeout management. You can craft destination rules to limit the number of simultaneous or pending connections to eliminate service overloads and failures.', 'Traffic shifting and traffic mirroring can be helpful for service version upgrades. New versions can be deployed and tested so traffic can be switched to new deployments. It is also possible to use JWT based authentication either with Auth0 or another suitable provider. There is no need to insert any authentication code into the Apollo server code because everything can be handled by Istio services after adding special authentication policies.', 'Below is an example of this kind of deployment:', 'It is a good idea to use Apollo Platform in conjunction with service mesh. This is because \xa0this can provide additional functionality that isn’t supported by Apollo itself, but may be important for production deployment.', 'Below is a summarised comparison of WSO2 API Manager and Apollo GraphQL based on our research and experience:', 'WSO2 is one of the few vendors that provides both open source and paid versions of API Manager. The paid versions include a subscription as well as a cloud version. The open-source version allows organizations to use API Manager at no cost and our research indicates it still maintains a range of useful functionalities.', 'It should be noted that although we were able to perform most of the defined tasks using the open-source solution, we still encountered some problems for which it was difficult to find detailed solution information on. The WSO2 documentation has fairly generic information only, such that it’s difficult or impossible to find detailed information for any special tasks that need to be performed. We therefore had to spend a large amount of time working out how to integrate with a third-party Authorization server or to find out the best way to create an adapter service.', 'Some compatibility problems were also encountered between different WSO2 products and versions. For example, we started our research with WSO2 APIM 2.6.0,, which meant that it became difficult to then perform some tasks when the newest version was rolled out (in the 3.0.0. version, most of the configuration was migrated from xml files to one common toml file).', 'It is also important to note that clients won’t receive bug-fixes with the open-source version. Instead they will need to wait for the next major WSO2 version. The ', ' can solve many of the above mentioned challenges as it provides the WSO2 Update Manager, which allows for product updates with bug-fixes. It also includes the ability to migrate to new versions of WSO2 products smoothly and most importantly, provides support from the WSO2 team, which can be very helpful for cases where your task is not described in any official documentation.', 'Our overall conclusion is that WSO2 API Manager is a good product however, using the open-source version can result in running into a range of problems. As a result, the dev team will likely need to spend more time familiarizing themselves with all the required WSO2 products and be prepared to try and resolve various tricky tasks on their own.', 'GraphQL is a query language and it’s main usage is to build “one-size-fits-all” style APIs that are convenient for all API clients. However, in our research we were also interested in analyzing whether it might possible to provide additional API Manager capabilities in addition to its great query language and query execution capabilities. We chose the Apollo Platform as it is one of the most popular GraphQL implementations. It includes Apollo Server, Apollo Gateway as well as some interesting tools such as Apollo Graph Manager.', 'In general, we determined that the Apollo Platform does provide some of the necessary features that we were looking for in an API Manager solution such as API contract validation, usage audit and monitoring, and admin portal. However, for each of those features, clients will have to use the subscription-based Apollo Graph Manager.', 'All authentication/authorization requirements should be implemented by the developer team. Similarly, with regards to regular NodeJS application, there is no additional support from the platform. For other special requirements such as rate-limiting, these should be developed from scratch, with particular attention given to careful design. There are no official libraries for such tasks, so each team needs to write their own internal tools.'], 'date': '\r\n\t\t\t\t\t\t\tMar 23, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tIgor Kononov', 'author_url': 'https://blog.griddynamics.com/author/igor/', 'tag': ['Microservices', 'API', 'DevOps', 'eCommerce']}, {'title': 'WSO2 vs GraphQL: overview and comparison of open source API management products', 'article url': 'https://blog.griddynamics.com/wso2-vs-graphql-overview-and-comparison-of-open-source-api-management-products/', 'first 160': ['In recent years, many organizations have provided public access to their digital ecosystems via APIs. This has transformed APIs from being a tool, into being a product. At the same time, wide adoption of microservices has led to an increased number of internal APIs used by organizations as well. ', 'The effect of these changes is that since 2005, the number of APIs has grown from a mere curiosity to an overwhelming trend. This is reflected by the ', ' public-API catalogue, which reports that the total API count has grown from just a handful in 2005 to over 22,000 today as demonstrated in the figure below.', 'With such strong growth of both public and internal APIs, there is a clear need for special tools and techniques to manage the full life cycle of APIs and provide a better experience for API clients. This is where API management solutions play an important role. They can offer a variety of features however, according to Forrester’s definition, they generally include at least three core components: an API user portal, a business admin portal, and an API Gateway.', 'The main goal of this research was to evaluate several popular open-source API management solutions. These evaluations were done with respect to Grid Dynamics’ own requirements and were aimed at allowing us to then decide if we could also recommend them to our clients.', 'In the “', '” report, there is a comparison of many popular API management solutions. The report named WSO2 as one of the leading solutions and as most of its products are open-source we have therefore included an assessment of WSO2.', 'In addition, we have chosen to evaluate the GraphQL stack, which is an emerging data query and manipulation language. GraphQL currently has ', ', who have started development of their APIs using this new architecture style. Some of our clients have also evaluated GraphQL for their use cases, so we already know that there is strong real world demand for this architecture style.', ' is a solution for end-to-end API management in the cloud, on-premise, or in hybrid environments. In our research we’ve focused on the latest version of WSO2 API Manager, which is 3.0.0. and was released several months ago. It consists of six main components: API Publisher (admin portal), API Developer Portal (user portal), API Gateway, Key Manager, Traffic Manager and API Analytics.', 'Even though WSO2 API Manager is an open source product, it offers several pricing options: an open-source version, subscription-based (with an ability to update products with bug-fixes), and a cloud version. We’ve focused here on the open-source version.', 'GraphQL is an open-source query language for APIs that includes a server-side runtime for executing queries over types and fields. These are defined in schemas. It’s commonly \xa0compared to REST, as both of these architectures provide ways to develop web APIs.', 'The GraphQL API is represented as a single endpoint, which is served over HTTP. Clients have the ability to define the exact structure of the data they need in a request query, so the amount of data transferred between the client and GraphQL server is lower compared to REST. The response is usually returned in a JSON format.', 'GraphQL has a specification for a query language and an execution language that describes capabilities and requirements of data-models for client server apps. It includes many ', ' written for different languages, so users can choose the one that best suits their needs.', 'We’ve used Apollo as an example of one of the popular GraphQL specification implementations. This product is maintained by the dedicated Apollo team.', 'Apollo Platform includes a production-ready GraphQL server (Apollo Server), schema management and monitoring tool (Apollo Graph Manager) and client (Apollo Client). It also includes ', ', which is a special configuration of Apollo Server designed to compose distributed GraphQL schemas.', 'Apollo Server is a spec-compliant JavaScript GraphQL server that can be used by any GraphQL client. It’s main goal is to validate and execute queries by using a type system that is defined in schema.', ' is a cloud service that helps to manage, validate, and provide some security features on data-graph. Apollo Server pushes schema and analytics data into Graph Manager when this feature is enabled on the server. Graph Manager offers some basic free features and requires a subscription for others so please check their ', ' for more details.', 'In order to check the main API management solution capabilities, we need to integrate them with some backend services. In our case we did our research based on the following architecture:', 'Here we have API Manager deployed in a separate K8S cluster. In the case of Apollo GraphQL Server, there is also the option to deploy it easily with Google App Engine. We tried both approaches.', 'API Manager solution is integrated with the backend services. In our case, we wanted to check several options within the integration including:', 'The above backend services expose REST, SOAP, and WebSocket APIs. These APIs provide the ability to receive, update, and delete carts-related information based on user ID and item ID request parameters.', 'We also deployed Prometheus to check if we can collect and analyze any analytics data from the API Manager. And we’ve used Auth0 as an Identity Provider in order to check if API Manager can be integrated with a third-party service to support authorization/authentication.', 'WSO2 API Manager can be deployed in a containerized environment. There is a WSO2 docker registry where WSO2 applications images are kept. At the time of writing this article, WSO2 had one ', 'for the 3.0.0 version, with the deployment split between WSO2 API Manager, API Analytics, and MySQL backend pods. However, we will most likely require a higher level of service isolation on production, so the configuration may need to be adjusted.', 'We used Apollo Server as a gateway to our backend services. Deployment was done using Google App Engine.', 'Initially, we had to use apollo-server-express as Apollo Server:', 'Create app.yaml, which is required by Google App Engine:', 'Set required ', ' field in ', ' file:', 'In order to build the GraphQL API, several basic steps should be performed:', 'The schema is a major component of the graph API. It’s a blueprint that describes all of the data types and their relations. It also describes ways in which the clients can fetch data (queries) and how clients can modify data (mutations).', 'A simple model for our REST service would be as follows:', 'Apollo Server is supposed to fetch data from any data source such as REST endpoints, gRPC services, databases, etc. DataSource is a class that’s responsible for fetching data from any backend. It also includes logic for caching and deduplication. There are some common datasource implementations in Apollo libraries for example, clients may use apollo-datasource-rest library, which provides RESTDataSource. For our example, we extended it to implement REST calls to our backend:', 'Resolver is responsible for fetching data (using data sources). It provides the instructions for turning GraphQL operations into data.', 'We’ve defined several features that will help us perform our research and understand what is supported in the products that we’ve chosen for evaluation:', 'In addition, there are some useful facts related to specific API Manager solutions, which we will go into in more detail after evaluating the items in the above list.', 'Now let’s start with our first requirement for API Manager and check if WSO2 and Apollo Platform can help us with its implementation.', 'In using the term “user portal”, we are referring to some kind of UI that enables users to find available APIs and evaluate and subscribe to them. While the term “admin portal” also refers to a UI, in this case we are referring to the ability to publish APIs and specify different limitations such as rate limits or security checks.', 'WSO2 uses its own terminology for user and admin portals. For its user portal, WSO2 API Manager provides Developer Portal, which allows API creators to host and advertise their APIs while allowing API consumers to self register, discover, evaluate, subscribe to, and consume APIs.', 'As an admin portal, WSO2 offers API Publisher, which allows API creators to develop, publish, scale, monetize, and version APIs.', 'Apollo Platform has fewer capabilities here. It provides only limited admin portal functionality in Apollo Graph Manager such as GraphQL schema overview, usage info, etc. It’s important to note that it isn’t safe to use as a user portal, as currently all members in an organization have full permissions set. This means that all users have the ability to remove schema for example, which isn’t a secure solution for a user portal.', 'With regard to user and admin portals, WSO2 API Manager has rich functionality. It contains both Publisher and Developer Portal, each with their own range of capabilities. Apollo GraphQL only contains Graph Manager, which is responsible only for several capabilities of admin portal however, there is no portal for API clients.', 'Here we are analyzing if API Manager solutions are able to integrate with different types of backend APIs. We expect to see support for the most popular protocols, i.e. REST, SOAP, and WebSockets.', 'WSO2 API Manager supports the following types of backend API - REST, SOAP, WebSocket, and GraphQL. In the case of WebSocket, STOMP is not supported. All types of APIs can be restricted for access by roles in Publisher and Developer Portal.', 'WSO2 API Publisher provides two variants for creating APIs:', 'The second variant is preferable as there is no need to manually describe all available backend resources. API Specification can be loaded from the provided URL as well as directly from the schema file.', 'It is notable that WSO2 API Manager provides two options during SOAP API creation - ', ' and ', '. In the case of ', ' WSO2 exposes the SOAP API and will route all requests to the SOAP backend. The ', ' option allows for the exposure of legacy SOAP backends as the REST endpoints.', 'As we already mentioned, corresponding data sources should be used in order to fetch data from the backend. For the REST backend, Apollo provides apollo-datasource-rest library, clients just need to extend the already implemented RESTDataSource.', 'For the SOAP backend, there is no official library. Clients can try to use a third-party implementation, ', ' is an example.', 'As for the websockets, in general GraphQL supports websocket protocols via subscriptions. However, ', ' points out that in the case of clients using Graph Manager, subscriptions should be disabled in Apollo.', 'As you can see, WSO2 API Manager provides a larger range of backend API implementation support and expanded capabilities such as support of API specification. For GraphQL only the REST backend is supported out-of-the-box. For all other APIs, the dev team will need to write their own libraries or find third-party alternatives.', 'In this part of the research we wanted to check if current API Manager solutions support FaaS components as a backend service.', 'In WSO2 API Manager documentation there is no information about FaaS as a backend service support. However, there are some workaround solutions that can solve this problem. This involves creating a separate backend API that can invoke the lambda function and publish it. Alternatively, it is possible to create a custom Message Mediation sequence in WSO2 API Manager and set this sequence to request flow for the definite API in Publisher.', 'In the case where a GraphQL datasource needs to call a cloud function, there is no special logic for that process. Instead, you need to write a datasource that can send the HTTP request to the trigger function. For some cloud providers, tools exist for better integration for example when clients use AWS Cloud with AppSync, they can use ', ' as a datasource.', 'Regarding FaaS (Serverless) components, there is no out-of-the-box support for this kind of service in either API Manager solutions. However, there are some workarounds that can be used to effectively address this issue.', 'In this part of the research, we wanted to understand what the options were for the integration of API Manager with backend services. For example, do we need to define each endpoint location manually or is there some form of automation tool that can help to locate available services?', 'When a new API is created in WSO2 API Publisher you need to define the backend endpoint (or endpoints) of the corresponding services. In WSO2 there are several ways in which this can be performed:', 'As we discussed previously, the definition of services should be implemented directly in the code. Therefore, there is the option of using a real service host/port or Ingress controller name, with the decision being based on the nature of the deployment infrastructure.', 'Both API management solutions offer the ability to define services via real endpoint addresses or by using Ingress Controller service discovery capabilities. Additionally, the latest version of WSO2 provides the ability to automatically publish new APIs by utilising an integration with Kubernetes Operators.', 'For this part of the research, we were looking to understand what kind of contract validations were supported by API Manager solutions. For example, if API Manager checks if new versions of the API have broken backwards compatibility.', 'WSO2 API Manager provides the ability to validate API schema. Validation occurs at the time of schema import. This validation is relevant only to the specification rules (e.g., the OpenAPI definition is validated in accordance with the OAS 3.0 specification), and not related to the changes in the API during new version creation. All versions of the API are independent of each other. Regarding version control, ', ' manager is very useful as it helps to control the state of different API versions.', ' is to avoid API versioning, which we usually use for regular REST-based APIs. The main point here is that clients read only those fields, which they have defined in a request, rather than the entire schema. In this case, most changes such as adding new fields won’t break clients.', 'However, some changes in schema such as field renaming or removal or a change in a field’s nullability will result in a breaking change in the GraphQL API as well. Therefore, there needs to be a mechanism in place to identify those breaking changes and notify the dev team about potential issues. Apollo Platform suggests using Apollo Graph Manager to track schema changes and report if something new may break clients.', 'Graph Manager generates differences between local schema and the one most recently registered. Note however that Graph Manager uses Graph Manager Analytics (which requires a paid subscription) to determine whether new changes affect queries/mutations that were executed on graph recently (the time window is customizable). It doesn’t compare two schemas entirely. So if a field wasn’t requested for some time, there is a chance that it’s changes won’t be reported by Graph Manager. Graph Manager then returns non-zero exit code if breaking changes were found. This means that this check can easily be integrated with CI.', 'WSO2 API Manager doesn’t compare previous and new versions of APIs. Therefore it won’t report if a new version contains breaking changes. However, it has the ability to validate API schema in accordance with some common rules when the API is created.', 'GraphQL doesn’t use versioning but Apollo Platform provides a mechanism to analyze schema changes and report if something is broken. This feature must be integrated with \xa0Apollo Graph Manager, which requires a paid subscription.', 'In our research, use of the term “adapter service” refers to the functionality or possibility of getting a combined response from several backend services. It also refers to the ability to apply transformations so that we have only one public facing endpoint and can shield clients from excessive backend complexity.', 'In WSO2 API Manager there are several options for implementing an adapter service:', 'It means that there is only one way to create the Adapter Service directly in API Manager - by using the custom Message Mediation sequence. In this case, there is also a need to create separate APIs in Publisher and add the created flows in the Message Mediation panel. This Adapter API is versioned, updated, and maintained as a common API.', 'With regards to Message Mediation, it is impossible to get information from xml in the Publisher UI. This means that you won’t be able to properly understand the flow of the Adapter from the UI. In addition, it’s important to note that Message Mediation will be applied to ALL resources from the API. All requests to the current API (regardless of the kind of resource) will follow this mediation sequence.', 'One of the main advantages of GraphQL is that clients can compose any request query that they need with all the required fields. GraphQL then fetches data from the underlying services and composes one response to the client. There is no need to write any special adapter service to compose the backend services into a single API.', 'In Apollo GraphQ, the Adapter Service functionality works naturally, as it is one of the core features of this technology. As for WSO2, it isn’t a convenient solution to write the adapter in xml files, or even using Java classes. It may also be problematic to debug it in case of any issues. So for WSO2, it appears to be more suitable to write a separate service, which will then act as an adapter.', 'We expected that the API Manager solution would provide useful API usage audit information such as API usage statistics, API response time, API usage by users, API subscriptions, etc. We also wanted to check how easy it would be to integrate the API Manager with a third-party monitoring system, which in our case is ', '.', 'In the WSO2 eco-system there is an ', ' application that is responsible for usage auditing. It includes various groups of statistics including API Publisher statistics (general information such as the number of API calls, total API count, top API creators, etc.), API Developer Portal statistics (number of application calls, faulty calls per application, etc.) and Admin Portal statistics (availability of APIs). Almost all metrics are related to the different statistics of API calls. WSO2 API Manager offers an ability to configure alerts based on collected statistics.', 'WSO2 API Manager additionally provides audit logs. They are enabled for user actions in Publisher, Developer Portal, and Carbon (sign-in, sign-up, API and application modification, etc.). Management Console can also be useful for monitoring purposes through the use of JVM Metrics (CPU, Memory, etc.).', 'What about integration with third-party monitoring systems? As discussed above, we assessed the capability for ', '. There are two steps that need to be undertaken to achieve this purpose. The first step is running Prometheus JMX exporter as a java agent for WSO2 API Manager. By default, all WSO2 products expose JMX MBeans during the startup. The related service URL should be used as a target that exposes JMX MBeans to the Prometheus server. The second step is the definition of the JMX exporter endpoint in Prometheus config.', 'The largest component of the metrics that can be found in Prometheus using API Manager are related to API Manager as a Java application. There are also some types of metrics related to API calls however, API Manager Analytics is more suitable for providing a wider range of other metrics.', 'Apollo Graph Manager provides information on performed queries, requests, and errors. To view these metrics Apollo Server needs to be configured to connect to Graph Manager. It is possible to view some common performance metrics including request rate and request latency but it’s important to note that the GraphQL API is represented as a single endpoint, so all those metrics will be shown for the entire endpoint. It’s also important to emphasize that Apollo Graph Manager is a paid product, and many of its major functionalities are available only under a subscription.', 'In cases where the client needs detailed information on a specific query execution, Apollo Graph Manager can use operation traces to provide ', ' such as:', 'It is also possible to segment metrics by clients. To achieve this, each request should contain corresponding HTTP headers. This helps to analyze data more granularly. In particular it allows you to better understand if a high failure rate for an operation is tied to a specific client version.', 'Regarding integrations with third-party monitoring systems, it’s possible to publish basic Node JS app metrics. Using Prometheus as an example, this can be done using the express-prometheus-middleware library. Then metrics like GraphQL server CPU usage, or average request duration can be enabled on Prometheus dashboards. As a result, the combination of Graph Manager and third-party monitoring systems provides a good outcome.', 'Both API Manager solutions provide the ability to execute usage audit monitoring for a range of graphs and metrics. However, there are some key differences with their respective approaches. WSO2 API Manager divides these metrics into separate APIs, while Apollo GraphQL bases their metrics on whole schema. It is possible to make integrations with third-party monitoring systems for both solutions. It is also important to note that the metrics provided via Apollo Graph Manager require a paid subscription.', 'We wanted to understand the capabilities of securing APIs using authentication/role-based authorization processes. We also wanted to check if it was possible to integrate with a third-party authentication server, which is in our case was Auth0.', 'WSO2 API Manager offers several types of application level security - OAUTH2, Basic, and Api Key. Specification of these types happens in API Publisher. There is the option to choose from several types of security level. In Developer Portal you can choose which type of access token will be supported by the application, OAUTH or JWT. The following grant types are supported in Developer Portal - Refresh Token, SAML2, Password, Client Credentials, IWA-NTLM, and JWT.', 'In WSO2 API Manager it is possible to define the scope for separate API resources. Scope presents a collection of user roles, which permit access to the specific resource. In this case, generation of the access token should be done by detailing specific roles. If there is a need to change the scope for some kind of resource in the API that is already being consumed in the application, the ', ' (relates to Key Manager cache).', 'When integrating WSO2 API Manager with a third party authentication server, ', ' should be used. It is an open-source identity and access management product that works as separate server. This product takes the role of Key Manager for API Manager.', 'What about ', '? Only the 5.9.0. version of Identity Server can be used with the 3.0.0. version of API Manager. In our case, we needed to use the prepackaged WSO2 Identity Server as a Key Manager (version 5.9.0.). It comes with the necessary configurations already installed that are needed to connect with WSO2 API Manager and is different to the default version of WSO2 Identity Server 5.9.0.', 'It’s necessary to set ', ' for API Manager and configure databases for API Manager and Identity Server (WSO2AM_DB and WSO2SHARED_DB should be common for both instances). A description of API Manager Gateway should also be added to Identity Server and the URL defined for Identity Server in the API Manager configuration.', 'The next step is to directly integrate with the third-party authentication server (in our case - Auth0 server). This requires the following steps:', 'What was the final outcome of this process? We integrated WSO2 API Manager with the Auth0 server using Identity Server (Authorization code was used as a Grant Type). However, we encountered a problem. This workflow works only with OAUTH token as a token type for application in Developer Portal. When we try to use the JWT token type, we always receive an ', ' error without any further description. Unfortunately, we weren’t able to find information on how to fix this problem in WSO2 documentation.', 'Apollo authentication and authorization should be implemented directly in the code. Therefore we wanted to check what needed to be done to integrate Apollo Server with Auth0, authenticate clients based on their credentials using OpenID Connect protocol, and authorize access to API based on roles.', 'GraphQL documentation says that we can use ', ' to add an authentication layer between the GraphQL client and server. The access token should be passed by the client as a specific request header. It can then be validated in the middleware and the authentication data will be set in the request object to pass to the next layers. ', 'Below is an example of the code where we have added token validation (it’s performed by Auth0). If the check passes, then we decode it and place it into context. In the case where the validation isn’t successful, then clients will get a 401 Error. Context is created for each request, so it’s safe to keep such sensitive information there. The context object can be accessed later in code for additional checks:', 'As for the authorization, GraphQL documentation says that there are only two places where authorization logic can be placed:', 'In the case where a user doesn’t have permission to view the resource, the status code of the response will be 200. Information about errors will be placed in the response body.', 'It’s not recommended to use resolvers in GraphQL best practices. Instead, it is suggested to use it only for prototyping and all related logic should be moved to the business layer. ', 'The Apollo documentation outlines several additional authorization techniques:', 'As you can see, there are several potential options available and GraphQL and Apollo only outline suggested best practices. It is up to the developers to select the option that best suits them.', 'WSO2 API Manager includes different built-in capabilities to secure the API with authentication/role-based authorization. It’s also possible to integrate WSO2 API Manager with third-party authentication servers. In the case of Apollo GraphQL, authentication and authorization logic should be implemented by developers directly in the code.', 'There needs to be a way to enforce agreement on API usage. One of the parameters is rate limits for the number of API calls that users can make. Those limitations can be set on different levels and with different rules.', 'In WSO2 API Manager there are four defined levels of throttling policy configurations:', 'WSO2 API Manager provides the option to create custom throttling policies in Admin Portal.', 'With the GraphQL API, it can be a difficult task to configure rate limits on any level other than the entire GraphQL endpoint. As there is no official GraphQL or Apollo library to perform this rate limiting, the dev team should look at third-party projects such as \xa0', ', or implement their own frameworks. In cases where rate limits need to be added more granularly, at the resolvers level for example, then ', ' can be used.', 'WSO2 suggests different levels at which rate-limits can be configured and this functionality is provided out-of-the-box. In contrast, Apollo doesn’t have built-in capabilities for rate limiting so the dev team should use third-party libraries, or alternatively write their own.', 'This part of the research is related only to WSO2 API Manager and describes one of its features.', 'In WSO2 API Manager version 3.0.0., a new feature was released - ', '. You can now deploy an API for individual services using Kubernetes. This API will act as the microgateway for related services, providing security, throttling, tracing, logging, and analytics capabilities. Using Operator for APIs you can also publish those APIs to the developer portal automatically, there is no need to create the APIs manually.', 'As with any other Kubernetes cluster, WSO2 API Manager can work with Istio. You can manually enable/install Istio in the WSO2 API Manager cluster to manage traffic routing, limitations, and security. However, WSO2 API Manager has native integration with Istio service mesh, meaning there is a separate ', '.', 'In this type of deployment, Istio provides data plane and control plane capabilities, while WSO2 API Manager provides management plane capabilities to manage microservices. When a new API is published via API Publisher Portal, the Portal pushes all related policies to the Envoy proxy via the Pilot and Mixer. The new API will then be available in WSO2 API Developer Portal, as usual. However, now clients access APIs not through the WSO2 API Gateway but by using the Istio Ingress gateway instead. It’s important to note that this approach may soon be changed however as there are discussions in the Istio community about moving away from the Mixer and Mixer plugin concept. As a result, WSO2 is currently working on an ', '.', 'The standard API Manager deployment is detailed at the start of this article. In this next section, we will assess Apollo GraphQL deployment in more detail as there are some interesting deployment characteristics that only relate to Apollo GraphQL.', 'Let’s look at deployment of Apollo Server as a serverless, autoscale GraphQL API to the cloud. Apollo Platform has support for both ', ' and ', '. As for Google Functions, there are also several deployment options:', 'For almost all options (with the exception of serverless-http), we’ll need to replace the standard Express server with a cloud-provider specific one. In this case, we’ll need to think about how to migrate logic that was done in Express middleware.', 'In comparison with standalone deployment types like Google App Engine or Google Functions, container orchestration with service mesh can provide some additional features that are not available in Apollo out-of-the-box.', 'For example, Apollo server doesn’t have built-in capabilities for circuit breaking in its data sources. However, Istio provides traffic management features that can be helpful upon deployment and production use such as circuit breaking and service timeout management. You can craft destination rules to limit the number of simultaneous or pending connections to eliminate service overloads and failures.', 'Traffic shifting and traffic mirroring can be helpful for service version upgrades. New versions can be deployed and tested so traffic can be switched to new deployments. It is also possible to use JWT based authentication either with Auth0 or another suitable provider. There is no need to insert any authentication code into the Apollo server code because everything can be handled by Istio services after adding special authentication policies.', 'Below is an example of this kind of deployment:', 'It is a good idea to use Apollo Platform in conjunction with service mesh. This is because \xa0this can provide additional functionality that isn’t supported by Apollo itself, but may be important for production deployment.', 'Below is a summarised comparison of WSO2 API Manager and Apollo GraphQL based on our research and experience:', 'WSO2 is one of the few vendors that provides both open source and paid versions of API Manager. The paid versions include a subscription as well as a cloud version. The open-source version allows organizations to use API Manager at no cost and our research indicates it still maintains a range of useful functionalities.', 'It should be noted that although we were able to perform most of the defined tasks using the open-source solution, we still encountered some problems for which it was difficult to find detailed solution information on. The WSO2 documentation has fairly generic information only, such that it’s difficult or impossible to find detailed information for any special tasks that need to be performed. We therefore had to spend a large amount of time working out how to integrate with a third-party Authorization server or to find out the best way to create an adapter service.', 'Some compatibility problems were also encountered between different WSO2 products and versions. For example, we started our research with WSO2 APIM 2.6.0,, which meant that it became difficult to then perform some tasks when the newest version was rolled out (in the 3.0.0. version, most of the configuration was migrated from xml files to one common toml file).', 'It is also important to note that clients won’t receive bug-fixes with the open-source version. Instead they will need to wait for the next major WSO2 version. The ', ' can solve many of the above mentioned challenges as it provides the WSO2 Update Manager, which allows for product updates with bug-fixes. It also includes the ability to migrate to new versions of WSO2 products smoothly and most importantly, provides support from the WSO2 team, which can be very helpful for cases where your task is not described in any official documentation.', 'Our overall conclusion is that WSO2 API Manager is a good product however, using the open-source version can result in running into a range of problems. As a result, the dev team will likely need to spend more time familiarizing themselves with all the required WSO2 products and be prepared to try and resolve various tricky tasks on their own.', 'GraphQL is a query language and it’s main usage is to build “one-size-fits-all” style APIs that are convenient for all API clients. However, in our research we were also interested in analyzing whether it might possible to provide additional API Manager capabilities in addition to its great query language and query execution capabilities. We chose the Apollo Platform as it is one of the most popular GraphQL implementations. It includes Apollo Server, Apollo Gateway as well as some interesting tools such as Apollo Graph Manager.', 'In general, we determined that the Apollo Platform does provide some of the necessary features that we were looking for in an API Manager solution such as API contract validation, usage audit and monitoring, and admin portal. However, for each of those features, clients will have to use the subscription-based Apollo Graph Manager.', 'All authentication/authorization requirements should be implemented by the developer team. Similarly, with regards to regular NodeJS application, there is no additional support from the platform. For other special requirements such as rate-limiting, these should be developed from scratch, with particular attention given to careful design. There are no official libraries for such tasks, so each team needs to write their own internal tools.'], 'date': '\r\n\t\t\t\t\t\t\tMar 23, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tVladimir Kondraschenko', 'author_url': 'https://blog.griddynamics.com/author/vladimir/', 'tag': ['Microservices', 'API', 'DevOps', 'eCommerce']}, {'title': 'Which Enterprise Data Warehouse performs better for your workloads?', 'article url': 'https://blog.griddynamics.com/edw-performance-comparison/', 'first 160': ['In this article, we detail the results of our performance comparison of popular cloud-based Enterprise Data Warehouses using popular benchmarks. The goal of the investigation was not to do the most comprehensive comparison of them possible, but instead to uncover usable and valuable information to help you choose the most suitable system for your needs.', 'Enterprise Data Warehouse (EDW) is an analytical database that collects data from different sources such as CRM, IoT, ERP, user interaction recordings, etc. It also provides an interface for data analysis.', 'The following provides an overview of the basic concepts:', 'Redshift is a cloud data warehouse that achieves efficient storage and optimum query performance through a combination of massively parallel processing, columnar data storage, and targeted data compression encoding schemes. Redshift has node-based architecture where you can configure the size and number of nodes to meet your needs. Scalability is available at any time.', 'The Redshift Query Engine is based on PostgreSQL 8.0.2 and has the same SQL syntax with the following supported features:', 'However, it is important to note that Redshift doesn’t support some data types. A full list of unsupported features, data types, and functions is available ', '.', 'Redshift supports connection to the cluster through ODBC, JDBC drivers, and psql terminal-based front end tools from PostgreSQL. ', 'Redshift architecture:', 'The leader node in an Amazon Redshift Cluster manages all external and internal communication. It is responsible for preparing query execution plans. Once the query execution plan is ready, the leader node distributes the query execution code on the compute nodes. In addition, it assigns data slices to each compute node. ', 'Compute nodes in their turn are responsible for the actual execution of queries. A compute node keeps from 2 up to 32 slices. Each slice consists of a portion of memory and disk space in the compute node. ', 'The following diagram shows a high level view of the internal components and functionality of the Amazon Redshift data warehouse:', 'For durability purposes, Redshift maintains three copies of the data during a 1-day retention period with no additional charges. However, the period can be expanded for up to 35 days.', 'The architecture has the following disadvantages:', 'Redshift provides a Workload Manager (WLM) that allows you to flexibly manage priorities within workloads. Automatic WLM determines the amount of resources that queries need and adjusts the concurrency based on the workload. When queries require large amounts of resources in the system, the concurrency is reduced. In contrast, manual WLM gives you the ability to specify values for query concurrency for up to 50 concurrent queries as well as memory allocation. The default concurrency for manual WLM is five queries, and memory is divided equally between all five.', 'The following design decisions heavily influence overall query performance:', 'Pricing of compute resources and storage is combined: you pay only for provided nodes per hour, with an unlimited number of executed queries within that time.', 'While running performance comparisons we noticed that Redshift provides useful information via charts on CPU utilization for each node in the cluster, the number of open connections, disk space usage, and much more. It is also worth noting that there was a case when a cluster got stuck. We were executing queries from the “inner and left join” category and noticed that the number of queued queries rose to 500. The root cause was overloading of the cluster and system queries couldn’t be executed. To overcome this issue we decreased the number of concurrent queries for all data warehouses in the specified category.', 'Snowflake is an analytic data warehouse provided as Software-as-a-Service. Ongoing maintenance, management, and tuning is all handled by Snowflake. It resides on AWS, Google Cloud Platform, or Microsoft Azure in a restricted number of available regions. Its features and services are identical across regions except for some that are newly-introduced. ', 'The Snowflake EDW uses a new SQL database engine designed for the cloud. Queries execute on virtual warehouses, which are a cluster of compute resources. Snowflake offers a predefined size of virtual warehouses with auto-suspension and auto-resumption. To meet user concurrency needs, multi-clustered virtual warehouses are available. Resizing enables you to experiment with the most suitable performance at any time, without stopping the entire warehouse. Data is divided across nodes in the cluster.', 'Snowflake supports standard SQL including a subset of ANSI SQL:1999 and the SQL:2003 analytic extensions.', 'Supported features:', 'Syntax of the SQL queries is the same as in Redshift, except for the names of the particular functions. You can connect to Snowflake using Python, Spark, or Kafka connectors or JDBC, ODBC and .NET drivers or SnowSQL CLI client.', 'Snowflake has a unique architecture compared to its competitors:', 'It has three main components:', 'The architecture provides the flexibility to adjust capacity needs cost effectively, but it does have some disadvantages. For example, the maximum concurrency limit for operations like INSERT, UPDATE, MERGE, COPY, and DELETE is 20. Other operations don’t have hard concurrency limits and are adjusted by the MAX_CONCURRENCY_LEVEL parameter. Snowflake has also introduced “credits” for calculating prices. The credit cost depends on the cloud provider and region you chose. \xa0', 'Snowflake offers two types of data loading - bulk and continuous. For the best performance and cost effectiveness, the official documentation recommends splitting larger files into smaller sizes of between 10 and 100MB of compressed data. Using Snowpipe you can continuously load new data from external storage within one minute if you follow the set of recommendations listed ', '.', 'Time-Travel and Fail-Safe are Snowflake’s most unique features. Time-Travel enables you to perform actions like querying data in the past that has since been updated or deleted. You can create clones of entire tables at or before specific points in the past as well as restore tables, schemas, and databases that have been dropped. These actions are allowed for up to 90 days. Once the defined period of time has elapsed, the data is moved into Snowflake Fail-Safe and these actions can no longer be performed. ', 'Fail Safe is a 7-day period during which historical data is recoverable by Snowflake after any system failures. No user operations are allowed here. Snowflake minimizes the amount of storage required for historical data by maintaining only the information required to restore the individual table rows that were updated or deleted. As a result, storage usage is calculated as a percentage of the table that changed. Full copies of tables are only maintained when tables are dropped or truncated.', 'To improve performance you can enable a clustering key that is designed to co-locate data in the same micro-partitions based on matched values in specified columns. As a result, the optimizer will read fewer micro-partitions.', 'Cache is divided into the following:', 'Pricing of storage, compute resources, and cloud services is separate. Storage fees are calculated for each 24 hour period from the time the data changed. Compute resources are billed per second, with 60 a second minimum. Usage for cloud-services is charged only if the daily consumption of cloud services exceeds 10 percent of the daily usage of the compute resources.', 'Snowflake provides user-friendly charts with explanations in billing terms. Query history enables you to understand overall performance of the specific queries using charts and a comfortable SQL editor. Snowflake has a poor SQL syntax compared to BigQuery. For example, there is no statement like ‘select * except (col1, col2...) from table_name’ for excluding one or more columns from the result.', 'It is worth noting that one of our customers faced an interesting issue: queries were failing with an internal error. This turned out to be because too many SQL variables were set. There’s nothing written about the issue in the Snowflake documentation and only Snowflake support could ultimately clarify why the queries were failing. This is why it is good practice to unset all variables at the end of a SQL query.', 'Like Snowflake, BigQuery is a serverless warehouse. You don’t have to configure clusters or think of their uptime, it’s completely self-scalable and self-manageable. All you need to do is pick the proper pricing plan. ', 'Though BigQuery has the most limited SQL capabilities of its competitors, it supports all basic SQL constructions you might need. We didn’t need to modify queries much to adapt them to BigQuery, except those that use UNION, because BigQuery supports UNION ALL only.', 'Supported features:', 'BigQuery can run queries in two modes: INTERACTIVE or BATCH. BATCH mode is used for regular applications. It means that each query is added to a queue to wait for resources. Queries that run in INTERACTIVE mode are marked as highest priority and usually start as soon as possible. It’s very handy for analysts: CLI and GCP console configured to INTERACTIVE mode by default. This feature has a limitation: you cannot run more than 100 parallel interactive queries.', 'BigQuery supports DML as well as INSERT, UPDATE, and DELETE operations. All DML queries are transactional, so you’ll never get partially updated tables.', 'BigQuery doesn’t have JDBC or ODBC interfaces. Instead, it provides libraries for most popular programming languages like: C#, Go, Java, NodeJS, PHP, Python, and Ruby. If you don’t see \xa0your language in the list, you can use the REST API directly, or check for third party tools.', 'BigQuery has original architecture:', ' is the heart of BigQuery, it builds a query execution tree of jobs. Dremel might optimize query on the fly: stop some jobs and prepare others. ', ' is a cluster manager that executes Dremel’s jobs on hardware. Colossus stores files in a columnar format named ', '. ', ' provides a petabit network to gain max IO speed. This design allows for the high performance and flexibility of BigQuery jobs. You can check further details of exactly how BigQuery works ', '.', 'This design has downsides though. For example, you cannot build an execution plan before running a query. And all tree tiers are known only when the full query is completed. In our opinion, the BigQuery execution plan was not very useful.', 'We also noticed that BigQuery is not good at joining and grouping. Some queries like inner_join and q72 of big tables were not able to complete in 6 hours even in on-demand pricing mode. BigQuery doesn’t allow queries to run longer than 6 hours. Query q67 with ORDER BY failed because of resource limitations. In documentation you can find the one common advice to use the “LIMIT” clause with “ORDER BY”, but q67 already has it and still fails.', 'BigQuery pricing has its own specifics with two types of it offered by Google:', 'Another type of payment is storage. But you can get a discount for tables that were not updated for 90+ days. We found BigQuery to be the most expensive EDW.', 'We tested both plans. In on-demand mode, queries consumed up to 10k slots. In the case of flat-rate pricing, we limited the amount of slots to 500 to be comparable in price to Amazon Redshift. Even 500 slots were more expensive, but this is the minimum purchase amount allowed.', 'For testing and comparing performance we decided to use the widely spread TPC-DS benchmark. TPC-DS models a data warehouse and focuses on online analytical processing tasks. The entity-relationship-diagram below shows an excerpt of the TPC-DS schema:', 'It contains the two fact tables: Store_Sales and Store_Returns as well as the associated dimension tables such as Customer and Store. ', 'Benchmark also contains tables: Web_sales, Web_returns, Catalog_sales, and Catalog_return but relations with dimension tables are approximately the same as between Store_Returns and Store_sales.', 'Brief overview:', 'A full description of the TPC-DS benchmark is available ', '.', 'TPC-DS benchmark provides 99 queries that we divided into the following categories:', 'The “low level” category contains queries that have less than three joins and simple other conditions in the where clause.', 'The “middle level” category contains queries that have greater than two joins and many other conditions in where and group by clauses.', 'The “where condition” category contains queries with a lot of conditions in the “WHERE” clause. They use the wide range of available keywords like IN, EXIST, BETWEEN etc. ', 'The custom category contains queries that we created for comparing specific cases. For example, we created a table with a long string column and populated it with around 55,000 characters. We then created a query to find specific words in that column using the LIKE keyword.', 'To compare performance, we ran queries on Redshift, Snowflake, and BigQuery in parallel using 50 connections with the following configuration of the data warehouses:', 'To demonstrate comparison results we created several charts. For better visualization we divided the query categories into two subsets. \xa0', 'This chart shows the average time of the queries by category from the first part.', 'Next to the category name you can see the average size of the scanned data. On demand pricing in BigQuery gives you the ability to execute queries very quickly but charges are also high. ', 'Here is the median time of the same categories of the queries from the first part:', 'In the below chart you can see the average time of the remaining categories of the queries.', 'This chart shows the median time of the same categories:', 'For better visualization of performance we calculated a relative comparison between Redshift, Snowflake, and BigQuery with 500 slots. For that purpose we divided Redshift/Snowflake response time by BigQuery. Results are presented in two parts. We won’t use BigQuery with an on-demand strategy of pricing in further charts in order to have a similar setup in terms of pricing. In the below chart you can see categories where BigQuery with 500 slots is faster than Redshift/Snowflake in a specified number of times.', 'In the chart below you can see the categories where BigQuery is slower than Redshift/Snowflake.', 'Here are expenses by EDW.', 'We want to note that BigQuery with an on-demand strategy was very expensive but showed the best results in terms of performance.', 'We have combined performance and cost comparison results by EDW in the table below. As we can see BigQuery with an on-demand pricing model is the most expensive setup from our evaluation experiment. But on the other hand it showed dramatically better performance than competitors and won in almost all of the queries categories. If we are going to compare relatively similar setups in terms of pricing, then it makes sense to consider the first three columns (Redshift, Snowflake, BigQuery 500 slots).', 'Each data warehouse met our requirements related to concurrency and number of open connections but, in general, they each have restrictions. In Redshift, the level of concurrency is limited to 50 queries, but official documentation recommends you don’t set it greater than 15. Snowflake doesn’t have hard limits related to concurrency. You can manage it using internal parameters. To meet concurrency requirements, Snowflake recommends using multi-clustered warehouses with auto-scale or maximized mode instead of improving the size of the warehouse. BigQuery has a limit of 100 concurrent queries in interactive queries, but the batch doesn’t have limits.', 'From the developer side we want to note that each data warehouse provides Web SQL editors. BigQuery has the most advanced editor with the ability to share SQL code, auto-completion, and pre-calculated sizing of the scanned data. Redshift and Snowflake don’t have this functionality. Snowflake has a good support team, we found a lot of answers on your questions in the community. ', 'If you have a very limited budget for EDW, Redshift is your best choice. However, Redshift requires additional effort from the developer side in order to reach good query performance. Snowflake is the most advanced data warehouse in terms of provided features. It has good performance but quite expensive pricing. BigQuery is an easy to use data warehouse, but can become expensive. You just need to load data to the tables.', 'In terms of performance, BigQuery with an on-demand strategy of pricing gives you the ability to execute queries very fast but charges in this case are also high. In terms of security, each data warehouse provides a comprehensive set of features to keep your data safe. This includes automatic data encryption, user roles, and data loss prevention tools.'], 'date': '\r\n\t\t\t\t\t\t\tJun 30, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tNikita Kotov', 'author_url': 'https://blog.griddynamics.com/author/nikita/', 'tag': ['Big Data', 'EDW', 'Cloud computing', 'Redshift', 'Snowflake', 'BigQuery', 'Enterprise Data Warehouse', 'Data Analysis']}, {'title': 'How to implement multi-select faceting for nested documents in Solr', 'article url': 'https://blog.griddynamics.com/multi-select-faceting-for-nested-documents-in-solr/', 'first 160': ['Nearly all major online retailers these days support faceted navigation, as it is a key instrument of product discovery. Facets provide a powerful, intuitive way to summarize search results from multiple perspectives. They also offer a straightforward interface to apply filters and refine navigation results. Without faceted navigation, product discovery in large catalogs would be a tedious business of either manually sifting through dozens of result pages, re-formulating search queries, or digging through huge category hierarchies.', 'Typically, a facet looks something like this:', 'Each facet summarizes the navigation results for product attributes, such as COLOR or BRAND. The facets show what values (specific brand, size, color, etc.) of the attribute are present in the current results, as well as the number of individual products (stock keeping units, or SKUs) that correspond to each value. Therefore, facet values allow us to easily apply a facet filter, so that we can limit results to the products which have the selected attribute values.', 'On many sites, facets have single-select filters: they allow only the selection of a single value to be applied as a filter. After using a single-selected facet filter for the color ‘Blue’, the facets look something like this:', 'As you can see, all the facets have recalculated their counts, and the SIZE facet lost one of its values, “S”, as no products in a size S were blue. The COLOR facet lost all of its values but the selected color (blue), since products can only have a single color value.', 'This is a good baseline experience for searching online, but it offers limited functionality. What if there are two colors you like, and you want to explore products for both of them? With a single-select facet search, you have to constantly select and deselect facet filters, a task which quickly becomes time-consuming and frustrating. In addition, if you are unsure about the brand you are looking for, the single-select facet experience is unfriendly.', 'What you need is a multi-select facet which will allow you to select multiple facet values at once:', 'In this case, we are searching for products which are red or black, and are size M. This type of filtered search provides a far more convenient user experience, and is the de facto standard for major online retailers.', 'However, this approach presents significant implementation challenges. With single-select facets, each facet count directly represents the current result set. For example, if 12 red products are selected, then the COLOR facet would just contain “red (12)”, and nothing more. However, with multi-select facets, we are also interested in showing how many blue and black products there are if red products are not selected. In other words, to calculate the counts of multi-select facets, we need to ignore the filters which are applied to the current facet, yet respect the filters which are applied to all other facets.', 'This means that we can’t just calculate multi-select facet values based on the current result set. For each of the facets with applied filters, we have to prepare a special result set for the original query and all the filters except for the filters on the subject facet. So, instead of a single search+filtration and facetization pass, we may need as many additional searches+filtration as there are facets with applied filters.', 'This can take a pretty heavy toll on the search performance, especially in the case of large catalogs and result sets. The situation becomes even more complicated when we have a structured catalog (such as a product/SKU relationship) and are going after precise filtration.', 'This post describes our journey to employ the powerful Solr JSON facet API to support multi-select facets in a structured catalog. We applied performance optimizations and added usability features to help with automated filter exclusion, which is necessary for the correct multi-value facet calculations. We contributed our results in ', ' and ', ', both of which were delivered with Solr 7.4. With this release, Solr now supports multi-select faceting with nested documents out of the box.', 'Now that the high level explanation is complete, we will continue with a more detailed walkthrough of the basics of faceted search in Solr, and then get into the technical details of our solution.', 'We will first set up an example using products, facets, and SKUs, and show how faceting works from a technical perspective with Solr. Let’s say that we have an e-commerce site with the set of products shown below:', 'In our previous posts ', ' and explaining ', ', we discussed the basics of faceting and parent-child relations (another term for nested documents). Given the above data, and what we know from those posts, if a customer searches ‘clothes’, for example, they should be shown the facet ‘COLOR’ with values ', '. These values represent the number of products that have at least one SKU with the corresponding attribute, not the total number of available SKUs in that color. Put plainly: one of the products has a SKU in red, one has a SKU in black, and all three have them in blue.', 'The next step is to write the data into documents. Once entered, we can use Block Join Query (provided by Solr) to search and filter through document hierarchies. The facets are then calculated using either the Block Join Facet Component, or with the JSON Facet API. Both of these processes require indexing SKUs as ', ', with one example shown below. These indices clearly show the three products and their nested child documents, with each document corresponding to a specific SKU of a certain size and color:', 'We are going to use this data set in several examples to come. The first step to operate it is to open Solr on your computer (let’s call the computer ', '), and add the above documents to the index of a preliminary collection named ', '. Each Solr index can consist of several collections, which are similar to database domains. When you index documents or execute a query, you need to specify which collection it should be directed towards. To do all this, use the ‘Documents’ tab of Solr Admin UI (with ‘Solr Command (raw XML or JSON)’ document type).', 'We will now shift our attention away from document structures and return to our example. Let’s say that our customer searches for ', ', and we want to show her the ', ' and ', ' facets. We would then send this HTTP request (for example, by opening this URL in the browser):', 'As expected, the Solr response contains counts for the products and brands:', 'Remember that each facet value here represents a filter that can be applied to the given search result, and that the respective counts show the number of documents that would pass such a filter. So, if the user decides to narrow their search results by using the filter ', ', it won’t show any non-jean products. In this case, this means that dresses won’t be shown: the user will only see two documents. But what happens with the facet counts?', 'The counts for ', ' in the response seem obvious: there is only one Calvin Klein and one Levi’s product in the filtered search result. The ', ' facet, however, looks a bit confusing because ', ' has a value of 0. On one hand, since we filtered out all of the dresses, a count of ', ' for the ', ' facet seems to be a reasonable output. But on the other hand, from the user experience perspective, each facet value is just a potential filter. Users expect that by choosing several filters for one field, the search result would contain all of the products that pass these filters. This means that if a user chooses the filters ', ' and ', ' together, they wouldn’t want to see a blank result page (nothing can be both a dress and jeans), but instead, they’d desire to see all of the results containing jeans or dresses. Therefore, for multi-select facets, a count of 0 for ', ' would not be correct. In our example, the ', ' facet should look like this:', 'This is because, as we explained above, if we add the ', ' filter along with the ', ' filter, we need to see the counts for both facets. In other words, the counts for the ', ' facet after filtering by ', ' should look exactly the same as they were before the filter was applied. Essentially, the facet counts for multi-select searches should be unchanged, and only the documents shown to the user should reflect the filters being used.', 'Let’s go one step further and suppose that the user wants to apply a second filter (like a specific brand) on top of the first: for example, they might only want to see Calvin Klein jeans. This is what that request would look like:', 'The standard facet response to this query would be:', 'However, for multi-select facets, we would want to see a different outcome: ', '. In order to get this result, when calculating the multi-select facet for a specific field, we must exclude the filter of this field from the set of applied filters so that the filtered facet will still appear with its full count. Basically, while we want this field to be filtered in a search, we need to still see its facet counts, because otherwise they won’t appear when another filter is added.', 'Thus, to calculate the ', ' facet, we need to exclude the ', ' filter and apply only ', '. Similarly, to count the multi-select ', ' facet, only the ', ' filter should be applied. This process looks pretty cumbersome: filters have to be applied to each facet, and there can be dozens or even hundreds of facets in a large catalog. There has to be an easier way -- and there is.', 'Finally, we have come to the basic solution to our problem. When the user applies filters, we need to calculate multi-select facets. In order to count facets for any given field, we must exclude all user filters applied to it. For simple, flat documents, Solr provides a standard solution for this problem: ', '. Using this approach, we should mark each filter with a tag and request to exclude the tagged filter in the scope of a particular facet calculation. For example, our previous request with two filters should be specified as:', 'It yields us a single document with the desired multi-select facet counts:', 'This is exactly what we are looking for: all the facet counts we want to see are present. This solution works correctly for flat documents. The next step is looking at nested documents, which is where multi-select faceted searches really get tricky.', 'Suppose that we need facets for the ', ' and ', ' fields. One of the possible approaches to counting them would be using ', ', which was described in our previous blog post ', '. Unfortunately, this component doesn’t give the ability to exclude filters, so it is not suitable for calculating multi-select facets. Therefore, we turned to JSON facet API, which provides a flexible functionality for processing different types of facets, statistics, aggregations, and more. The issue with the JSON facet API was that it was too slow to work on a large scale e-commerce platform -- we had to find a way to make it faster.', 'Before we continue, we should say that Grid Dynamics was heavily involved in the development and creation of an important update that was delivered with ', ', and included in Solr 7.4. Our new feature, called ', ', was made in the scope of this update for the purpose of speeding up the JSON facet API. To utilize this function, the request must be formed as follows:', 'As you can see, creating the proper request in JSON facet API is a bit tricky. First, we must search for products in the query. Next, we have to define the facet domain as all children of matched products, which we do here: ', '. Finally, we try to aggregate these children by unique value with the ', ' field, which is automatically added to all child documents as a reference on their parents during indexing. The trouble in creating this request is worth it though, because of ', '.', 'This new function is a lightweight replacement for ', ' aggregation, designed and optimized specifically for rolling up facet counts of child documents. An important element in the request above is an official recommendation to define ', '. We discovered this definition while debugging Solr, and found that for our use case it speeds up both traditional ‘unique’ aggregations as well as the new ‘uniqueBlock’ aggregations. This improvement is related to the internal details of the implementation, as ', ' is much faster when it has no value limits, while the default value for the ', ' parameter is 10. The end result is a function that works much faster for this particular task than the generic ', ' aggregation.', 'The Solr response contains both ‘usual’ counts (field ', ') that indicate the number of matched SKUs, as well as ‘rolled up’ counts equal to the number of products that have these SKUs (field ', ').', 'Using the request above provides the following result:', 'That looks good! But what if the user now wants to apply the ', ' and ', ' filters? Well, our task becomes even more challenging. First of all, in this case the flat search query ', ' doesn’t fit anymore, because we want to filter products that are both black and in a size large. Since we’re now filtering by children documents, we need to use ', ' block join query to see which parent documents have children that match the filters. Second, domains for the ', ' and ', ' facets should be formed by a whole set of applied filters, with the filters of their own fields being excluded. While the JSON facet API is flexible enough to define a proper set of filters for each facet, it would be convenient to have a generic approach which would enable us to exclude specific filters from facet domains. This solution was developed in the scope of ', '. For our example, the request should look like this:', 'If you hadn’t noticed, both our search block join query and products filter ', ' are tagged as ', '. This is done to exclude them from children facet domains, because if you look at it from the performance point of view, it’s better to create the child-level domains from scratch. For SKUs we use the whole set of filters, but add some exclusions, for example, ', ' with ', '. It’s very important not to miss the original parent’s filter, which is why we need to add the ', ' block join query to each of the children’s domains. We have 2 blue and 1 black products that are size L, while we have 1 L and 1 M products that are black, so the response comes out as shown below. The counts and product counts now perfectly correspond to their correct values -- the multi-select faceted search for nested documents is finally complete.', 'The ability we introduced to exclude filters by facet domain significantly simplifies Solr requests for counting multi-select facets on the hierarchies of nested documents. It is highly recommended to use this feature with the new ', ' aggregation, as this substantially boosts the performance of request execution. Further potential improvements are being developed, with one known addition being the use of a pre-cached filter of parent documents. Because this retains a set of matched parent documents in memory, it makes switching from matched children to parent documents much quicker.', 'Ultimately, a multi-select faceted search is a must-have for any large retailer that utilizes Solr. It creates a vastly better user experience, making it quicker and easier for consumers to find the precise products that they need. Previously, it was almost impossible to count facets in nested documents in Solr due to the slowness of the JSON facet API and the difficulty of excluding filters. However, speeding up the JSON facet API with uniqueBlock and using our method of tagging and excluding filters for specific facets has created a fix for this problem. This step-by-step process should work for everyone: it is a solution that has proven to be very successful. While the methodology looks complex, we have already laid out most of the process for you, so it should be easy to implement in your system.', 'If you have any questions or want to learn more about multi-select faceting in Solr, please send us a call or leave us a comment below!'], 'date': '\r\n\t\t\t\t\t\t\tAug 14, 2018\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'Advanced Solr/Lucene topics: nested search for e-commerce applications']}, {'title': 'How to Release Components Independently when you have a Distributed Monolith', 'article url': 'https://blog.griddynamics.com/how-to-release-components-independently-if-what-you-have-is-a-distributed-monolith/', 'first 160': ["It's an e-commerce web application for a top-tier North American retailer. Quite a large application system - a few hundred developers are working on it. The system was rebuilt from scratch from a heavy monolithic enterprise application platform to a number of separately deployed components interacting over the network and running in the cloud. Why? Because they need new business features fast, 365 days a year. The problem? Feature releases are still once a month.", "The issue is not specific to e-commerce. Most business applications have their functions spread over many parts of the system, which is also continually changing. It is difficult to fit into a thin and stable API. That's why so often a re-platforming from a monolithic application system to an ensemble of components communicating over the network doesn't end up with a microservice design. More often it becomes a tightly coupled conglomerate, where changes have to be orchestrated and come to production in aggregated big-bang releases.", 'There are many signs that point to this issue. A centralized QA team aiming at end-to-end application validation, usually by UI tests. The individual component teams without testers or even without some component-level tests. A centralized release team. Pieces of code shared between components so a change in the shared part requires updating all the dependent modules. Circular dependencies between application components.', "A common recommendation is to step back. Restrain the component interfaces and segregate different functions to different modules. Or build comprehensive suites of component tests and build multi-functional application teams. That does work. Except for one problem - it is often prohibitively expensive. And it usually doesn't give the desired change velocity because a business feature is not a change in a single API service.", "So we prefer to think differently and put on the developer's shoes. As a developer of a business feature, I most likely won't have an opportunity to develop my feature and reliably test it in isolation. At most I can rely on the fellow QA team that can validate if my feature works correctly as a part of the whole system. In a perfect world, I'd want a full copy of the production system for development. I’d then want users to leave the production system for some time so I could deploy and check the new version without disruptions, hand it to the QA and then, if everything goes well, let the users come back.", 'For most cases I don\'t need the production system itself, a perfect copy is fine. A full copy of the production environment for each feature would be way too expensive. But a "perfect copy" is not necessarily a "full copy". A "', '" is just as good. To make a shallow copy I don\'t have to replicate each piece of the system, instead I access the system components by references and copy only the references. So I get a perfect clone essentially for free.', 'Having a full application system at my disposal, I update my component to the new version implementing the desired feature. The "', '" technique helps - it is complementary to "shallow copy". Here I deploy a new instance of my component side-by-side with the production one and update the reference to it. Now my clone of the system differs from the original one, but the difference is exactly my deployment.', 'So my company needs just a single non-production instance of the application system. Everyone will have a (shallow) clone of it for development and testing. And a single full production system - I deploy my update to its clone and switch users to it when ready.', 'It looks similar to communication channels within a telecom network. The channels share the network but they don\'t intersect and don\'t see each other. Similarly, we refer to these shallow clones of application systems as "channels" or "deployment channels". These five operations cover the life cycle of a channel:', "Once they’re implemented, each channel can be used just as a regular independent system instance. But let's move from abstractions to real world implementations. Fortunately most of it can be done with available open source tools or cloud services. So let’s see.", "Assuming we're still dealing with a web application, it may expose APIs for mobile clients or serve web pages for desktop browsers - it doesn't matter. In any case, user actions come in the form of HTTP requests. HTTP doesn't maintain user sessions. But HTTP requests come with metadata that allows segregation of users, so requests from a user can be distinguished from requests by another user. It could be done by way of HTTP cookies or authentication tokens, or in the most obscure cases by source network addresses. It’s not a problem if we don't separate requests from two similar users, we just need to ensure that requests from a single user don't come to different buckets.", "Incoming requests don't pour directly to the application services. Instead they come to a border proxy service or an ", ' such as ', '. It is the first tool in the kit. It does a few things - segregates requests from different users, sorts users to cohorts according to operator-defined criteria, and labels requests from a cohort with a tag. For example QA engineers evaluating the new functionality may be one cohort, a pilot user group for early access the other, and the bulk of the user base the third. Then the border proxy configuration associates user cohorts with deployment channels and indicates the selected channel with a label in the request metadata. For HTTP the label is a request header with a known name and the channel name as the value. But the requests from different cohorts still go through the same fire-hose.', "At this moment we may need to send requests from different cohorts to different application system instances. That's pretty easy if they are full copies - the copies don't intersect. They have different network endpoints so the border proxy just needs to forward requests from a bucket to the specific network address. But when the channels are shallow clones they share application components and their network endpoints. Apparently the usual L3 network traffic routing is not enough, we need it respecting application level context - the request labels. ", ' is a class of application networking middleware doing exactly that. ', ' is the most prominent example but Buoyant ', ' pioneering the service mesh trend and HashiCorp ', ' are also worth mentioning. Service mesh is the second piece in the kit.', 'Service mesh acts at the application layer (L7) rather than the network layer (L3) where the regular network routing works. When an application code calls an HTTP service, it makes an HTTP call to a URL (say ', "). From the application side the URL is the entry point to the remote service. However, for the networking stack the URL doesn't make any sense. Instead it sees a TCP connection to port 443 at a host with the IP address resolved from ", ' by DNS. The rest (', " URL path, request method, request metadata) is out of its scope. So the application and the networking stack act in the same context (HTTP call to the URL) but see it completely differently. And that's a problem.", 'In contrast, a service mesh middleware sees an HTTP request to ', ', exactly as the application. Then it may apply various logic mapping to the service endpoint (the URL) to network endpoints of particular service instances and back. This is regardless of scaling, upgrades, experiments, and failure recoveries. So the application code communicates directly to ', ' service as it is implemented rather than relying on assumptions, conventions, and operations craftsmanship.', 'A well-implemented service mesh is transparent for application servers and clients. Effectively it can be injected to a running system on the go - with zero configuration it works like a bare network. Though in our case the service mesh routes HTTP requests not just by the host part of the URL, but also observes the request labels assigned by the border proxy or the API gateway.', 'The last part of the toolkit cannot be taken from the shelf. It belongs to the application. The border proxy labels incoming requests and the service mesh routes requests according to the labels. But the application code needs to retain those labels. For example if it is a catalog UI component calling catalog, inventory, and pricing API services, the catalog UI must relay the incoming request label to the resulting outgoing requests to API services. If the incoming request has a "v1" label in its HTTP headers, the catalog UI calls to inventory and pricing APIs must have the same HTTP header.', "A frequent perception is that it is not a problem, that such a complication is not necessary, and that the regular traffic shifting facility of any modern load balancer does the job. So if I want to roll out a new release, I deploy it alongside the old version and gradually shift traffic to it. Unfortunately it works to a limited extent only in the case of a true microservice architecture when each service is independent, comprehensively tested in isolation, and fully backward compatible. So end-to-end testing before release is surplus. However, it is rarely the case. Moreover, shifting a fraction of the traffic means a fraction of each user's traffic. So each user will get a mixture of the old and new experience. It also makes comparative evaluation (A/B experimentation) of application code features impossible.", "Now let's see how it plays out in a real environment - an e-commerce web application, Istio service mesh, and Apigee Edge API gateway. For simplicity the application runs on a Kubernetes cluster but technically an Istio service mesh can be deployed at multiple Kubernetes clusters, bare cloud VMs, or in a mixed Kubernetes-VMs setting. Istio has a dedicated API object (VirtualService) controlling traffic routing in the service mesh. Here it is:", 'At this point the new parts (Apigee, Istio and the application support for request labels) add nothing. But the actual process under the hood becomes a bit more interesting. When I enter the site URL in my browser, the request goes by Apigee Edge proxy that adds a channel header with, say, "v1" label and forwards it to Istio ingress proxy at the edge of the Kubernetes cluster. By default the service mesh ingress proxy forwards the request to a frontend service instance ignoring the label. With additional conditional routing, it does the same - for now, because only the single frontend instance is available.', 'To make use of it, let\'s release a new version of the catalog service. What\'s interesting is that the application users don\'t interact with the catalog service directly, they see only the UI frontend. So they cannot be diverted to another network endpoint to see the new version. To complete the task, one should take a few steps. The first is to make a shallow clone (a channel) of the system for a new deployment. Let\'s call it "v2". "Make" means a set of Istio VirtualService routing objects that apply to requests with the label header equal to "v2". For now these routes point to the same old application component instances.', 'Next is to add a rule for the Apigee Edge proxy. It will select QA users by a specific HTTP cookie that QA users set in their browsers. QA users are assigned the "v2" label. So as a QA user I set the cookie in my browser, enter the site URL... and see the same picture because for now the "v2" channel coincides with "v1".', 'Now I deploy the new version of the catalog service. I deploy it alongside the existing one, label it as "v2" and update service mesh routing (the catalog VirtualService) to send requests with the "v2" label to the new ("v2") catalog instead of the old ("v1") one. It wouldn’t change anything if the application doesn\'t relay label headers because the catalog service doesn\'t get labeled requests directly from the border proxy that adds those labels. But because every application component adds the same label to each outgoing request it sends to fulfill the incoming one, the whole tree of requests caused by the initial one from a user is routed consistently, to the same instances, within the same channel - either "v1" or "v2".', 'It does not change anything for the majority of users. Their traffic (labeled with "v1" at the border proxy) still goes by frontend to the "v1" catalog API service. But the QA users accessing the same site get the new catalog experience. They may validate the system end-to-end using a traditional monolithic testing approach. Effectively, QA users and everyone else see two different application systems, as if they were deployed separately and share only the data. Practically these two systems are virtual, sharing most of the same components.', 'Once everyone is satisfied, the Apigee Edge proxy can be reconfigured to gradually re-assign the production users to the "v2" channel. Once done, "v1" routes may be removed. This leaves the old ("v1") catalog API service without any link pointing to it, so it can be destroyed (garbage collected).', 'At this time I take minimal risk - users see exactly the same instance of the application system as was validated. Not "similar", not "using the same artifact versions" - such similarities leave a lot of room for discrepancies that can and will cause issues. Just exactly that instance, certified.', 'Still in production, I want to evaluate two new versions of the catalog service. This time I make two shallow clones of the production system - channels "a" and "b". The new versions of the catalog service are deployed to these new channels so the production baseline and the new channels are now different. The border proxy is configured to segregate two test groups of users, say 1% of the Australian user base each. It is then set to label requests from those two groups with the "a" and "b" channel labels. Now one test group sees the application with catalog version "a" and a second group with catalog version "b", while nothing is changed for the others. So I can measure conversion, retention, or whatever else using the convenient tools. The experimental channels may differ as little or as much from the baseline as desired. It could be either a different feature flag, or it could be a complete rewrite - both are fine.', 'Recalling the original problem, the application system in question is a distributed multi-component one. But its components are not independent microservices. They cannot be developed and tested in isolation. As a developer I need a complete application system as the environment for my component. Now I have it - a shallow clone of the shared baseline non-production deployment. Usually the baseline deployment follows the production one. Each development group has its own clone or several clones. From the cost perspective these clones are as much as the deployments by the group, usually a single component instance.', 'If several components are getting dependent features (for example if a UI improvement depends on a new catalog API feature) development teams may create their deployment channels in a chain - the catalog team clones the baseline (and deploys new catalog API component builds there) to make its working channel. The UI team in turn clones it and deploys new frontends. Of course it is more fragile than independent development. But sometimes it allows for shipping a business feature within one development sprint, while with an independent process it may take two or three.', 'It may appear that this technique with service mesh and deployment channels solves most SDLC issues of real (imperfect) business server applications without the constraints of rigorous microservice architecture. Of course however, it comes with limitations:', 'However, the microservice architecture is not a panacea either. We saw in practice that if a business problem cannot be decomposed to a flat set of independent small sub-problems, the corresponding application system may be multi-component, but it will not adhere to a microservice architecture. It may be a conscientious choice - in the ', " we showed that it may be the right way to scale application development. Enforcing strict service isolation regardless that may lead to an over-complicated system and development inefficiency. In such (quite common) cases, it's better to accept the explicit system complexity and manage it by the right middleware (service mesh) and use patterns (deployment channels)."], 'date': '\r\n\t\t\t\t\t\t\tOct 29, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tKirill Evstigneev', 'author_url': 'https://blog.griddynamics.com/author/kirill-evstigneev/', 'tag': ['service mesh', 'monolith', 'Microservices', 'continuous delivery', 'cloud migration', 'AB testing']}, {'title': 'Add anomaly detection to your data with Grid Dynamics accelerator', 'article url': 'https://blog.griddynamics.com/add-anomaly-detection-to-your-data-with-grid-dynamics-accelerator/', 'first 160': ['Modern businesses in e-commerce, banking, insurance, manufacturing and many other areas generate huge amounts of data every day. Effectively utilizing that data helps increase business value by making the right decisions at the right time based on insights generated from targeted data analytics.', 'In this article we describe our real-time cloud based Anomaly Detection Solution. We will cover its design and applicability to the most common use cases: monitoring, root cause analysis, data quality, and intelligent alerting. The solution is AI driven and implements a flexible approach based on normal state and behaviour patterns extraction, but does not rely on purely statistical methods. Therefore it can catch not only suddenly occurring outliers but can also reveal changes in the distribution of very noisy data. ', 'The solution is configurable for various types of components and services and detects \xa0abnormal behaviour in networks of connected services and collected data profiles. It can visualize abnormal places and make intelligent notifications based on them and provides tools for further deep analysis and RCA.', 'Most application systems consist of multiple services and middleware components. This includes databases, queues, search engines, storage, identity services, caches, and in-memory data grids. They also include multiple stateful or stateless microservices and mobile application proxies — all connected by data and processing flows. ', 'Each component produces system and application logs and usually collects system and application metrics through metric collection agents. In addition, data itself may contain important information that represents the state or behaviour of a service, process, or user. All of these can be aggregated and served as data sources for future analysis and real-time anomaly detection using the Anomaly Detection Solution.', 'The solution covers the following use cases:', 'An alert is a more high-level entity than just an anomaly. It is essentially a group of anomalies and the number and density of them are determined by business rules that are set via the dashboard.', 'Experiments have been run with the current solution based on real-world data. They \xa0showed a decrease in false positives (non-important abnormal cases) as well as false negatives (missed important abnormal cases). As an example, the following are evaluation metric values achieved for real-world e-commerce domain service anomaly detections:', 'To measure the classification metrics, which provides information about the anomaly detection quality, significant effort is required because a huge volume of metrics and timepoints must be analyzed and marked as normal or abnormal. The solution proposes to analyse only abnormal cases and related KPI’s (see ', '). It uses the solution labeling tool to collect user experiences in the anomaly importance verification process. ', 'The solution is part of ', '. It consumes Amazon Services including AWS S3, RDS, EKS, Cloud Map, Secret Manager, Cloud Formation, Kinesis, Lambda, EMR, Sagemaker, etc., and has been published to Amazon Marketplace.', 'The current solution is an implementation of the Neocortex Neural Networks approach described in the ', ' white paper ', ' with Amazon Cloud Services. It is based on the Nupic HTM framework ', ' integration and anomaly detection is based on the likelihood calculation for prediction errors (anomaly scores) distribution. ', 'The solution provides complete implementation of a real-time anomaly detection workflow, including its deployment and scaling, external data onboarding, configuration, and tuning of ML models for specific tasks and domains.', 'The following features are available:', 'Let’s now walk through the solution design.', 'The solution implements an anomaly detection workflow. It covers the end-to-end process from data collection through to information delivery on abnormal behaviours.', 'The first step occurs in the data domain. It consists of application, system, and analytics metrics values collected from monitoring of services and middleware components, data quality profiles, and logs. This data is pushed to one of the available channels: Elasticsearch index and streaming. ', 'The Elasticsearch index is used for showcase purposes but at the same time demonstrates the capability to ingest data from data sources connected via the HTTP protocol. The streaming channel is ready for enterprise tasks and is a datasource for real-world customers’ data onboarding to the anomaly detection ecosystem.', 'The next step is data processing. It prepares ingested data to be input for ML models training and inference in real-time. The anomaly detection inference is a ML models prediction, where anomaly scores are calculated to obtain the likelihood (probability of abnormal cases) for comparison with the threshold. ', 'The last step is anomaly visualization and notifications to recipients. It includes signal distribution and anomaly statistics charts as well as anomaly dependencies graphs.', 'The anomaly detection pipeline includes a data workflow. It consists of cells where each cell is a combination of the data input pin, configuration pin, data processing unit, and data output pin.', 'The data processing unit contains algorithms for one or more anomaly detection workflow steps. The data processing configuration is a set of parameters for data processing algorithms. The following diagram shows the processing units and transitions for the solution data workflow:', 'Each step is configured with settings stored in the relational DB. Data is persisted to and fetched for the next steps from Elasticsearch indices. The root point of the workflow is the ETL job implemented outside the solution by the customer to onboard real-world data to the anomaly detection ecosystem.', 'The anomaly detection core is the HTM Model together with its inference and the anomaly likelihood calculation logic. Neocortext Neural Network and Nupic HTM framework ', ' integration is described in the ', ' section where we describe the ML model’s life cycle, their deployment units, and helper processes. However, to provide a brief description of the process:', 'Grid Dynamics designed and published the solution for the Analytical Data Platform (ADP). It provides the number of components deployed using Amazon Cloud Services to implement data and machine learning pipelines. All supported pipelines cover various business areas that consume data ingest, batch, and streaming data processing algorithms, etc. ', 'Machine learning tasks are represented by computer vision, programmatic marketing, data quality, and anomaly detection. The anomaly detection case uses ADP components that provide the toolset, services, and resources for real-time processing and streaming. The following diagram represents the ADP components consumed by the anomaly detection elements.', 'The copy of the anomaly detection solution is run as an ADP use case or single item via Amazon Marketplace. It is a “one-click” deployment including configuration of ADP and anomaly detection parameters.', 'All anomaly detection resources and components are registered in the AWS Cloud Map for internal system discovery and discovery for user access. All credentials are generated in deployment time and available via AWS Secret Manager. ', 'AWS Account allows the user to touch and consume all components of the solution. Only the Anomaly Detection Dashboard site is available via public network (ethernet). It is protected by the fact that credentials are created automatically during the deployment phase to access the site. All other components are available from the AWS VPCs or with port forwarding with kubernetes CLIs from a local desktop or laptop. Grafana is also available in the “viewer” mode from ethernet proxied by the Anomaly Detection Dashboard. ', 'HTM models are hosted and run using the Amazon Sagemaker endpoint. One model version is hosted by only one endpoint instance. The current solution deployment provides only one Sagemaker endpoint for all HTM models. The solution design additionally only allows one instance per endpoint. Our default VM configuration for instance is ml.m4.xlarge. The maximum number of hosted HTM models is about 50. It’s possible to increase the number of models by scaling up the Sagemaker endpoint VM properties (instance type) or by scaling up horizontally the number of Sagemaker endpoints.', 'To measure the classification metrics that provide information about the quality of the anomaly detection process, significant effort is required. This is because a huge volume of metrics and timepoints need to be analyzed and marked as normal or abnormal. ', 'Playing with various real-world domain data and simulations, we concluded that the true positives metric works sufficiently well. At the same time, the false positives metric should be decreased to avoid or decrease the number of noise alerts. ', 'The labeling tool allows the user to mark abnormal cases as normal or abnormal. The precision metrics are calculated using labeled data:', '$$', '\nPrecision = \\frac{TP}{TP + FP}', '\n$$', 'where:', 'The labeling sparsity (LS) is a measure of labeling coverage:', '$$', '\nLS = \\frac{Labeled Anomalies}{Total Anomalies}', '\n$$', 'So the precision and labeling sparsity allow for the construction of KPIs for the anomaly detection quality:', 'The first one is achieved by the labeling tool consuming effort. It is required for the precision calculation and relevance. The expected precision is achieved by tuning the likelihood threshold and in some cases by more relevant metrics onboarding to the anomaly detection. \xa0For example, when garbage collector metrics have changes in their signal distribution and lead anomalies, but the application is not impacted, then it is required to replace those metrics by a more relevant system or application metrics such as: heap size, available memory, latency, throughput, etc.', 'In cases where it is not possible to achieve an appropriate value for precision via the listed ways, it is necessary to conduct the tuning in the alerts configuration step. There are two threads to do it:', 'The solution internal processes are not visible for an end-user but it is possible to provide or modify their configuration, onboard data, and visualize processes behaviour and output. All these things are available via the solution tools:', 'The Anomaly Detection Dashboard web application is the solution entrypoint and management portal. It provides tools to configure data ingest, new domains onboarding, ML models, and anomaly detection parameters.', 'The tool is designed to minimize the data science effort in ML models design and tuning. All model hyperparameters are configured with default values that are appropriate for most cases. It means that any analyst or production engineer can start to work with anomaly detection. At the same time the solution allows for tuning of the hyperparameters by creating a new version of the model and validating anomaly detection in real-time for new parameter values.', 'There are two types of hyperparameters:', 'It is possible to manage the training and detection processes using Train and Detect flags. The Detect flag enables HTM model prediction and anomaly detection logic. The Train flag enables HTM model learning (training). The HTM model is trained in real-time. Each new metric value is used for model training and for model inference (anomaly detection). So it is possible to suspend training if a user makes a decision that a signal pattern has been defined and all future changes of a signal behaviour can be explained as abnormal. Conversely, if the user needs to continue learning to define new pattern properties, then it is possible to resume training using this flag.', 'The Anomaly Detection Dashboard configures notifications about anomalies as emails sent to recipients such as production and support engineers or customers. It also provides a labeling tool to collect user experiences in anomaly validation. ', 'The Anomaly Detection Dashboard is an anomaly monitoring panel. It visualizes anomaly statistics and service dependencies graphs to assist in Root Cause Analysis (RCA). It highlights anomalies and anomaly graph states in real-time or may be used for historical anomaly analysis.', 'It also provides a Metric Generator, which allows for the building of a simulation of user services and metrics, or trying various ingest channels and anomalies. This essentially allows users to trial a wide range of metrics and services to gain a comprehensive picture of the solution and tools.', ' is used by thousands of companies to monitor everything from infrastructure like power plants to applications and beehives. The anomaly detection solution provides Grafana dashboards to visualize metrics time series, anomalies highlights, ML models behaviour as distribution of anomaly scores and likelihood.', 'Grafana dashboards are referenced by metrics links visualized in Anomalies graph in the Anomaly Detection Dashboard. ', 'Now it is time to describe anomaly detection use-cases covered by the solution implementation.', 'The Anomaly Detection Dashboard contains a predefined anomalies graph “Showcase” built with simulated metrics and services. Users can modify or create new graphs to run simulations with real-world components and data.', 'Anomalies graph is a services/components/subsystems domain dependencies chart. It displays services as nodes and logical data workflow or functional relations between them as graph edges. All nodes are “green” in the normal state. But if the solution detects an abnormal case for a metric then its service will be shown in “red”. \xa0', 'The graph shows states for specified time points and stores their histories. It means that by browsing time points (via calendar control), users can obtain current and historical states of the graph. ', 'If a user clicks on any node of the graph then a list of all its metrics available for a specified time point is shown. Those with anomalies are shown in red for the select times. ', 'It is possible to enable auto refresh to show the current state of the graph every minute.', 'Any graph plays several roles:', 'The Showcase anomalies graph demonstrates the high-level design of the current solution and simulates real-world domains. Each component of the architecture contains a list of simulated metrics. For example, the Airflow service contains the CPU Load and Available Memory metrics. ', 'On the screen below, the abnormal state of the graph is shown:', 'Three related services are in an abnormal state: Airflow, Sagemaker Model Endpoint, and Dashboard API. The Airflow CPU Load metric has an anomaly. The Sagemaker Model Endpoint also has an abnormal case for the Latency metric: the Available Memory metric is abnormal for Dashboard API.', 'So it is possible to create the following hypothesis for the root cause analysis (RCA):', 'The Anomaly Detection Dashboard also contains the Statistic chart. It is a very useful component of the dashboard because it highlights anomaly impacts across two dimensions:', 'This section details the solution application for real-world cases for example, anomaly detection in the data quality profile. The customer business system collects data, which is then split across several data domains:', 'Each domain has the same list of data quality metrics:', 'Data profiles are also described by data business markers:', 'We assume that data domains have dependencies. Access data metrics impact transaction and feedback domain metrics and feedback domain metrics impact transactions.', 'The first step in the anomaly detection process is data ingestion from a client data source into the anomaly detection ecosystem. The solution provides the Amazon Kinesis data stream configured for the client to transfer data. ', 'Data records should have a valid JSON format with corresponding schema. Data example:', 'There are various ways to implement the ETL process for this step. For example, users can create an Airflow DAG fetching their data from the original datasource and push records to the destination data stream consuming the Kinesis data stream hook.', 'As soon as data begins being sent via the ETL job to the solution data stream, it is necessary to add the data ingest configuration to allow the data to be onboarded for the anomaly detection pipeline. This step allows for the data source to be defined, ingest tasks partitioning, and criteria to be filtered for data pushed to the data stream. It also allows for the creation of metrics names to be represented on the solution side with specified patterns. It is managed by the Data Ingest Configuration tool provided in the Anomaly Detection Dashboard:', 'Ingested data is now available for anomaly detection. The time series for ingested data can be analyzed with the Grafana Anomalies dashboard to get minimal and maximal values or to consume original knowledge about data distribution. The anomaly detection is enabled by configuring HTM models for each metric. Its behaviour is visualized via the Grafana Anomalies dashboard:', 'Corresponding to default configuration and likelihood calculation logic, the anomaly detection will begin producing scores immediately however, real likelihood values (greater than 0.5) will be generated after lkh_learning_period + lkh_estimation_samples (288 + 100 = 388) minutes. \xa0', 'Designing the dependencies graph between data domains and their metrics with the solution Graph Management tool, it is possible to create a new panel to monitor data quality anomalies and visualize abnormal states for further RCA:', 'Alerting is implemented with emails sent by the solution job, which is configured with the Anomalies Detection Dashboard. The anomaly detection solution allows for the configuring of parameters within the SMTP server and credentials via AWS Cloud Map and AWS Secret Manager.', 'Alert configuration is created for one or more anomaly graphs. Each email contains information about abnormal metrics, time points, and anomaly distribution charts as attachments. The alerts machinery calculates aggregations for metrics of those graphs. ', 'The business rule is a logical expression that contains logical and comparison operations and aggregations names as variables. Logical expression is constructed using JSON-like DSL. It’s schema is shown below:', 'From the schema there are two aggregation variables:', 'For example:', 'It means that the logical expression is TRUE if the number of abnormal cases is greater or equal to 5 along the analyzed time period. If the logical expression (rule) has value TRUE, then a notification is sent. ', 'The user can change existing alerts or create new ones by filling out additional email and job properties:', 'The following is an example of the expected alert email content that recipients would receive:', "The solution allows users to collect user experiences in the anomaly validation process. This is achieved via the Labeling Tool, which asks each user (analyst, production engineer, data engineer etc) only one question about each anomaly along the analyzed time period: “Is the anomaly important?”. The users' answers are then stored to the Elasticsearch index.", 'This experience can then be used for model and algorithm tuning, threshold configurations, and alerts design with supervised machine learning.', 'The Labeling Tool is available with the corresponding page in the dashboard for each configured anomalies graph.', 'It visualizes metrics distribution and abnormal places at selected time points. Users answer questions for all anomalies detected along the time period.', 'Each analyzed metric must have its own HTM model. There can be several versions of the model for a metric with different hyperparameters but there can only be one active version of the model that performs predictions. ', 'In the Anomaly Detection Solution the HTM model consists of two artifacts: a NuPIC HTM model and an appropriate NuPIC Anomaly Likelihood object. The lifecycle of the model is as follows. Right after the creation of the HTM model, it contains the raw NuPIC model and likelihood objects tuned with the specified hyperparameters and is ready for training and predictions. ', 'As described in the ', ' section, the HTM model can be in two states: train or detect. After some time, a user can turn off model training, after which the model will only make predictions. It also periodically makes backups of the model in storage, no matter what state the model is in.', 'A Spark Streaming job executes inference logic: every minute it sends data for an individual metric to an appropriate HTM model to generate an anomaly score and anomaly likelihood. If the received data was known for the model, then the anomaly score is zero. If the data was not known, then the score is one. Partially known data has a score between zero and one. ', 'The anomaly likelihood shows the probability of anomaly behaviour in the context of historical changes of metric data. If the received anomaly likelihood is greater than the predefined likelihood threshold for this metric, then the data point is an anomaly.', 'The HTM model is hosted on a SageMaker Endpoint in a custom Docker container. One SageMaker model contains several HTM models. Each HTM model is loaded to RAM for the purposes of real-time training and inferences.', 'There are two ways for HTM models to be loaded to a SageMaker Endpoint. The first is to create a new model via a user request and the second is to import an already existing HTM model from the Amazon S3 bucket to a SageMaker Endpoint. ', 'The creation process of a new HTM model is shown on the HTM model workflows diagram and labeled with the number 1. In this case, meta information about the model (including its hyperparameters and parameters of likelihood calculation) is persisted to the database. Anomaly likelihood objects and a new NuPIC HTM model are then created in RAM and immediately persisted to the Amazon S3 bucket. It is necessary to be able to import this new model if the user changes the endpoint of the model for example. ', 'The import functionality is implemented by using Apache Airflow (the process is labeled with the number 2 on the workflows diagram). Import DAG periodically checks whether the HTM models are loaded into the appropriate endpoints. It retrieves the mapping between the HTM models and SageMaker Endpoints from the database, then sends requests to endpoints to determine whether it needs to import an HTM model or not. If a SageMaker Endpoint does not contain a specified HTM model then it downloads it from S3 bucket. At the same time, the Import DAG periodically checks the status of that process and notifies if any errors have occurred.', 'The Anomaly Detection Solution provides export functionality, which allows for the periodical backup of all HTM models to S3 bucket and thereby saves their learning progress. It is also implemented using Apache Airflow (the process is labeled with the number 3 on the workflows diagram). ', 'The logic of the Export DAG is as follows. It first maps the HTM models and SageMaker Endpoints from the database and sends export requests to the appropriate endpoints. Each endpoint prepares the HTM models for export: temporarily disables learning of the model and packs it with an appropriate likelihood into a single artifact. As soon as Export DAG knows that the artifact is ready, it downloads it to the filesystem of the Airflow virtual machine, fits the artifact to the specific structure, then uploads it to S3 bucket.', 'The current solution is built to implement an end-to-end real-time anomaly detection pipeline. It contains tools to manage data processing units, build ML models, and visualize abnormal behaviour. ', 'The solution is available as an Amazon marketplace service. It contains showcases with simulated data, which allows the user to touch and use all the features and steps of the pipeline. It is ready for real-world anomaly detection cases and customer data onboarding and is able to be scaled up to increase the number of processed metrics. ', 'The core property of the solution is real-time machine learning. It is implemented with the Nupic framework and helper processes, which are integrated to the anomaly detection workflow. ', 'It is deployed with a “one-click” procedure, which requires that the user configure the deployment properties only. It consumes Amazon Cloud Services and components are deployed and configured with Grid Dynamics’ Analytical Data Platform. ', 'Grid Dynamics proposes that the solution can be extended and further customized for specific business cases spanning computer vision, NLP, fraud detection, and many other potential uses. '], 'date': '\r\n\t\t\t\t\t\t\tJul 15, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tMichael Ionkin', 'author_url': 'https://blog.griddynamics.com/author/michael-ionkin/', 'tag': ['Anomaly detection', 'Machine Learning and Artificial Intelligence', 'Data Science', 'Data Quality', 'Outlier analysis', 'AWS', 'ADP']}, {'title': 'Turn Data Into Insights Faster With Grid Dynamics Analytical Data Platform Accelerator on AWS Cloud', 'article url': 'https://blog.griddynamics.com/turn-data-into-insights-faster-with-grid-dynamics-analytical-data-platform-accelerator-on-aws-cloud/', 'first 160': ['Every company wants to take advantage of their data to turn it into actionable insights. It can be used to build customer 360, reduce customer churn, plan marketing campaigns, and optimize pricing, inventory, and supply chain. Data is also key to increasing productivity and the efficiency of the workforce. In order to manage increasing volume, velocity, and variety of data without sacrificing quality, security, and accessibility, companies need to build a robust analytical data platform.', 'To reliably generate high-quality insights, augment decision making with AI and ML, and increase the level of business intelligence, the data analytics department needs to follow three steps:', 'Steps two and three are usually repeated in that new data and insights are continually added and generated over time. But business value is only ever created in Step 3 and is dependent on the quality of the insights that are generated. So while executives want to get to insights as soon as possible, without a robust foundation in the form of an analytical data platform, data analysts, scientists, and engineers will not be productive. ', 'To help companies reduce the cost, effort, time, and risk of building analytical data platforms, Grid Dynamics together with AWS created an accelerator that should satisfy the data analytics needs of small technology startups and large enterprises.', 'In the ', ', we analyzed why traditional data lakes do not satisfy the needs of modern data analytics teams. The blueprint of an analytical data platform includes capabilities for easy and secure access to data by analysts and scientists, data governance tooling, stream analytics, data monitoring and quality, enterprise data warehousing, reporting and visualization tooling, as well as AI/ML platforms.', 'Together, all these capabilities facilitate ', ' and MLOps processes and provide a solid foundation for data scientists, analysts, and engineers to generate business value quickly and reliably.', 'Companies can use the analytical data platform accelerator in three different cases:', 'No matter what the starting point, the analytical data platform accelerator built by Grid Dynamics and AWS can provide the following benefits:', 'The analytical data platform is built with AWS cloud services, open source components, and includes other Grid Dynamics accelerators for data quality, data monitoring, and anomaly detection. It has a modular structure, so companies don’t have to provision the entire platform. Although the core accelerator is industry-agnostic, it contains two sample AI/ML use cases from retail industry, which demonstrate the end-to-end data and ML pipeline, image recognition for automatic product attribution, and promotion planning.', 'The high-level architecture of the accelerator is outlined in the diagram below:', 'The following capabilities of the analytical data platform are implemented as separate modules and can be provisioned independently:', 'Following enterprise cloud best practices, different parts of the accelerator can be provisioned in separate VPCs or cloud projects.', 'This is an example of a single cloud project installation:', 'The easiest way to get started with the analytical data platform is by using the AWS Service Catalog. In advanced cases, a company may decide to provide the platform via a self-service portal. ServiceNow, Jira, or custom portals can be integrated with the AWS Service Catalog to orchestrate provisioning of necessary capabilities and AI use cases, guaranteeing compliance with the internal security and IT policies.', 'In the next article we’ll provide a step by step getting started guide for provisioning the solution using the Service Catalog.', 'Grid Dynamics offers services to plan, design, prototype, integrate, and implement the analytical data platforms accelerator in the client ecosystem, along with onboarding of batch and streaming data sources, migration of data and platform components from on-premise to the cloud, and implementation of required AI/ML use cases. The typical engagement models can be found below:', 'Building an analytical data platform can be a daunting and difficult task. Most companies want to get to insights as soon as possible and avoid investing too much time and effort into the foundational capabilities. To address this need, Grid Dynamics created a pre-integrated accelerator for the modern analytical data platform and published it on AWS service catalog. We are excited about this new solution and welcome everybody to use it to increase their speed to market and reduce the risk and effort of implementing the platform from scratch. If you are interested in a demo or would like to explore how the accelerator can help you, ', '.'], 'date': '\r\n\t\t\t\t\t\t\tOct 20, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Mezhensky', 'author_url': 'https://blog.griddynamics.com/author/dmitry-mezhensky/', 'tag': ['data platform', 'batch jobs', 'streaming jobs', 'data governance', 'Data Quality', 'data monitoring', 'ML Platform', 'MLOps', 'DataOps', 'AWS', 'CloudFormation', 'CICD']}, {'title': '5 Technology Enablers for DataOps', 'article url': 'https://blog.griddynamics.com/5-technology-enablers-for-dataops/', 'first 160': ['After the initial excitement that data lakes would help companies maximize the utility of their data, many companies became disillusioned by rapidly diminishing returns from their big data efforts. While it was easy to put large volumes of data in the lakes, turning that data into insights and realizing value from it turned out to be a much more difficult task.', 'Many of these problems were related to poor quality of data, lack of alignment between business and technology, lack of collaboration, and absence of proper tooling. When software development faced similar challenges, Agile and DevOps techniques helped solve the problem. ', 'In the world of data, the industry invented the term ', ', which takes Agile and DevOps principles and applies them to data engineering, science, and analytics. We are not going to focus on DataOps techniques in this post, as there are many ', ' ', ' on the topic. Instead, we will focus on the technology enablers that facilitate DataOps implementation.', ' we wrote about how data lakes are insufficient to satisfy enterprise data analytics needs and that companies need to build fully featured analytical data platforms. In this article we will explore the capabilities of an analytical data platform that facilitates a DataOps methodology and helps companies derive value from the data more quickly and efficiently.', 'Modern analytical data platforms contain thousands of data transformation jobs and move hundreds of terabytes of data in batch and real-time streaming. Manual management of complex data pipelines is an extremely time consuming and error-prone activity, leading to stale data and lost productivity.', 'The goal of automated data orchestration is to take the effort of scheduling execution of data engineering jobs off the shoulders of data engineering and support teams and automate it with tools. A good example of an open source data orchestration tool is Apache Airflow, which has a number of benefits:', 'Some DataOps articles refer to ', ', which we call data monitoring. Data monitoring is the first step and precursor to data quality. The key idea behind data monitoring is observing data profiles over time and catching potential anomalies. ', 'In the simplest form, it can be implemented by collecting various metrics of the datasets and individual data column, such as:', 'Then for each metric, the system would calculate a number of usual statistics, such as:', 'With this information, we can observe whether the new data item or dataset is substantially different from what the system has observed in the past. The data analytics and data science teams can also use collected data profiles to learn more about data to quickly validate some hypotheses.', 'The simple methods of data monitoring can be augmented by AI-driven anomaly detection. Modern anomaly detection algorithms can learn periodic data patterns, use correlations between various metrics, and minimize the number of false positive alerts. To learn more about this technique, read our recent article on various approaches to ', '. To simplify adding anomaly detection to the existing analytical data platform, we have implemented an accelerator, which you can learn more about in ', ' or ', ' to us to try it out.', 'While data monitoring helps data engineers, analysts, and scientists learn additional details about data and get alerted in case of anomalies, data quality capabilities take the idea of improving data trustworthiness, or veracity, to another level. The primary goal of data quality is to automatically detect data corruption in the pipeline and prevent it from spreading.', 'Data quality uses three main techniques to accomplish that goal:', 'If a team already uses automated data orchestration tools that support configuration-as-code such as Apache Airflow, data quality jobs can be automatically embedded in the required steps between, or in parallel to, data processing jobs. This further saves time and effort to keep the data pipeline monitored. To learn more about data quality, please refer to ', '. To speed up implementation of data quality in the existing analytical data platforms, we have implemented an accelerator based on the open source technology stack. The accelerator is built with cloud-native architecture and works with most types of data sources.', 'Data governance is a ubiquitous term that also encompasses people and process techniques however, we will focus on the technology and tooling aspects of it. The two aspects of data governance tooling that have become absolute must-haves for any modern analytical data platform are the data catalog and data lineage. ', 'Data catalog and lineage enable data scientists, analysts, and engineers to quickly find required datasets and learn how they were created. Tools like Apache Atlas, Collibra, Alation, Amazon Glue Catalog, or Data Catalogs from Google Cloud and Azure can be good starting points in implementing this capability.', 'Adding data catalog, data glossary, and data lineage capabilities increases productivity of the analytics team and improves speed to insights.', 'The concept of DevOps is one of the cornerstones and inspirations behind the DataOps methodology. While DevOps relies on culture, skills, and collaboration, modern tooling and a lightweight but secure continuous integration and continuous delivery process helps with reducing time-to-market when implementing new data pipelines or data analytics use cases.', 'As is the case with regular application development, the continuous delivery process for data needs to follow ', '. Such best practices allow the organization to scale, decrease time to implement and deploy new data or ML pipelines, and improve overall quality and stability of the system.', 'While having many similarities with application development, continuous delivery processes for data have their own specifics:', 'Traditional tooling such as GitHub or other Git-based version control systems, unit testing and static code validation tools, Jenkins for CI/CD, and Harness.io for continuous deployment, find their principal use in the data engineering world. Using data pipeline orchestration tools, which allow configuration-as-code such as Apache Airflow, streamline the continuous delivery process even further.', 'DataOps has become an important methodology for a modern data analytics organization. As is the case with Agile and DevOps in traditional software development, DataOps helps recognize value sooner and achieve business goals in a more reliable way. To be successful with DataOps, companies need to learn new skills, adjust their culture, collaboration, processes, and extend their data lakes with a set of new technical capabilities and tools. ', 'At Grid Dynamics, we’ve helped Fortune-1000 companies adopt a DataOps culture, onboard required processes, acquire the necessary skills, and implement the needed technical capabilities. To help our clients get to insights faster, we’ve created accelerators for all necessary capabilities including data orchestration, data monitoring, data quality, anomaly detection, and continuous delivery. To learn more about the case studies on implementing these capabilities at the enterprise scale, ', '. To try our accelerators, see the demos, and discuss how to onboard them, please feel free to ', ' to us.'], 'date': '\r\n\t\t\t\t\t\t\tOct 15, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Mezhensky', 'author_url': 'https://blog.griddynamics.com/author/dmitry-mezhensky/', 'tag': ['data platform', 'batch jobs', 'streaming jobs', 'data governance', 'Data Quality', 'data monitoring', 'ML Platform', 'MLOps', 'DataOps', 'AWS', 'CloudFormation', 'CI/CD']}, {'title': 'From Data Lake to Analytical Data Platform', 'article url': 'https://blog.griddynamics.com/from-data-lake-to-analytical-data-platform/', 'first 160': ['Over the last ten years, the role of data in modern enterprise has continued to evolve at a rapid pace. Companies launched initiatives to define and execute their data strategy, appointed Chief Data Officers, and created large teams to collect, manage, and generate insights from data. With increased awareness and higher demands for data, the concept of an analytical data platform evolved as well. ', 'It is now no longer enough to implement a data lake to manage increased ', ' of corporate data assets. The ', ', or trustworthiness of data has also become vital along with accessibility, ease of use, and support of DataOps and MLOps best practices. In this article, we will describe the blueprint for the modern analytical data platform and the journey from a data lake to a modern data analytics toolset.', 'In the good old days, when data was small and structured, OLAP cubes ruled the world of analytics. EDW technologies and analytical databases coupled with reporting and limited machine learning capabilities were enough to manage corporate data assets and satisfy corporate analytics needs. The number of data sources was manageable and the vast majority of data collection was done in batches on a weekly or daily basis via ETLs.', 'But the digital era brought changes that traditional analytics ecosystems couldn’t deal with well:', 'EDWs couldn’t handle the new volumes, variety, and velocity of data. The new generation of systems called data lakes, which were often based on Hadoop (HDFS) and Spark technologies, took over the analytics world.', 'These data lakes quickly matured and by the mid-2010s most companies had deployed data lakes and used them to aggregate all enterprise data assets. The maturity of cloud computing was lagging so most deployments were done on-premise in data centers. It was surprisingly easy to dump the data in the lakes and the cost of storing data in the lakes wasn’t prohibitively large.', 'Unfortunately, getting value out of data was becoming a much bigger problem. The early data lakes had a variety of issues:', 'All those challenges led to a rapid deterioration of trust in the data from analysts, scientists, and business stakeholders. Companies quickly saw diminishing returns and observed their data lakes turning into ', '.', 'By the mid 2010’s, the simplicity of getting data into the lakes resulted in many companies recognizing they had simply created data swamps. In many ways, it was a reasonable position to reach in the journey towards a truly data-driven organization. However, there is no excuse to repeat the same mistake in 2020 and get stuck with a basic data lake. ', 'After helping a number of Fortune-1000 companies and tech startups build analytical data platforms from scratch or migrate them to the cloud, Grid Dynamics created a technology blueprint for a modern analytical data platform.', 'When coupled with the right data governance, DataOps, and MLOps best practices, processes, and organizational structure, it provides a robust set of capabilities to satisfy the data, analytics, and AI/ML needs of any enterprise. Let us briefly describe its capabilities.', 'The data lake still plays a central role in the picture. All data ingest and processing initially goes through the data lake. However, the new data lake is not limited to storing and processing data. It has a number of important capabilities:', 'The lake has two major ways to ingest data: batch and streaming. Batch processing is used to get data from services via API calls, import files, and database dumps. Stream processing relies on modern messaging middleware such as Kafka, or cloud services. Products such as Lenses.io help to maintain data quality at this level. The streaming data ingestion is not limited to clickstream and is often coupled with application design patterns such as ', '.', 'While solving a number of new problems, data lakes do not completely replace enterprise data warehouse systems. EDW is an important part of a modern analytical data platform and helps with fast and secure access to structured and pre-processed data. To ensure high performance and scalability, most reporting and visualization tooling should be implemented on top of EDWs instead of directly having access to data lakes. In many cases, data scientists will find it more convenient to work with “golden data” in EDW than having direct access to data lakes.', 'Reporting and visualization remain important capabilities of any analytical data platform. Most business users still interact with data via reporting and visualization tooling instead of getting direct access to EDWs or data lakes.', 'The rise of AI and machine learning in recent years increased the importance of augmenting human decision-making processes. To be able to scale AI/ML efforts at the enterprise scale, and implement efficient MLOps processes, data scientists need a platform. Modern AI/ML platforms have a number of capabilities to make the life of data scientists easier including:', 'The AI/ML platform as well as the MLOps process deserves a separate blog post so we will not get into a detailed discussion here.', 'As with any code, data processing pipelines have to be developed, tested, and released. Analytical data platforms often miss CI/CD capabilities, increasing the number of defects in data processing and leading to poor quality of data and unstable system performance. At the same time, DevOps and continuous delivery processes for data pipelines have their own specifics:', 'Overall, the principles we outlined in the article about the ', " apply to testing of data pipelines. Data pipelines and data processing jobs can be thought of as microservices in the regular application architecture. Instead of API contracts in the case of microservices, data pipelines' contracts are files or data streams.", 'There are many technology options for each capability in the blueprint. Having practical experience with many of them, we will give several examples in the table below: one based on open source stack, one based on 3rd party paid products and services, and one based on cloud services of the three major cloud providers: Amazon AWS, Google Cloud, and Microsoft Azure.', 'Building an analytical data platform with all the necessary capabilities may seem like a daunting task. No one wants to spend months building a core platform from scratch, learning, and making mistakes in the process. The “speed to insights” is too important. ', 'At Grid Dynamics we help companies large and small build a robust platform from scratch, upgrade their existing lakes into enterprise-ready analytical data platforms, and migrate their data lakes to the cloud while upgrading them. Whether you want to build a platform from scratch, migrate on-premise data to the cloud, or upgrade an existing cloud platform, we can recommend the best technology stack depending on the current state of the business and plan the journey towards the analytical data platform together.', 'To further speed up the implementation of the core platform, we have implemented a number of accelerators for the entire platform and specific capabilities such as data quality, data monitoring, anomaly detection, and machine learning.', 'The data analytics needs of modern enterprises extend far beyond basic data lakes. To increase accessibility and trustworthiness of data and realize the value of data, companies need to implement capabilities such as data governance, access layers, API, data quality, and AI/ML platforms. Utilizing existing blueprints and accelerators that combine the best products from open source, 3rd party products, and cloud service offerings helps to minimize the time spent on building scaffolding and foundation, achieve high stability, and arrive at implementing value-added AI and ML capabilities faster. ', 'At Grid Dynamics we’ve helped Fortune-1000 companies across industries design, implement, and upgrade their existing cloud analytical data platform or migrate an on-premise platform to the cloud. ', ' to schedule a briefing or workshop to explore more details of the capabilities of modern analytical data platforms, technology options, cloud migration journeys, and optimized paths to upgrade. '], 'date': '\r\n\t\t\t\t\t\t\tOct 06, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Mezhensky', 'author_url': 'https://blog.griddynamics.com/author/dmitry-mezhensky/', 'tag': ['data platform', 'batch jobs', 'streaming jobs', 'data governance', 'Data Quality', 'data monitoring', 'ML Platform', 'MLOps', 'DataOps']}, {'title': 'When microservices migration doesn’t make sense', 'article url': 'https://blog.griddynamics.com/when-microservices-migration-doesnt-make-sense/', 'first 160': ['While microservices architecture is a great way to design modern systems, most enterprise applications are still built as monoliths. While working with both Fortune-1000 companies and startups we often hear the same three questions:', 'Although the exact answer to each question depends on the specifics of each case, in this article we will provide answers to some of these questions and describe when it doesn’t make sense to implement or migrate to microservices architecture. In this article, we intentionally avoid technical details and focus on the management side of the issue.', 'Before we answer the main question of this article, we would like to analyze what benefits and value microservices can bring to a company. Roughly speaking, microservices architecture helps address the problem of scale. The problem of scale, in turn, can manifest itself in two main ways at a high level:', 'The first is the “', '” dimension of scale, while the second is the “', '” dimension. Notice that we do not mention the volume of code, number of features, or complexity of the functionality here. All of these factors are derivative from the first two and don’t directly influence the architectural decision.', 'There are two management know-hows that explain the value of microservices in addressing the “space” scale problem:', 'When an organization needs to implement a large system, and especially when it needs to do it quickly, it will have to scale to tens, hundreds, or even thousands of engineers. To make those engineers productive, an organization will need to split them into “two-pizza” teams.', 'In order for these teams to maintain high output and deliver large numbers of features over a period of time, they will need to work with some degree of independence and isolation. And the best way to do it is to split the whole system into services with very well defined contracts and APIs. To ensure high levels of productivity, the release process will have to support independent releases of these services, and the entire source code management and CICD will have to provide a good degree of independence. We spoke about designing such a release process in ', '.', 'Understanding the underlying principles of solving the “space” scale problem with microservices gives a partial answer to the questions of when to use the microservices architecture and how micro to go. Different organizations can get to the problem of scale in different ways. Some will start big and will have to design the system for scale. Some will start small and will scale gradually. The latter ones will often find themselves working on a large monolithic codebase and face challenges with deteriorating productivity and observe diminishing returns when scaling the team beyond a certain size.', 'What if the size of the team is small and will likely stay small for a long period of time? The single teams or even single developers trying to implement microservices architecture will often find themselves in this situation when formally they have used microservices architecture, but they:', 'It is hard to blame the team when it happens. Microservices architecture brings inefficiencies, and good developers hate inefficiency. When you and your teammates know the entire system and communicate often, the team will gravitate towards a monolith. In large-scale environments with multiple teams, creation of shared libraries and negotiation of API changes gets difficult, because of an overhead on human-to-human communication.', 'In this case, does it make sense to implement microservices architecture and have several truly “micro” services per team or per developer? To answer this question, we should look at the second aspect of the problem of scale - the “time” dimension. If a system will be actively developed for many years, there will be times when parts of the system will need to be rewritten. There are many reasons for this:', 'If a system is designed as a monolith with a lack of APIs and well-defined contracts it will be difficult to replace one part of the system with a new one, remove and retire a part of the system, or add a new feature. So microservices architecture is useful to address the “time” dimension of scale. However, in this case it may not make sense to invest into independent CICD and retain a monolithic release process.', 'Looking at the benefits that microservices architecture and release processes provide, and the problems of scale they help address along dimensions of “space” and “time”, we can now summarize when it does and doesn’t make sense to implement or migrate to microservices architecture.', "It usually doesn't make sense to migrate to microservices architecture when you don’t have scalability problems in both “space” and “time” dimensions. Practically speaking, it means that migration doesn’t make sense when you do not expect new massive feature development or growth of the system. Even if the existing team is large, but the development is expected to stay at the current level or to decrease, migration may bring more challenges than solve problems due to the changes in the culture, process, skills, and technologies required. The only reason why you may consider migration in this case is if the productivity of the team is very low and keeps degrading, and while the development won’t grow dramatically, it will stay large for a long time. ", 'When considering a new development, it makes sense to start with microservices architecture in almost every case, with the exception of small applications that will not be developed for a long period of time. In case development is expected to continue for a long time, but the team is expected to stay small as well, it makes sense to invest in microservices architecture but keep the monolithic release process.', 'Microservices architecture is an excellent tool to address organizational scalability and development longevity problems in software engineering. However, it brings its own inefficiencies and can be avoided if an organization doesn’t have scalability and productivity problems. Small teams can be productive with monolithic release processes, and small short-lived applications can be implemented cheaply and efficiently with monolithic architecture. ', ' to learn more about how to solve organizational scalability and efficiency problems with microservices architecture and a continuous delivery process.'], 'date': '\r\n\t\t\t\t\t\t\tOct 05, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tKirill Evstigneev', 'author_url': 'https://blog.griddynamics.com/author/kirill-evstigneev/', 'tag': ['Cloud', 'DevOps', 'continuous delivery', 'CICD', 'microservices architecture', 'modernization', 'continuous deployment', 'release management', 'monolith']}, {'title': '5 strategies to reduce cloud cost', 'article url': 'https://blog.griddynamics.com/5-strategies-to-reduce-cloud-cost/', 'first 160': ['After initial migration to the cloud, companies often discover that their infrastructure costs are surprisingly high. No matter how good the initial planning and cost estimation process was, the final costs almost always come in above expectations. ', 'On-demand provisioning of cloud resources can be used to save money, but initially it contributes to increased infrastructure usage due to the ease and speed at which the resources can be provisioned. But companies shouldn’t be discouraged by that. And infrastructure teams shouldn’t use it as a reason to tighten security policies or take flexibility back from the engineering teams. There are ways to achieve both high flexibility and low cost but it requires experience, the right tooling, and small changes to the development process and company culture. ', 'In this article, we present five strategies that we use to help companies reduce their cloud costs and effectively plan for cloud migration.', 'In one of our ', ' we discussed how companies can migrate to microservices but often forget to refactor the release process. The monolithic release process can lead to bloated integration environments. Unfortunately, after being starved for test environments in the data center, teams often overcompensate when migrating to the cloud by provisioning too many environments. The ease with which it can be done in the cloud makes the situation even worse.', 'Unfortunately, a high number of non-production environments don’t even help with increasing speed to market. Instead, it can lead to a longer and more brittle release process, even if all parts of the process are automated.', 'If you notice that your non-production infrastructure costs are getting high, you may be able to reduce your total cloud costs by implementing a lightweight continuous delivery process. To implement it, the key changes would include:', 'See our ', ' that goes into more detail on the subject.', 'The lift and shift strategy of cloud migration is becoming less and less popular but only a few companies choose to do deep application modernization and migrate their workloads to containers or serverless computing. Deploying applications directly on VMs is a viable approach, which can align with immutable infrastructure, infrastructure-as-code, and lightweight CICD requirements. For some applications, including many stateful components, it is the only reliable choice. However, VM-based deployment brings infrastructure overheads.', 'Containers improve resource (memory, CPU) utilization for approximately 30% compared to VM-based workloads because of denser packing and larger machines. Asynchronous jobs further improve efficiency by scavenging unused capacity.', 'The good news is that container platforms have matured significantly over the last few years. Most cloud providers support Kubernetes as a service with Amazon EKS, Google GKE, and Azure AKS. With only rare exceptions of sine packaged legacy applications or non-standard technology stacks, the Kubernetes-based platform can support most application workloads and satisfy enterprise requirements. ', 'Whether to host stateful components such as databases, caches, and message queues in containers is still open for choice but even migrating stateless applications will reduce infrastructure costs. In case stateful components are not hosted in container platforms, cloud services such as Amazon RDS, Amazon DynamoDB, Amazon Kinesis, Google Cloud SQL, Google Spanner, Google Pub/Sub, Azure SQL, Azure CosmosDB, and many others can be used. We have recently published an article comparing a subset of ', ' and ', '.', 'More advanced modernization can include migration to serverless deployments with Amazon Lambdas, Google Cloud Functions, or Azure Functions. Modern cloud container runtimes like Google Cloud Run or AWS Fargate offer a middle ground between opinionated serverless platforms and regular Kubernetes infrastructure. Depending on the use case, they can also contribute to infrastructure cost savings. As an added benefit, usage of cloud services reduces human costs associated with provisioning, configuration, and maintenance.', "There are two types of scalability that companies can implement to improve the utilization of cloud resources and reduce cloud costs: reactive auto-scaling and predictive AI-based scaling. Reactive autoscaling is the easiest to implement, but only works for stateless applications that don’t require long start-up and warm-up times. Since it is based on run-time metrics, it doesn't handle well sudden bursts of traffic. In this case, either too many instances can be provisioned when they are not needed, or new instances can be provisioned too late, and customers will experience degraded performance. Applications that are configured for auto-scaling should be designed and implemented to start and warm up quickly.", 'Predictive scaling works for all types of applications including databases, other stateful components, and applications that take a long time to boot and warm up. Predictive scaling relies on AI and machine learning that analyzes past traffic, performance, and utilization and provides predictions on the required infrastructure footprint to handle upcoming surges or slow downs in traffic.', 'In our past implementations, we found that most applications have well-defined daily, weekly, and annual usage patterns. It applies to both customer-facing and internal applications but works best for customer applications due to natural fluctuations in how customers engage with companies. In more advanced cases, internal promotions and sales data can be used to predict future demand and traffic patterns.', 'A word of caution should be added about scalability, regarding both auto-scaling and predictive scaling. Most cloud providers provide discounts for stable continuous usage of CPU capacity or other cloud resources. If scalability can’t provide better savings than cloud discounts, it doesn’t have to be implemented.', 'To take advantage of both dynamic scalability and cloud discounts for continued usage of resources, a company can implement on-demand provisioning of low-priority workloads. Such workloads can include in-depth testing, batch analytics, reporting, etc. For example, even with lightweight CICD, a company would still need to perform service-level testing or integration testing, in test or production environments. The CICD process can be designed in such a way that heavy testing will be aligned with the low production traffic. For customer-facing applications, it would often correspond to the night time. Most cloud providers allow discounts for continued usage even when a VM is taken down and then reprovisioned with a different workload, so a company would not need to sacrifice flexibility in deployments and reusing existing provisioning and deployment automation.', 'The important aspect of on-demand provisioning of environments is to destroy them as soon as they are not needed. Our experience shows that engineers often forget to shut down environments when they don’t need them. To avoid reliance on people, we implement shutdown either as a part of a continuous delivery pipeline and implement an environment leasing system. In the latter case, each newly created on-demand environment will get a lease and if an owner doesn’t explicitly renew the lease it will get destroyed when the lease expires. Separate monitoring processes and garbage collection of cloud resources are also often needed to ensure that every unused resource will get destroyed.', 'An additional cost saving measure that we effectively used in several client implementations is usage of deeply discounted cloud resources that are provided with limited SLA guarantees. Examples of such resources are spot (AWS) or preemptible (GCP) VM instances. They represent unused capacity that are a few times cheaper than regular VM instances. Such instances can be used for build-test automation and various batch jobs that are not sensitive to restarts.', 'The famous maxim that you can’t manage what you can’t measure applies to cloud costs as well. When it comes to monitoring of cloud infrastructure, an obvious choice is to use cloud tools. To make the most out of cost monitoring, cloud resources have to be organized in the right way to be able to measure costs by:', 'While the first points might be obvious, the last one might require additional clarification. In modern continuous delivery implementations, nearly every commit to source code repository triggers continuous integration and continuous delivery pipeline, which in turn provisions cloud infrastructure for test environments. This means that every change has an associated infrastructure cost, which should be measured and optimized. We have written more extensively about measuring change-level metrics and KPIs in the ', '.', 'Multiple techniques exist to properly measure cloud infrastructure costs:', 'With the proper cost monitoring and the right tooling, the company should be able to get a proper understanding of inefficiencies and apply one of the cost optimization techniques we have outlined above.', 'Cloud migration is a challenging endeavor for any organization. While it’s important to estimate cloud infrastructure costs in advance, the companies shouldn’t be discouraged when they start getting higher invoices than originally expected. The first priority should be to get the applications running and avoid disruption to the business. The company can then use the strategies outlined above to optimize the cloud infrastructure footprint and reduce cloud costs. Grid Dynamics has helped numerous Fortune-1000 companies optimize cloud costs during and after the initial phases of cloud migration. ', ' if you have any questions or if you need help optimizing your cloud infrastructure footprint.'], 'date': '\r\n\t\t\t\t\t\t\tSep 23, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tKirill Evstigneev', 'author_url': 'https://blog.griddynamics.com/author/kirill-evstigneev/', 'tag': ['Cloud', 'cloud migration', 'cloud cost', 'cost reduction', 'DevOps', 'continuous delivery', 'CICD', 'microservices architecture', 'modernization', 'continuous deployment']}, {'title': 'Avoid the monolithic release process when migrating to microservices', 'article url': 'https://blog.griddynamics.com/avoid-the-monolithic-release-process-when-migrating-to-microservices/', 'first 160': ['The trend of implementing and migrating to microservices isn’t new. The industry is full of war stories where companies invested a lot of time, money, and effort to migrate microservices architectures but were never able to realize the full benefits. Every company’s journey is unique and mistakes that companies make are unique too. However, there are some mistakes that are remarkably common across many organizations and can be avoided with a little key knowledge. ', 'In this article, we will describe one of the biggest mistakes companies make, which is quite subtle and hence often overlooked. The mistake is limiting microservices guidelines to architecture, while leaving the monolithic change management and release process. Technically the microservices architecture is implemented, but the speed to market is still slow. This results in a high cost of infrastructure and a non-scalable development process. What’s worse, teams often don’t even realize they’re making this mistake, which can leave a sour taste following the migration.', 'For the simplest way of describing how and why this mistake is made, let’s take a look at the modernization journey in the technology department of the Acme company. Imagine that one department is responsible for the digital customer experience platform. The platform was built some time ago with a monolithic architecture. It has been reliably serving its needs, helping the company grow an online presence, and delivering stable digital revenue growth. New features and modules have been steadily added to the codebase and both the platform and the team developing it gradually increased. ', 'At some point in time, the cost of continued customization and extension got too high, the speed to market got prohibitively slow, and the team made the decision to re-platform to keep up with business demand. Microservices architecture was chosen as the new standard due to its promise to scale development, facilitate continuous delivery, and increase speed to market.', 'At this point, it doesn’t matter what approach the team chose to implement the new architecture. It could’ve been strangling the monolith with gradual decoupling of microservices. It could’ve been a big bang re-platforming. It also doesn’t matter whether the platform was built in the cloud or on-premise, or what specific technology stack was chosen. ', 'But in the end, the team followed all the microservices architecture best practices and implemented a new platform:', 'Each microservice was nicely designed as a bounded context.', 'Everything was done right and by the book. And still, every change took days or weeks and required the involvement of most of the teams. Infrastructure costs increased, test environments were unstable, tests were brittle, and development productivity decreased instead of increasing.', 'The mistake that is often made is that migration to microservices doesn’t affect the release and change management process. Even if it is fully automated, both requirements are being tested and deployed in production as one monolithic system. ', 'The monolithic release process is easy to identify:', 'There may be multiple root causes for the monolithic release process:', 'Even with a microservices architecture, the monolithic release process leads to slow speed to market, a long continuous delivery pipeline, brittle environments and tests, and high non-production infrastructure costs.', 'Understanding the issue and identifying the root causes behind it are the first steps in fixing the monolithic release process. Improvements will directly follow from dealing with the specific root causes. ', 'Some specific steps that need to be taken are:', 'It is normal however if some level of integration testing still remains. Some end-to-end verification is expected on the level of UI and front-end applications. In order to increase speed to market, minimize the number of environments, and reduce infrastructure costs without sacrificing quality and stability, the lightweight continuous delivery pipeline can be implemented.', 'The lightweight continuous delivery process can be designed to satisfy all enterprise change management policies. It includes “shifting production left” or “shifting testing right”, depending on your frame of reference. Moving integration and user acceptance testing to the production environment can be implemented in such a way that it will satisfy all enterprise change management policies without disrupting production traffic and sacrificing quality. Techniques like service mesh, test data management, and smart test design should be implemented.', 'Microservices architecture has its value even with the monolithic change management process. However, moving the release process to microservices continuous delivery when each service or application can be released independently greatly increases speed to market, enables scalability of the development organization while decreasing infrastructure costs, and raises development productivity. ', 'At Grid Dynamics, we’ve helped numerous Fortune-1000 enterprises migrate to a microservices architecture and release process to recognize the maximum value from their microservices migrations. ', ' if you’d like to learn more.'], 'date': '\r\n\t\t\t\t\t\t\tSep 16, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tKirill Evstigneev', 'author_url': 'https://blog.griddynamics.com/author/kirill-evstigneev/', 'tag': ['Cloud', 'DevOps', 'continuous delivery', 'CICD', 'ADP', 'microservices architecture', 'modernization', 'continuous deployment', 'release management', 'monolith']}, {'title': 'A Universal Approach to eCommerce GUI Test Automation', 'article url': 'https://blog.griddynamics.com/a-universal-approach-to-ecommerce-gui-test-automation/', 'first 160': ['Allowing any element of a digital product to fail can have significant implications. Functional failures have tangible consequences for conversion rates, which ultimately impact the entire business. Difficulties with browsing web sites affect user impressions of the brand meaning they may very well leave and never return. This is true for any digital product but is especially the case with eCommerce, where competition is high and any failure can lead directly to customer attrition.', 'This makes testing of eCommerce sites extremely important. The good news is that modern eCommerce platforms have more or less similar main customer journeys - from account creation to purchase and post-purchase actions. In this article we will show how it’s possible to take advantage of the similarities that occur between various business operations to build a unified solution for testing eCommerce web and mobile sites.', 'What is the most common issue faced when testing eCommerce platforms? Management are always looking to speed-up changes to be delivered into production and need to be confident that no new major issues will emerge after the update. In order to be able to achieve this in any reasonable time frame, at least some ', ' must occur and test automation is a significant part of that process. ', "It is common for businesses to invest in test automation but unfortunately it's also common for them to run into the following issues:", 'You might be interested to learn that we know how to overcome each of these issues. And our approach is viable regardless of the eCommerce platform size, from giant globally renowned department stores right down to small specialized web and mobile sites.', 'At Grid Dynamics, we’ve been involved in testing eCommerce platforms from the initial establishment of the company. And we continue to do so today across multiple levels - from experimental distributed backends to customer-visible production interfaces. We have also developed a universal accelerator that enables eCommerce site automation within just a few days. UniECom - remember the name! ', 'Due to the depth of our expertise across eCommerce automation, our teams have been able to develop a stable framework that covers almost all customer journeys. It’s also \xa0versatile enough to address all of the main eCommerce test automation challenges. The principal aim of the solution is to give test automation engineers a powerful tool that can be adapted for the UI elements of all types of eCommerce customers. ', 'The following eCommerce functionalities are included:', 'This results in coverage of more than ', ' and ', '. ', 'The following schematic provides a typical end-to-end eCommerce scenario:', 'To check the possibility of adapting the accelerator for production testing, the QE team used production environments to develop the examples. The experiment’s results showed that it’s possible to apply UniECom for ', ' (while taking all relevant security issues into consideration).', 'One more factor needs to be mentioned here. While several years ago most purchases were completed directly via the web, today’s eCommerce sites accept a huge range of cross-device and mobile-only orders. Therefore it’s crucial to cover both web and mobile applications with regression testing and pay close attention to the cart merge and other cross-session functionalities. ', 'The following platforms are covered by UniECom:', 'So what makes it all possible? Let’s take a look at the dynamics of test creation by a team of two engineers and compare the traditional approach with UniECom’s approach.', 'As the chart above shows, the pace of test creation is significantly higher in first iterations. This occurs because the testing framework, test data management, and a large number of test scenarios already exist and most of the time is spent on interacting with UI elements (real testing!) as well as integrating with data sources and infrastructure.', 'Let’s summarize the main features of our accelerator solution and the benefits it can deliver:', 'Test coverage is known in advance because all test scenarios are already written and addressed for one or more business functionalities. No significant changes in the scenarios are expected.', 'Initial sets of tests can be run against production. This means they can be used on any low-level environment as well. The approach is CI-ready and provides good step-by-step reporting, which is needed to accurately analyze test results.', 'Due to the well-thought-out architecture of the testing framework and the decomposition of UI elements and test data from the test steps, it’s easy to support a test suite that is intended to be “green” all the time until an issue appears. Of course, it also requires close collaboration with the development team to reflect changes in the UI or business logic in \xa0a timely manner.', 'There is no need to wait several weeks for the first fragile tests to appear. You can receive them in just a few days, with all the tests adjusted within a few weeks.', 'There are several other positive side effects:', 'The ability of the accelerator to be rapidly adapted for use in new eCommerce customer channels like web and mobile is always a major priority for eCommerce businesses. However, our experience has shown us that eCommerce automation is very often accompanied by a number of challenges that drastically decrease automation speed. Overcoming these challenges was another important goal we had while working on the accelerator.', 'The following questions are commonly encountered when you start test automation:', 'Usually the team needs several weeks to be able to answer all of these questions. But unfortunately, the answers reached aren’t always on the mark, or can even be completely incorrect. This is where the UniECom solution shines. It has the right answers and encapsulates a set of best-practices in UI test automation as well as having a well-designed, business oriented test suite out-of-the-box.', 'Now let’s take a closer look at some of the key QE automation challenges:', 'The majority of issues can be resolved via integration with the Test Data Management System (TDM). This area will be the subject of our next in-depth article, but let’s touch on some of the key details here.', 'While working with complex eCommerce platforms you’ll realize that you need to be able to integrate with a number of data sources. And the data there should be in-sync. In addition, the operations you can do with data differ between low-level and top-level environments that require more engineering time to support these differences. ', 'Our approach of managing test data helps to unify and separate the data and test scenarios. Moreover, in the example above we used an intermittent approach, which helps to speed-up test data management even without access to original data sources. It can help to avoid the “I’m blocked” test development approach so that you can continue to work with tests in parallel, while management resolves administrative and organizational issues by accessing the data sources.', 'UniECom supports multiple “screens” - web browser, mobile, tablet etc. This means the test automation team needs only to worry about UI locators and configuration parameters to make it possible to run ', ' on different screens. Of course, if business flows differ it can require additional “test coding” but maybe this presents the opportunity to ask the important question - “Isn’t that actually a sign that the business flows should all be the same?”', 'Browsers or mobile devices from cloud farms can also be used without additional effort. Alternatively, an in-house mobile farm can be established. How can this be achieved? Make sure not to miss our article covering this in our ', ' section in the near future.', 'In contrast to multipurpose enterprise testing tools, the accelerator is targeted specifically to the eCommerce segment. Enterprise tools also come with significant price and education time requirements so represent a serious investment. By contrast, UniECom is based on open-source software so initial costs are close to zero. However, there is still a learning curve that needs to be considered (miserable if you know BDD, Java, and SQL).', 'Engineers are always greedy to experiment. They’re interested in new libraries, frameworks, and “why not?” attempts. Curiosity is good and one of the most important characteristics of a test engineer. However, sometimes it’s too expensive for the business. UniECom provides a solid framework that helps to redirect the tester’s curiosity from “how to test” to “what to test”.', 'eCommerce business is intrinsically dynamic and therefore is very likely to be subject to recurring changes. In addition, typical eCommerce site architecture is complex and commonly integrated with a huge amount of 3rd party solutions and a number of data sources. This ensures that eCommerce automated testing remains a complex challenge with long establishment times and the expectation that problems will be encountered across testing and reporting. ', 'A universal solution that is implemented specifically for eCommerce sites and takes into consideration all the gaps and issues of eCommerce applications provides a clear path to simplifying and speeding up test automation. It additionally provides the benefits of operating from the framework level up and enabling test automation teams to avoid false positive or negative reports. ', 'The UniECom test framework from Grid Dynamics addresses most of the issues discussed in this article and can reduce the initial preparation time from several months down to just a few days. It also opens up the opportunity to put in place test automation capabilities for relatively small retailers. ', ' to learn more and see it in action.'], 'date': '\r\n\t\t\t\t\t\t\tDec 09, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tLilia Urmazova', 'author_url': 'https://blog.griddynamics.com/author/lilia/', 'tag': ['QA', 'WebUITesting', 'mobile testing', 'production testing', 'eCommerce', 'Test Automation', 'GUI testing']}, {'title': 'How to replatform Endeca rules to Elasticsearch', 'article url': 'https://blog.griddynamics.com/how-to-replatform-endeca-rules-to-elasticsearch/', 'first 160': ['Merchandising is an ', '. Automated merchandising platforms allow business users to control search and browse results by affecting product ranking decisions, providing curated content and referring customers to relevant categories or thematic landing pages.', ', once a popular product discovery platform, provided a comprehensive set of business rules to support key merchandising use cases. However, these days Endeca is a stagnating technology and many firms are looking to ', '.', 'At Grid Dynamics, we have performed several Endeca migration projects with Tier-1 online retailers, and summarised our solutions in the ', '. Support of Endeca business rules is always an important item on the roadmap. In this post we will share a few details about migration and support for Endeca business rules.', 'Conceptually, the Endeca business rule consists of two parts: ', ' and ', '. A trigger is used for rule targeting, e.g. it defines a condition in which a business rule has to be activated, or ', '.', 'Typical examples of triggers include the situation when a search phrase contains specific words, or some set of filters is applied by the user. A rule can have several triggers, any of which have to be satisfied for the rule to fire.', 'When a rule is fired, it performs ', ' This is when a merchandiser has an opportunity to affect customer experience and tune it to her liking. Here are examples of such actions:', 'You can find more details in Endeca ', ' documentation.', "Let's consider the structure of Endeca rules in more details. As an example, we are going to look at the rule type called ", ": a rule which redirects the user to a predefined page when the user's search phrase matches a particular set of words (called “search terms”).", 'Endeca supports 3 types of keyword match:', 'Let’s consider how to replatform this type of rules to Elasticsearch - a popular open source search engine based on Lucene. ', 'Elasticsearch contains a powerful ', ' feature which helps to support these three match types.', 'We will deal with the problem of triggering Endeca rules as a search problem. In a large-scale system there can be thousands, or even tens of thousands of rules, and we don’t want to evaluate each rule trigger on every request to check if the rule should fire. ', 'Second, we should support fuzzy matching of keyword-triggered rules with respect to synonyms and spelling correction, so we can still show facets for jeans if a customer types “jeanz”.', 'So, we will model the rules as a separate Elasticsearch index with nested documents where the rule will be a parent document and its triggers will be children documents. The rule document will contain a list of actions. In our simplified case this field will contain a URL to the page where the user will be redirected. Each trigger has a field with a configured search term, let’s call it a “phrase”. ', "We have to evaluate all the triggers against a customer phrase and if a trigger is matching, we will return a corresponding parent rule into the result list of fired rules. However, we cannot just search for a customer's phrase within the trigger's phrase field. This way, a customer's phrase “how to make an order” will not match a trigger’s phrase field value: “how to”. Of course, we can tokenize and analyze the customer query and carefully check whenever all words matched. However, it quickly becomes really cumbersome and slow. ", 'What we actually need is an ', ' Instead of searching for parts of the search phrase in the trigger, we should be searching for all the triggers within the search phrase. So, instead of searching for a search phrase within the index of all triggers, we are creating a temporary index containing a single document with the search phrase text and running all trigger phrases against this index checking if our single document matches each trigger.', 'Sounds complicated? Luckily, Elasticsearch has a special module called “', '” that turns a problem of inverted search into a piece of cake. This module consists of a special field type called “percolator” that provides an ability to store queries and a special “percolate” query. Percolate query takes a document as an argument and returns all queries this document matches (See ', ').', 'Let’s get our hands dirty and look at the example below. Ensure that you have a fresh version of ', ' running (mine is 7.5.1). I also suggest you install and run ', ' of the same version: all requests below are formatted to be used in this application. ', 'First, let’s create an index for rules:', 'Let’s see what is going on here:', 'Now let’s index some rules.', 'The first rule has a trigger with a “', '” type. Elastic’s “', '” query is the best for this purpose:', 'The second rule has a trigger with a “', '” type. Simple “', '” query is perfectly suitable for such a case. ', 'The last rule has a trigger with a “', ' type. Elastic’s “', '” query is used with “AND” operator.', 'Now we are ready for some searching!', 'We are going to test our little rules engine with three search phrases:', 'This is what a rules search request looks like:', "Let's test the first phrase “how to make an order”:", 'The response will look like:', 'The rule with “_id” = “1” was returned, as expected.', 'Now we are going to test the second phrase “order status”:', 'The response looks like:', 'Which means that the rule with “_id” = “2” was matched as expected by the percolator.', 'The last phrase “what is the best oven for cooking pizza”:', 'Returns the rule with “_id” = “3” as expected:', 'As you can see, we just created basic support for one of the key types of Endeca rules using Elasticsearch percolator. Of course, the real-life solution has to be a bit more complicated and support things like fuzzy matching and synonym expansion, yet it illustrates the approach well enough. \xa0We can support other kinds of Endeca rules in a similar manner.', 'In the history of our Endeca replatforming project, we often had to support this kind of trigger matching in Solr which required quite a sophisticated code in the ', ' (at the time of writing). However, an ability to match documents against queries was recently ', ' to Lucene. We expect that percolator will appear in Solr soon and we will update this blog post with a Solr-based solution.', 'In this blog post we described one of the significant problems of most Endeca replatforming projects - support for Endeca-like merchandising rules. As it turns out, it can be solved really nicely with the Elasticsearch percolator module.', 'We will continue sharing our Endeca migration experience in other blog posts. ', 'Stay tuned and happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tFeb 18, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'Retail Search']}, {'title': 'A rapid response to COVID-19 \nsupply chain and market shocks: Emerge from the crisis stronger', 'article url': 'https://blog.griddynamics.com/rapid-response-to-covid-19-supply-chain-and-market-shocks/', 'first 160': ['The COVID-19 crisis is clearly the biggest economic disruption the world has seen in the 21st century. This disruption impacts virtually all industries and all areas of enterprise operations through market demand, supply chain, and human capital.', 'The adverse impact of the COVID-19 pandemic and prolonged economic recovery will be influenced by many factors, including the public-health response and the effectiveness of government economic policy. For the business world, it clearly indicates a “survival of the fittest”  mode of operation:', 'Every crisis is an opportunity, and it is important to view the coronavirus situation not only in terms of crisis fighting but also within a strategic context. It is vital for business leaders to recognize the opportunities created by changes in the competitive landscape, consumer behavior, and supply to navigate through this challenging time and emerge from the crisis stronger. Conversely, the failure to understand these next horizons could be fatal.', 'In this article, we provide some practical recommendations to technology and business leaders on how advanced data analytics and engineering capabilities can be leveraged to rapidly and efficiently respond to the COVID-19 disruption. These recommendations are largely based on early examples of response actions we have already observed across the industries.', 'In this section, we provide an introductory case study based on one of our most recent projects for a media products company. This example highlights only one facet of the company’s COVID-19 response strategy, but it clearly showcases the importance of data analytics and the ability to react to a disruption rapidly. In the next sections, we develop a more comprehensive framework.', ' A leading developer of digital media products used a demand forecasting model to optimize pricing and promotional decisions across more than 70 countries, multiple product lines, and multiple digital and retail channels. The model was able to forecast weekly demand numbers for up to 24 months ahead with reasonable accuracy. The system generated multiple forecasts for different discount levels, and pricing managers used these forecasts to understand how price elasticity would change over time and to determine optimal promotional times and depths.', ' The company faced an unprecedented problem when the COVID-19 outbreak in China and subsequent quarantine measures rapidly changed the demand pattern, so the actual sales numbers quickly diverged from the forecast. In the case of media products, demand numbers sharply surged compared to the forecast. In a few weeks, it became clear that COVID-19 had turned into a global massive disruption that invalidated demand forecasts for all geographies.', ' The company made a decision to immediately update the forecasting models. After the initial analysis and experiments, the data science team decided to modify the existing solution as follows:', 'Figure 1. Adjustments to the demand forecasting model introduced as a part of the COVID-19 rapid response program.', ' The model adjustments were implemented in about two weeks, which helped the forecasting accuracy to return to acceptable levels and stabilized the revenue management process. This was a major contribution toward the resilience of the company in this time of crisis.', 'The analysis and modeling of demand shocks also produced useful insights for operations and leadership teams. For example, it turned out that the demand in Asian countries (e.g., China, South Korea, and Japan) surged much more rapidly after the introduction of quarantine measures compared to European countries, the United States, Canada, and Australia. This helped to segment countries into several groups and facilitate the analysis and operationalization of the insights produced. Another helpful insight for the operations teams was that the disruption affected not only the selling numbers but also product usage patterns.', 'COVID-19 is disrupting ongoing operations and imposing uncertainty on future plans, so it has become very challenging to adopt a course of action that will strike a proper balance between resolving immediate issues and positioning an organization to rebound. The development of a balanced strategy can be facilitated by framing the problem as the development of incremental advantages compared to the industry-average trajectory:', 'Data analytics and engineering capabilities can contribute significantly toward all of these categories of objectives by providing more accurate forecasts, decision support and optimization tools, and high-impact customer-facing features. We present a framework for planning and rapidly developing these capabilities in the coming sections.', 'Figure 2. A conceptual framework for planning COVID-19 response activities.', 'The value added by analytics during the COVID-19 stabilization and recovery period can be significant. Consider the following use cases that are directly linked to revenue and cost optimization:', 'Solving these use cases using descriptive and/or predictive analytics is challenging for a couple of reasons. First, the development and delivery of analytical and data science projects usually takes a considerable amount of time. Second, COVID-19 is a unique situation, so patterns and models learned from historical data might be inapplicable under these new conditions.', 'Some leading companies are working through these challenges by creating special analytics teams (S.W.A.T. teams) that are tasked to quickly develop simple but valuable tools and models for the above use cases. These teams usually include highly experienced data scientists and data analysts who are able to develop non-standard solutions rapidly. S.W.A.T. teams need to be closely integrated with execution teams and provided with strong support from COVID-19 response leadership. A typical setup is illustrated in the organizational diagram below.', 'Figure 3. An organizational framework for rapid COVID-19 response with data-driven corrections and optimizations.', 'The analytical, forecasting, and optimization problems posed by the COVID-19 disruption can be highly non-standard and thus can require innovative solutions. The following real-life examples show how companies are working through these challenges by using uncommon data sources, quick adjustments of existing models, and new forecasting capabilities:', 'Figure 4. Common adjustments to data sources and model designs in response to COVID-19 disruptions.', 'Analytical capabilities cannot be properly developed and used outside of the operational context. For example, even an accurate demand forecasting or price optimization model aligned with the new reality of coronavirus disruptions might not be helpful if the supply chain and pricing operations are not properly adjusted to operate under higher uncertainty. The common adjustments that one should consider implementing include the following:', 'We can observe today how the pandemic is changing the way in which customers want to engage with companies. From Asia and Europe to the United States, industry leaders are adopting new ways to engage with customers while addressing safety concerns and helping communities. We can say with full confidence that digital transformation has become a matter of survivability for businesses, and it will be crucial in the next several months.', 'While not all companies entered the crisis on an equal footing, corporate and technology agility is the defining factor of who will emerge as leaders after the dust settles and the new world takes shape. For example, while grocery delivery companies, such as HelloFresh, are seeing a natural surge in demand due to their business model', ', traditional retailers, such as Walmart or Walgreens, are switching to fully contact-free services', ' and are expanding drive-through shopping', ', helping them address customer concerns and win business in the long run. Companies in the media, telecommunications, and gaming industries will face challenges with retaining customers when the load on their services increases and discretionary spending shrinks. Financial services companies will face challenges with a slowing economy, along with customer issues ranging from inability to make regular payments due to job disruption to protection of wealth in the situation of a stock market crash.', 'There are three major drivers behind the emerging changes in customer engagement:', 'Figure 5. The major drivers behind recent changes in customer engagement.', 'Based on the current state of the industries, there are a number of customer engagement scenarios that companies should invest in to address the above goals:', 'The list of scenarios above is not comprehensive but gives insight into how to survive and prosper in the new world. The list of capabilities will obviously grow as conditions change.', 'While new capabilities in customer engagement can relieve stress on fulfillment and supply chains, many companies will need to invest in renovating those systems. The digital transformation of the last few years was focused on customer experience; it has become evident that without agility in back-office systems, advances in front-office technology can quickly come to a halt. For example, even when a customer experience platform works well, department stores such as Macy’s and T.J. Maxx are turning off checkout functionality due to inability to fulfill orders. Improvements in the back office will be driven by four factors:', 'A company’s back-office IT ecosystem may be too complex to transform over a couple of months solely as a reaction to the current pandemic. However, in many cases, significant business value can be created by implementing tactical changes and modernization. Short-term improvements can be achieved by modernizing applications and systems to expose APIs and to provide access to internal data—in a batch or real-time streaming mode—by adopting event-sourcing patterns, as shown in Figure 6. Such APIs will provide enough transparency and agility to optimize supply chain and fulfillment decisions in real time. AI-driven decision engines can then be created to augment traditional, manual analytics. Some AI use cases of computer vision can be implemented on the side by installing cameras in warehouses and stores and implementing an IoT infrastructure to support analytics and decision making at the edge.', 'Figure 6. An API layer on top of supply chain–related systems and data sources improves transparency and facilitates the development of decision making and optimization components.', 'A deeper digital transformation of a company’s entire ecosystem can be started later, after the immediate gaps are closed. New use cases and best practices will appear as the world changes to a novel mode of operation, and the agility of technology and corporate teams will play a bigger strategic role than immediate fixes will.', 'Companies have been talking about agility for at least a decade. This pandemic is presenting an ultimate test for companies, which will either survive and prosper or lose market share and perish. The outcome of this test will come down to the agility of a company on all levels. As this pandemic will require companies to move to digital engagement models with customers, the agility of digital and technology organizations will be paramount.', '\nA group of researchers focusing on technology organizations recently published a story proving that high-performance technology organizations are associated with higher corporate agility and better business performance', '. Furthermore, the researchers identified 24 capabilities that are required to build high-performance technology organizations. These capabilities can be grouped into five categories:', 'Although building the right culture and processes from scratch in a company is a long process, it can be sped up by injecting a high-performance culture and talent from other companies. In many cases, collaboration with vendors can jumpstart the process, given the right engagement model. In a recent study, Forrester published a number of guiding principles that companies should look for in vendors', '.', 'Investing in building agility on corporate and technology levels is a long process. But in order to succeed in this crisis, companies have to start now. In the foreseeable future, agility will be one of the primary factors that will separate winners from losers.'], 'date': '\r\n\t\t\t\t\t\t\tApr 06, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tMax Martynov', 'author_url': 'https://blog.griddynamics.com/author/max-martynov/', 'tag': []}, {'title': 'Implementing autocomplete with Solr', 'article url': 'https://blog.griddynamics.com/implementing-autocomplete-with-solr/', 'first 160': ['In recent years, autocomplete has become a staple feature for searches of all types.', 'Whether Google, Amazon, or smaller sites and vendors, predictive typing, as it’s otherwise known, (also sometimes called auto-suggest, search-as-you-type or type-ahead) has become an expected part of an engaging, user-friendly search experience.', 'Solr, an open-source framework that powers many of the world’s most popular e-commerce sites and applications, supports three approaches to auto-complete implementation:\xa0', 'Solr supports all three of these approaches via ', ', which defines how data in a given field is interpreted and queried.\xa0', 'Suppose our requirement is:', 'Match as a ', ' that starts ', ' with the phrase ________\xa0', 'If we have the “', '” phrase in our index, it ', ' show when the user types:', 'However, prediction will not occur when the user types:\xa0', 'In this case, we need the search to take into account every phrase in our index, as well as every partial phrase the user types in as a single term, and use ', ' for the suggestion field:', '...and then use wildcards when querying:\xa0', 'An alternative to using a wildcard query is the ', ' approach. It requires storing every prefix for a given phrase in the index. Let’s take “my little pony,” with this configuration as an example:', '\xa0Acceptable partial search phrases would be:', 'This approach has an obvious benefit in that a query that is powered with a larger index is not only simpler; but also performs faster, in general.', 'In this case, assume that our requirement changed to:', 'Match as a suggestion ', ' that contains the user’s phrase as a ', '.', 'Whereas, prediction should not occur when the user types:', 'Setting up the query to behave in this way isn’t hard: \xa0it requires every phrase to be split in the index. The user’s phrase also needs to be parsed into a set of words:', '...and search for:\xa0', ' AND', ' AND', 'The same edge n-gram technique mentioned above can be applied on a single-word basis, with prefixes for each individual word stored as well.', 'In this this last case, our requirement now is:', 'Match as a suggestion ', ' that contains the user’s phrase as a ', '.', 'This means “my little pony” ', ' appear when the user types:', 'Whereas, prediction wouldn’t occur in these cases:', 'One solution is to amend our first use case by adding a leading wildcard to the query:', 'The problem with applying this solution is that Solr will have to execute a ', ' to check for every term that is a possible match, rendering the query very slow. Additionally, the ', ' technique shouldn’t be applied to this use case because we have wildcards on both ends, making the query even slower.', 'Luckily, But we can go a different way by expanding Solr’s functionality a bit. ', 'First, we need to create word-based suffixes for phrases in the index. For “my little pony” they would be:', 'Solr doesn’t have built-in features to generate this, but we can implement our own token filter:', 'Every element from the generated collection of prefixes will now be matched as a suggestion if and only if \xa0“it starts identically to the user’s phrase.” \xa0By implementing our own token filter we can now apply both ', ' and ', ' to achieve the desired functionality.', 'In this article, we have covered the basics of implementing generic autocomplete requirements in Solr. But this is just the tip of the iceberg. Queries are complex, and creating a positive user experience is directly correlated to how easy it is to find a given product, service or the right information. There are any number of nuances that we can take into consideration. For instance, what if you want to match “my little pony” to “my small pony,” or even the common acronym “MLP” but do not want to match it to “my pony is too little?” These are just a few of the many advanced applications of autocomplete functionality. Be sure to watch for our next blog post on Solr, which will contain more information about how to implement autocomplete, including a \xa0', ' tutorial.', '\xa0', 'For load generation you need to create'], 'date': '\r\n\t\t\t\t\t\t\tFeb 22, 2017\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAndrey Kudryavstev', 'author_url': 'https://blog.griddynamics.com/author/andrey-kudryavstev/', 'tag': ['Search', 'Implementing autocomplete with Solr', 'Solr autocomplete', 'Solr suggester']}, {'title': 'Progressive Web Apps: Comparison of PWA with Other Approaches to Build Mobile User Experience', 'article url': 'https://blog.griddynamics.com/pwa-research/', 'first 160': ['With the increasing popularity of Progressive Web App (PWA), we thought it would be the ideal time to explore the differences between PWA and other approaches to building mobile user experiences. To conduct this research, we gathered a list of the most important mobile application criteria and from the information collected, created a matrix to help us answer the following key questions:', 'PWA is catching up with the features that are enabled by default in native apps by creating new standards and implementing them in browsers. PWA can replace a native app if the key features of the application can be developed with the current PWA capabilities. Since progressive web apps don’t provide advanced graphic rendering and the overall development complexity is low, it can be the perfect choice for certain scenarios such as building a MVP that is needed in a short timeframe. ', 'Fundamentally, the approaches for PWA and hybrid apps are very similar however, there are still some important differences when it comes to their architectures. Hybrid apps can use devices hardware/software features via plugins, which provides advantages for the number of features they can contain and the amount of computing resources available. ', 'As is the case with native apps, PWA can be used to replace a hybrid approach if the application is not dependent on advanced rendering and is within existing PWA capabilities. In addition, an important benefit of using the PWA approach is that it decreases the CPA (Cost Per Acquisition) rate because it is a web-based application and can be supported with SEO. ', 'The main advantage is that with implementing a manifest file and creating a Service Worker with a caching strategy, you have a solution that can be used from a web browser or be installed on devices to work without an internet connection. It means that you can publish the PWA to app stores like the Google Play Store or Apple App Store, which increases the visibility of your app.', 'But there are various other questions that still need to be explored including: ', 'The following research investigation provides a detailed review of the differences between the various approaches and answers not only the above questions but will also provide you with the information you need to determine which approach will work best for your business.', 'The quantity of PWA projects is growing larger every day. Articles like the ones below are continuing to generate interest in the approach as a way to create an exceptional mobile user experience:', 'As a result of the accelerated buzz around the PWA approach, it became increasingly important to be able to accurately compare the specifications of PWA to other available approaches in order to be able to find the best solutions. To achieve this, we created a list of \xa0the most important mobile app criteria and then analyzed them in the context of each approach to find their strengths and weaknesses.', 'We defined the approaches as follows:', 'In the next section we will explain what metrics we chose to determine these differences. We will then provide a comparison matrix of all the metrics from which you can determine your main priorities. You will then be able to review a comprehensive summary of PWA vs the alternative approaches that includes detailed information and business use cases.', 'In order to find the best option to develop a mobile experience and to learn about the exact differences between available choices, we gathered a list of criteria that contains the most important KPIs of a mobile app such as conversion and retention rates etc.', 'We divided them into three categories:', 'In the functionality section, there are two criteria that could themselves be the topic of separate research investigations, “performance” and “device access and features”. Performance by itself is an abstract word, which is why it’s important to define a performance indicator to be measured, for example performance of app initialization or memory usage. In this analysis, we selected runtime responsiveness as our performance metric as it is one of the most important criteria for the user experience. It gauges how quickly the page responds to user interaction after the page loads. For simplicity, we will be measuring performance on a 3 point scale (with 3 being the highest score). ', 'The “device access and features” criteria include a variety of different features and for the sake of simplicity and clarity, we decided not to include every single one of them in the comparison matrix. However, if you’re interested in looking at them in more detail, you can find the complete criteria here ', '.', 'Compiled approaches like React Native and Flutter will be considered as hybrid, since the topic of current research is to compare PWA to other approaches collectively rather than individually.', '* — Please see the specific metric notes under the metrics section to review the limitations', 'The splash screen is a screen that is shown to the user while the app is in the initialization process. It usually contains the app’s name and version number. Splash screens typically serve to enhance the look and feel of the application. The absence of a splash screen can confuse the user and increase the churn rate (the number of users that uninstalled the app or cancelled a subscription etc). However, having a great splash screen can make the initialization feel faster to the user.', ' At present, there are limitations on iOS because support is not fully accessible on Apple devices. However, there is a workaround and with a little effort splash screen functionality can be implemented. Additionally, when it comes to PWAs, there are limitations on splash screen design where only text, icon, and background color can be changed. Although efforts are underway to improve their functionality properties, for now there is nothing officially available.', 'This criterion represents the overall experience of using a mobile app, even when not technically a native app. This aspect is more important for web-based apps because there is a difference between web and mobile UI/UX standards. For example, in the browser-based apps, we are not including the address field and browser navigation elements because they are not \xa0natural for mobile apps. The controls have to interact mostly with touch events and give immediate feedback in a form of an animation. Failing to fulfill this criteria would confuse the user and could increase the churn rate.', ' The experience of native controls can be achieved in PWA with HTML and CSS. Although there are some already built UI kits (see the links above for more info), they are only imitating native controls, so there is the possibility of glitches. In addition, it’s important to highlight that if the app has poor overall performance, it can cause problems with the proper rendering/animation of user interactions with the app.', 'Animations can have a positive or negative impact on the look and experience of applications. It’s important to be able to clearly show responses to user actions so they’re not doubting themselves for example asking, “have I pressed the OK button?”. Good quality animations that run smoothly make a positive impression that encourages users to keep using your app.', 'Animation performance directly correlates to the graphic computational speed, which varies depending on the approach used. We will take the default graphic computational capacity and for simplicity measure it on a 3 point scale (with 3 being the highest score).', ' Smooth animations for PWA are a necessity. It is possible to create good quality animations using the PWA approach but it’s important to recognize that the capacity for high quality animations is higher using other approaches. This is because PWA is a web app and computations are done by the browser and its resources (which are limited). There is the option of using WebGL 2, which can increase animation performance for web based projects including PWA, but it is not supported by all mobile browsers yet. Therefore, applications that heavily utilize animations and graphics are still not the best choice for the PWA approach.', 'Installation criteria includes the look and experience of the process of downloading application code to your physical device from app stores like the Google Play Store or Apple App Store.', ' It’s important to note that PWAs don’t have to be installed via app stores like the Apple App Store or Google Play Store. They can also be downloaded directly from the browser, which is generally a more convenient way for users to complete the installation rather than needing to conduct a search in an app store. However, there are still a lot of people (potential clients) who carry out their searches within app stores so to maximize your potential audience it is still advisable to publish your PWA in a traditional app store too.', 'Push notifications are short text messages that are displayed when a user is not currently using the application. You can find more information about their importance ', '. But in a nutshell, they are extremely important because “App users who receive any amount of notifications in their first 90-days have 190 percent higher retention rates than those who do not, while more frequent messaging increases mobile app retention rates by 3X to 10X”.', '* — Please see notes for limitations', 'PWA can send push notifications using the Web Push API. Implementing them is not a complicated task, but there are limitations when it comes to sending them to iOS devices, which can be critical in some cases. Moreover, push notifications will not be included in the notification history.', "Offline mode is needed for situations where connectivity isn't strictly required, so that your app performs the same offline as it does online. Unlike the normal web where content is only available with a connection, PWA works differently. Once the Service Workers (a built-in mechanism responsible for many of PWA’s progressive features) have loaded the necessary files, offline viewing is made possible and users can interact with the app even when offline.", ' Even though PWA and native mobile run differently, they both provide a similar offline mode experience. The main difference is that a native app uses the content and the functionality it managed to cache when the connection was still there. This is available due to local storage and smooth data synchronization with the cloud. Therefore, native apps definitely win in this category. ', 'While it’s great that PWA technology is catching up and allowing users to access cached content, they’re still not quite at the point of being able to tap into a mobile device to stay connected under all circumstances. One of the biggest disadvantages of responsive web app is its inability to function offline. This means that in order to work, the user must have an active internet connection at all times. Web apps also tend to be noticeably slower as they access data from a server.', 'Performance by itself is an abstract word, which is why it’s important to define a performance indicator to be measured, for example performance of app initialization or memory usage. Analyzing all the performance criteria for mobile applications could be a topic for separate in-depth research. Therefore, for the purposes of this investigation, we will focus on only one of the most important performance elements - runtime responsiveness. ', 'Runtime responsiveness measures how quickly the page responds to user actions once a page has loaded. It is essential to meeting user expectations in terms of app agility across all environments and devices. Having poor runtime responsiveness is one of the major reasons that users simply move to another competitor. For simplicity, we will be measuring performance on a 3 point scale (with 3 being the highest score).', 'Web based apps run their code in the browser environment, which has its performance limitations. It is therefore fair to say that native apps can handle loads more efficiently. To achieve good performance on PWA you need to create a cache strategy and have a performance budget that will determine whether you are capable of handling user interactions across a variety of devices. However, PWA can still handle low and middle complexity projects such as news portals. Prime examples that prove this are Twitter, Starbucks, and Uber. Additionally, WebAssembly is a promising star in the web performance field. Maybe in the future it could narrow the gap in performance between native and PWA applications.', "This criterion includes hardware or software features of the device such as audio/video capturing or home screen installation. As there are a wide variety of features possible, their detailed analysis could be a topic of another research investigation. In cases where some features are not implemented, this can decrease the overall usability of an application or be a primary reason for switching to another app. That's why it's often important to be able to incorporate as many features as possible.", '* - Hybrid apps have plugins for this purpose and they need to be developed, tested, and stabilized', '\n** - According to ', ' Undoubtedly, native apps have the largest list of possible features, with hybrid apps the next best option. PWA has all the features that the HTML5 specification allows for. In general, it’s fair to say that if a project requires a specific device API, PWA cannot be used. On the bright side, the list of enabled PWA features is increasing every year. For more information, see the following list ', '. It includes a feature list including availability statuses.', 'Discoverability is a very important criteria because it has a primary correlation with one of the most important KPIs of a mobile app - download counts. The easier it is for users to find and use your app, the bigger the number of downloads. There are two main ways to find your app - through the web and through app stores. To maximise your chances on the web, you have to follow best SEO practices and in app stores you have equivalent rules that can be found under the ASO (App Store Optimization) tag.', '* - Applications can be downloaded only from platform app stores, but can still have a separate website to increase discoverability.', ' One of the great advantages of a PWA is that it can be discovered through both mobile and web, offering the best of both worlds. Another benefit for users is that they can try it without installing it. The download action is a commitment and without a good description, images etc. this can decrease the conversion rate. In the discoverability field, PWA is definitely the winner.', 'Publishing is a process of making an application public and available for customers to use. When it comes to this process, there is a main difference between native, hybrid, and web applications. Mobile and hybrid apps historically were published in app stores like the Google Play Store or Apple App Store, while web apps were hosted on servers and were available from browsers. However, PWA can be made public in both places because it’s essentially a web and mobile app at the same time.', '* - Please see notes for limitations', 'In this area, PWAs shine as they can be published both as a web resource as well as a mobile app with some additional effort. There are some tools that help to convert PWAs to the app store formats (check the links above for more information about these tools) or you can do it on your own (the latest version of Android allows you to submit PWAs directly to the Google Play Store). ', 'From a financial perspective, listing your PWA in app stores will cost:', 'It should be emphasized that compared to Apple, Google is pushing more towards PWAs. It makes sense since Google is leaning towards the web while Apple wants to offer the best UX and also makes a large profit from its app store.', 'Code reusability is a simple criterion that shows that one codebase can be used for several platforms like iOS, Android etc. It can drastically cut down on development costs, since you only have to invest in it once and it can then be used repeatedly across multiple platforms.', '* - Responsive web apps are used from browsers and each platform has a web browser. So, it could be considered as a cross-platform approach.', 'In this case, all other approaches have an advantage over native applications. Because of the versatile use of one codebase throughout all platforms, there is no need for the companies to have in-house teams or to outsource several development teams, which automatically results in cost savings.', 'Software development effort is the process of predicting the most realistic amount of effort required to develop a working piece of software. It is normally expressed in terms of person-hours or money-like cost effectiveness required to develop or maintain software depending on the contribution of an individual developer or a team.', ' When developing PWAs there can be issues with costs, depending on the design complexity and/or number of features required. On the other hand, PWA will be much more cost-efficient than creating an application using the native development approach because with native, you will always be limited to a certain market segment. By developing a PWA, you automatically go global – you build the app only once and it is then suitable for all platforms. ', 'PWA is a great choice if you have a small business or start-up. Since the development of progressive web apps is simpler, faster, and cheaper, it may be a more affordable and suitable option for your emerging business than following a lengthy and expensive mobile development process. At the same time, this doesn’t mean that PWA isn’t also suited to medium-sized or large businesses. Progressive web apps have proved themselves to be effective for such tech giants as Facebook, Pinterest, and Twitter since they offer significant business development value.', ' A website costs between ', '3,000 and ', '10,000 to develop and a native app will set you back between ', '20,000 and ', '80,000. While a progressive web app costs between ', '6,000 and ', '20,000 to develop, many businesses will only ever need the PWA – meaning there is a significant cost saving over developing a website and a native app separately.', 'eCommerce example (2015): Building a PWA from scratch will take something between 3400 wh (for a small app) to 9000 wh (a dedicated project we’ve done). This equates to a cost ranging between ', '400K and ', '1m.', 'Using a cloud platform will be at least 75% cheaper, and Time to Market will also be 75% faster (2-3 months instead of 8-12 months).', 'Development tools are used to create, debug, maintain, or otherwise support programs and applications. Developers should be aware of the tools available for building better applications that are convenient for them as well as for the user.', "The main difference between web apps and native apps is that native apps are made especially for one platform – whether it be Android, iOS, or Windows Phone. They use the development tools provided by the operating system owner to access its functionalities. When developing a native app, you will use a variety of supporting tools in conjunction with the relevant OS. Building native applications doesn't depend much on open-source libraries and platforms. In comparison, PWAs don’t have the same options of simplifying development and streamlining the overall process because they are not created solely for one platform.", 'Each option fits different business needs and requirements and will differently affect key project metrics such as cost, time, and quality.', 'It would be wrong to say that one approach is better than the other, however we can still present the key differences to help with making a choice that suits a particular set of needs. ', 'We selected the following five metrics to describe these differences:', ' PWA is faster to build because one codebase is used for all platforms. However, the difference is not that significant because of the amount of work that must be put into achieving a satisfying user journey. You have to create a caching strategy to enable offline mode and you have to add fast and attractive animations that provide a unique user experience. Additionally, you cannot forget about code optimization because it is executed in a browser tab.', ' Developing a PWA is cheaper than building a native app because the same web developers write code for different platforms. Furthermore, if you have a website there is no need to hire additional mobile developers. Keep in mind that each platform will need a separate team, iOS developers will not be able to create Android apps.', ' There is a huge difference between a PWA and a native app in terms of the amount of effort that needs to be made (PWA being more labour intensive). It affects the main criteria such as time and cost because there are many additional tasks in PWA development that need to be fulfilled (push notifications, caching strategy, mobile UI/UX, etc), which are already built-in to native apps. ', ' Because of all of the features that are enabled by default on a native app, PWA will always be at a disadvantage in this area and needs to rely on additional implementation efforts by browser providers. If a critical functionality of your app is based on top of hardware/software features that are not currently supported on PWA, it will automatically disqualify PWA as a feasible choice. This criterion is the most common reason why a lot of new apps are not developed as PWAs. ', ' In this category, PWA has a lot of advantages since it can be downloaded from the web or from app stores, while native apps are only available on app stores. Since PWAs are technically web apps, all SEO features are available for them and the CPA (Cost Per Acquisition) rate is much lower than with native apps.', ' Developing a PWA or hybrid app from scratch takes approximately the same amount of time. The two approaches also have a similar amount of additional work to be done including mobile UI/UX, performance budget use, HTML API or hybrid app plugins. Although, if you already have a web app. it would take less time to convert it into a PWA.', ' Costs will be nearly the same because one team will make applications for several platforms with nuances for each approach. \xa0', " Hybrid and PWA apps have the same problem to solve - to make an HTML/CSS with JavaScript app look and feel like a mobile app. That's why the overall amount of effort required for each approach is very similar for both methods. \xa0", ' PWA has fewer feature lists than the hybrid approach because of its architecture. Hybrid apps need plugins to work with a device’s hardware/software features, while PWA needs to create a new HTML standard and wait until the browser implements it. \xa0', ' Because PWA can be found in app stores and can be also downloaded from the web, it has more discoverability than the hybrid app, which can be installed only from app stores like the Apple App Store or Android Play Store.', ' \xa0In case of web apps, it doesn’t take much time to make them simply installable. However, the time required dramatically increases with project complexity because the app needs to be fast and responsive in every condition and on every device in order to achieve a mobile application experience. ', ' Additional cost is needed to implement a manifest file, a Service Worker with a caching strategy, and mobile UI/UX design to make a web app a progressive web app. ', ' You need to put in extra effort to make a web app installable and functional without an internet connection, which is why PWA requires more work.', ' PWA and web apps have the same list of functionalities using HTML standards. However, PWAs also have additional functionality including the ability to be installed on a device or being able to interact with other applications that are installed on the device.', ' PWA has more discoverability because apart from the web, it can also be found via app stores.', 'Publishing stories of leading digital companies is a good way to get acquainted with how they adopt PWA approaches to products and services and help them engage with more users, achieve their goals, and ultimately make their products more successful.', 'George.com is a leading UK clothing brand, part of ASDA Stores Ltd., which is owned by Walmart. After upgrading their site to a Progressive Web App (PWA), the brand saw a 31 percent increase in mobile conversion.', 'With consumer expectations around the mobile shopping experience at an all-time high, George.com realized they needed to revamp an outdated mobile solution and thereby improve their offering to customers. The team embraced a mobile-first approach, placing focus on design, speed, and functionality to drive mobile conversion.', 'The George.com team recognized that to meet this challenge the business had to enhance the mobile experience by building a progressive web app and incrementally deploying it. The team was able to realize the benefits immediately. ', 'Site speed was the most crucial part of the initiative. Following PWA best practices, in accordance with Google, George.com reduced page load time for shoppers by 3.8x times. The business also saw a 31 percent increase in conversion and 20 percent more page views per visit.', 'By implementing site wide HTTPS, George.com now offers a more secure, end-to-end shopping experience for customers. This includes the incorporation of modern browser capabilities such as Service Worker, which allows consumers to stay in touch with the brand whilst offline.', 'In addition, the brand implemented an “Add to Home Screen” prompt, which resulted in an increase in customer time on the site by 28 percent, creating a truly native app-like experience on the web.', 'Building great mobile experiences is an indispensable part of AliExpress’ success, as mobile commerce is growing three times faster than eCommerce. The mobile web is their primary platform for discovery so they have always focused on its design and functionality. However, AliExpress found it difficult to build an engaging experience on the web that was as fast as their mobile app. They looked at the mobile web as a platform to transition a non-app user to an app user. However, not everyone downloaded their app and getting users to install and re-engage with it was challenging and costly.', 'They built a cross-browser progressive web app (https://m.aliexpress.com) to combine the best of their app with the broad reach of the web. After implementing their PWA, AliExpress saw conversion rates for new users increase by 104 percent. This investment in the mobile web also resulted in conversion rates on Safari increasing by 82 percent. The new strategy also delivered a much better experience. Users now visit twice as many pages per session, and time spent per session increased an average of 74 percent across all browsers.', 'After analyzing usage for unauthenticated mobile web users, they realized that their old, slow web experience only managed to convert 1 percent of users into sign-ups, logins, or native app installs. The opportunity to improve this conversation rate was huge, leading them to an investment in a PWA.The Pinterest PWA started because they were focused on international growth, which led them to the mobile web.', 'In only three months, Pinterest rebuilt their mobile web experience using React, Redux, and webpack. Their mobile web rewriting led to several positive improvements in their core business metrics. They also defined a pre-cache for the initial bundles loaded by the application shell (webpack’s runtime, vendor, and entryChunks). By using Webpack, their mobile web rewriting led to several improvements in performance. Not only did they break-up and shave hundreds of KB off their JavaScript, taking down the size of their core bundle from 650KB to 150KB, they also improved on key performance metrics. First Meaningful Paint was down from 4.2s to 1.8s and Time To Interactive reduced from 23s to 5.6s.', 'Their mobile web rewriting led to several improvements in performance:', 'Twitter noticed that users had to overcome many obstacles while using their mobile website.', 'Some of them were on a slow mobile network or \u200bhad little storage space on their mobile device.\u200b As a result, visitors were reluctant to spend time on Twitter’s website or engage in posting and commenting. Twitter wanted to find an attractive alternative for people who didn’t use their native app or didn’t have enough storage space to download it.', 'Twitter decided to build a progressive web app\u200b because it seemed to be the best\u200b combination of a modern website and native features. Instant loading, lower data consumption, and greater accessibility were features that Twitter was looking for.', 'As a result, they were able to increase engagement with the “Add to Homescreen” prompt, web push notifications, and lowering data consumption. The PWA optimized images to help reduce data consumption by as much as 70 percent as users scrolled through their timelines.', 'The outcome turned out to be very impressive – the numbers speak for themselves:'], 'date': '\r\n\t\t\t\t\t\t\tApr 27, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tIhor Stetsiuk', 'author_url': 'https://blog.griddynamics.com/author/ihor-stetsiuk/', 'tag': []}, {'title': 'High-performance auto parts search: moving from Endeca to Elasticsearch', 'article url': 'https://blog.griddynamics.com/high-performance-auto-parts-search-moving-from-endeca-to-elasticsearch/', 'first 160': [', US online sales of auto parts and accessories amounted to $12B and, while new car sales projection are looking bleak, the future of the automotive aftermarket is quite positive. Both specialized auto parts retailers and online commerce powerhouses like Amazon and eBay are competing for the share of this market.', 'When you are looking for auto parts online, it is crucially important to ensure that part you are ordering will fit your vehicle. That is why all online retailers selling auto-parts first try to establish what vehicle you have in mind when searching for auto-part.', 'Vehicle information is used to filter parts catalog and surface only the parts which will fit the target vehicle. This kind of filtering should work seamlessly with both keyword search and category browse. ', 'Turns out, it is pretty hard to achieve high quality search with legacy search platforms like Oracle Endeca. Many specialized retailers struggle with performance and scalability issues which are preventing them from rolling out new high-value features to successfully compete with tech giants like Amazon on auto parts market. ', 'In this blog post, we will describe how to overcome those issues with modern open source search platforms.', 'Auto parts catalogs are unique because they significantly depend on part fitment information. \xa0', ', also referred to as ', 'or', ', refers to the vehicle year, make, model, and engine that your parts and accessories fit. Often this information expands to trim/submodel, motor, and so on. ', 'Products in auto parts catalog may have any number of fitments, in the range from zero, \xa0which means that product fits any vehicle, to hundreds of thousands for some widely applicable products. Typically, products have about a hundred of fitments. ', 'Essentially, we are dealing with parent-child relationship here, where product (parent) has multiple children (fitness) entities. Our core use case is to filter products by attributes of their fitnesses, e.g. by a full or partial specification of the vehicle.', 'Parent-child relationship requires special modeling in search engines due to the nature of the very data structure which gives them their speed - inverted index. Core abstraction for search engine is a ', ' which is a collection of ', ', where each field can have multiple ', '. Each document is completely independent and doesn\'t "know" about any other document in the index.', 'So, in order to support parent-child relationship with inverted index, we have to apply special techniques. Performing join and filtering in query time is technically possible, yet it is a known performance killer. So, most search implementations go for variety of tricks to move join work to the index time. ', 'Endeca goes for all-in denormalization, which leads to the structure of index which looks like this:', 'With denormalization, or ', ' approach, parent attributes are copied to child documents and only child documents are indexed in the search engine. At the query time, child documents are searched, retrieved and grouped by parentId to return only parent documents. Here is how it works in auto parts search use case:', 'This is a straightforward approach and is widely used in many search applications. However, it has important drawbacks which significantly affect search performance:', 'Those issues may lead to response times in several hundreds of milliseconds or even seconds and strongly impact customer journey, creating perception of sluggish search experience. Customer patience is short, and options are plenty, so we should strive for immediate response from the system. ', 'Is there a better way to model a parent-child relationships? Sure there is! One of the neat techniques available in Apache Lucene is index-time join (a.k.a. block-join) of parents and children, which essentially encodes those relationships as the order of documents in the index. This functionality is nicely exposed by both Solr and Elasticsearch search engines. Here is how it works:', 'Parents and children are arranged in the blocks where child documents are followed by their immediate parents. This means that in order to find a parent document by attribute of the child, all you need to do is to find a matching child and jump ahead in the index to the closest parent, which is a very natural and fast operation for inverted index. Similar approach works for finding children by their parents.', 'This approach has multiple advantages over all-in denormalization:', 'Here is how the same search for "filter for ford mustang" will work in Lucene-based block-join index', 'Of course, as it usually happens, there is no free lunch, and this data modeling approach has some drawbacks as well: ', 'In majority of applications, this trade-off is perfectly acceptable.', 'One additional important consideration is the scalability of search solution. With large catalog, it is increasingly important to be able to parallelize query processing over multiple nodes to be able to keep response time under control. This is achieved with index sharding and map-reduce query processing. This is another key scalability feature not available in Endeca.', 'Original Endeca implementation had typical response time of 600 ms, with frequent multi-second "spikes" for high-recall queries. Let\'s see if we can do better than that with a modern search engine.', 'We implemented a sample auto parts search application based on Elasticsearch. Please note that a very similar system can be built with SolrCloud, as both systems rely on core functionality available in the Apache Lucene search library.', 'We used a catalog of auto-parts with about a 1M parts. We used just a single cloud node and deployed a 4-shard \xa0Elasticsearch cluster. ', "We emulated the Endeca's denormalization indexing approach and compared it with block-join (nested) index structure with respect to both index size, indexing time and query response time.", 'As you can see, nested index is 4x more compact compared to flat index due to absence of data duplication. Indexing time is also 3x better for nested index.', 'Lets look at the query time statistics:', 'As of query time, we can see that nested query shows consistent high performance, while flat index queries are 3-5x time slower. even though they can be brute-forced with additional shard parallelism, at the expense of overall search cluster capacity.', 'If we look at the performance of some selected queries, we will see a typical picture of search performance highly depending on number of matching documents. ', 'We can see that typical response time stays below 50 ms. It becomes slower for queries which match big portion of the catalog, like "bulb" or "engine", but still stays under 100 ms. Also, there is no direct correlation between latency and number of fitments in retrieved products. ', 'In this blog post, we discussed a practical approach to solving the main auto parts search use cases with modern open source search engines. \xa0Turns out, even on the single node we can improve Endeca response time ~10x. On top of that, migration to Elasticsearch or SolrCloud unlocks horizontal scalability and allows to support catalogs of arbitrary size without response time impact.', 'If you are dealing with auto parts search and experiencing performance or search quality issues, we would be happy to help. ', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tJun 23, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tIvan Mamontov', 'author_url': 'https://blog.griddynamics.com/author/ivan-mamontov/', 'tag': []}, {'title': 'Why e-commerce search engines must be aware of inventory', 'article url': 'https://blog.griddynamics.com/why-e-commerce-search-engines-must-be-aware-of-inventory/', 'first 160': ['In online commerce, catalog navigation functionality is one of the key aspects of the overall user experience. Customers spend the majority of their time on the site discovering and selecting products and making purchase decisions. The mission of quality search and browse functionality is to provide the customer with the smoothest and shortest path to the most relevant products. Removing frustration from the product discovery and selection experience is a big part of this mission.\xa0', 'In this post, we will talk about ', '. Just imagine the frustration of an online shopper who, after hours of shopping, found a perfect dress only to discover on\xa0the product data page, or even worse, on the', 'checkout page, that the\xa0desired item is out of stock. This problem is especially painful in fashion, luxury, jewelry, and apparel retail, as well in online specialty marketplaces, where many products are unique, inventory is low, and customers spend significant time discovering products and making purchase decisions.\xa0', 'Some systems try to avoid the problem of making search engine inventory-aware. After all, out-of-stock products can be hidden from search results as a post-processing step. This works, \xa0and is definitely an improvement over showing out-of-stock inventory as if it was available to buy. However, faceted navigation becomes clunky; as a result of post-filtration, facet counts calculated by the search engine do not match actual product counts seen by the shopper. This may become very visible when the customer uses facet filters and gets "zero result" pages instead of the expected products, which is pretty frustrating.\xa0', 'An alternative approach is to grey-out product thumbnails for unavailable products on the UI. This solves the results vs facets inconsistency problem, but wastes precious real estate on the first page of results with unsellable items.\xa0', 'Neither of these approaches deals with a situation where some products become available at some point in time after being out-of-stock. Depending on the catalog reindexing cycle, this can postpone reappearance of the product on search and browse result pages for hours or even days.', 'To avoid these issues, e-commerce search system designers need to embed up-to-date information about inventory availability right into the search system. Not only must it accurately and consistently filter out-of-stock items, and reveal in-stock items in a timely fashion, but having this information available also unlocks a range of powerful features. Omnichannel customers can find products available in nearby brick-and-mortar stores. Products can be boosted (or buried) in search results based on underlying inventory levels. In-store customers can filter enterprise catalog by what is available in the store where they are shopping right now.', 'Making a search engine support inventory availability is easy. After all, a search engine such as Solr excels in filtering. What is hard, though, is keeping inventory data up-to-date. From a technical perspective, displaying up-to-date inventory availability means supporting high volume, high-frequency updates to the search index. This is bad news for most search engines that are designed for read-mostly workloads and are optimized assuming immutability of the index.\xa0', 'In this post, we focus on e-commerce search applications based on Solr, yet the same ideas \xa0should be applicable to other search engines and verticals concerned with embedding near real-time signals of various types in their search indexes.\xa0', 'Let’s consider some of the approaches implemented in the industry.', 'The brute force approach would be to delete and reindex all documents affected by each availability change. This is typically easy to implement and is perfectly fine for small-scale, low-traffic web shops. For large-scale, high-traffic sites this approach becomes impractical. Brute-force indexing adds quite a bit of overhead, as product and SKU documents have to be repeatedly retrieved from a data source, processed by business logic, and transmitted to the search service, where they are re-indexed again as they pass through all analysis chains. This is obviously the heavies and lowest-performing solution possible. Can we do better than that? We certainly hope so!', 'The brute force approach can be improved upon by not constantly \xa0reloading and retransmitting product information. All we need to transmit is information about what actually changed inside the existing document —\xa0in this case, the SKU availability field. This is called partial document update, and it is supported in both ', ' and ', '. It seems like we are done now, doesn’t it? Unfortunately, this is not so. Both Solr and Elastic support partial document update through full reindexing under the hood. This means that they store the original document data in the index and, once a partial update request \xa0is received, restore the document from the original, patch it according to the partial update data, and reindex it again as a new document, deleting the old version.\xa0', 'In general, this approach takes a high toll on search performance:', 'This makes partial document update suitable only for infrequent, low volume updates, e.g. 10-15 minutes apart, thousands of documents in update.', 'The problem with volume of updates can be addressed by moving inventory information into separate documents and performing query time join in ', ' or ', ' (based on Lucene ', '). This way, we will have a product documents and inventory documents which can be separately updated.\xa0', 'This approach has the advantage that separate lightweight inventory-only documents will save a lot of reindexing costs, especially in the case of block index structure.', 'Few things to keep in mind: in the case of distributed search custom sharding is required to correctly join products and inventory documents. Also, Lucene query time join is not a true join as only parent side of relationship is available in the result, e.g. it is similar to SQL inner select.\xa0', 'Still, this functionality is enough to support filtering by inventory availability. We can create a join query to select all available SKUs. How will it work under the hood? JoinUtil will first filter all inventory records by availability, then retrieve their SKU IDs, then filter SKUs by those IDs and intersect with the main query applied to the SKUs.\xa0', 'If this doesn’t feel like a high-performance solution, your intuition is correct. Indeed, ', '. It can only traverse a relationship in one direction —\xa0from child to parent, which can be very wasteful if the main query for the parent is highly selective. It also requires access to the child’s foreign key field value, which can be relatively slow even with docValues. The search by primary key on the parent side is faster, but still requires expensive seek on the terms enumerator.', 'Also, this solution still suffers from the necessity to commit to the Solr core and reopen searches, losing accumulated caches in the process.', 'Query time join can be avoided with the following trick. ', '\xa0we have been able to perform query time join across separate collections. \xa0Assuming we have separate products and inventory collections, we can have a post-commit listener on inventory collection which will access and patch an availability filter within a product core. This approach eliminates the performance hit of query time join and retains caches in the product collection. Still, there is no such thing as a free lunch. This approach requires a custom Solr plugin and pretty involved code. Out-of-the-box Solr uses filter cache in order to speed up cross-core-join performance. Using the plugin, this cache needs special treatment and must be regenerated asynchronously.', 'It is well-known that the secret sauce of search engine performance is an inverted index. However, for many tasks which are traditionally performed by search engines, such as faceting and sorting, an inverted index is not efficient. To facet or sort documents you need mapping from the document ID to a value of the field. Historically, in Lucene, this mapping was delivered by ', ' which was created by “uninverting” the indexed field. Such an approach is not optimal because:', 'To address these issues, Lucene developers introduced the concept of ', ', which by now has completely replaced fieldCache. In a few words, “docValues” is a column-oriented database embedded directly into a Lucene index. docValues allow flexible control over memory consumption and are optimized for random access to the field values.\xa0', 'But there is one feature which makes them stand aside: docValues are ', ', and, more importantly, can be updated without touching the rest of the document data. This unlocks true partial updates in the search engine and can support very high throughput of value updates (as in tens to hundreds of thousands per second). Replication of the updates across the cluster also comes out of the box and are pretty efficient, as docValues changes are stored separately from the rest of the segment data and thus do not trigger merge policies or require segments to be replicated.\xa0', 'This makes updatable docValues great candidates to model inventory availability. Unfortunately, updatable docValues are not yet exposed through Solr in the general case (track ', '). Still, Lucene support is there, so it is pretty easy to plug in your own update handler.', 'However, there is still a caveat: making docValues updates visible requires commit and reopening of the segment readers and wiping out valuable filter caches, which may take many seconds or even minutes to regenerate. Fortunately, in the case of pure availability updates, no actual segment data is changed so all existing filter caches are still valid, and Solr ', ' can be used to recover cached filters from previous Searchers into the new ones.\xa0', 'Updatable docValues work well for inventory availability tracking for simple use cases when there is a predictable number of updatable fields to support, and it is acceptable to pay the price of a Solr/Lucene commit periodically, e.g. when updates can be micro-batched, say with a 1-5 minute period.\xa0', 'However, even updatable docValues are not a silver bullet. There are limitations and tradeoffs to be aware of.', 'In case true near real-time availability updates are needed, or the number of available fields to track is very large, off-the-shelf features in Lucene and Solr won’t be much help. Only custom solutions tailored for particular use cases can work.', 'The main idea of such solutions is to embed a highly optimized in-memory store (NRT store) into each Solr instance. Lucene searches can be bridged to this store by a ', ' interface providing ', 'Implementing this approach, we are almost completely on our own. We can not rely on Solr/Lucene to keep our NRT data durable or replicate it across the cluster. So, in general, following concerns have to be addressed:', 'With such a design, it is possible to build a search system which is completely decoupled from the normal Solr document update/commit cycle that can, indeed, support a near real-time view of inventory availability.', 'Up-to-date inventory availability is a must-have feature in a modern e-commerce search system. There is a whole spectrum of implementation approaches, varying in complexity and cost, which can be applied depending on the actual use case. In our practice, we have found that the updatable docValues approach is sufficient and works well in a \xa0majority of e-commerce cases. However, for some high-end implementations a more sophisticated custom solution will be necessary. The best thing to do is to experiment with your data and use cases to see what method of handling inventory availability is the best for you!'], 'date': '\r\n\t\t\t\t\t\t\tDec 12, 2016\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'Solr parent child']}, {'title': 'Digital transformation requires new technology partnerships and Agile co-innovation', 'article url': 'https://blog.griddynamics.com/digital-transformation-requires-new-technology-partnerships-and-agile-co-innovation/', 'first 160': ['The demands on IT service providers are changing because companies are embracing digital transformation. Traditionally most IT services were confined to packaged software implementation, system integration, and managed services. Companies grew to rely on traditional outsourcers for these kinds of IT projects. However, the advent of digital transformation has added a new demand for IT companies, in addition to regular IT jobs. Analysis shows that the digital transformation market will grow to 120 Billion in 2020, with 90% of companies saying they are not digital yet. Meeting this new demand cannot be addressed using the vendors, contracts, and sourcing strategies used for traditional IT. These strategies work best with conventional IT projects for which they were initially designed.', 'Let’s take a second and define what we mean by digital transformation and software innovation. We see these activities as:', 'Achieving these milestones of digital transformation before being crushed by more digitally proficient competitors requires engaging a new kind of strategic partner. Agile co-creation partnerships are the preferred way for companies to partner with vendors because they help large companies deliver digital transformation and innovation while controlling their destiny. This form of engagement also enables companies to inject a high-performance engineering culture and automation into their team. This blog post aims to explain the Agile co-innovation engagement model to demonstrate how it is utilized to accelerate digital transformation.', 'For digital transformation, clients are turning away from traditional vendors in favor of Agile co-creation. Clients seek vendors who focus on:', 'Below is the high-level breakdown of how ', ' stack up against traditional outsourcing vendors.', 'Current industry practices contain negative financial incentive issues because they create zero-sum scenarios that pit the vendor against the client. Contracts like fixed-price projects and open-ended T&M do not focus on delivering business results. Instead, these contracts provide short-term incentives for the client to focus on cost savings that hurt the vendor. Likewise, the vendor focuses on increasing profits from the project that hurt the client. Additionally, request for proposal (RFP) documents that focus on project requirements rather than business results shift the priorities from delivering a successful product to fulfilling RFP requirements. On the other hand, co-innovation engagements focus on defining key performance indicators (KPIs) and building trust-based strategic partnerships. Companies should pursue contract terms that ensure that short-term profits take a back seat to longer-term partnership goals.', 'One of the most common engagement models is a large fixed-price project. These projects have a set price for set deliverables. In theory, any unforeseen issues impacting the schedule or cost should be absorbed by the vendor. However, unscrupulous vendors are known to game the system by winning a contract by deliberately entering a low-ball bid, only to drive the final cost up by forcing the client to accept numerous change requests. The client ends up absorbing the cost overruns and may not see the results they had anticipated. Alternatively, Agile co-creation uses small fixed-price projects for initial engagements designed to build trust that is replaced with a soft contract when a successful discovery scales into a full project.', 'Another engagement type is a ', ' or ', ' (T&M) project. This type of project pays the vendor by the hour for services that place all the cost risks on the client if time and materials charges are unbounded. Traditional outsourcing vendors tend to “abuse” T&M contracts by bidding low rates to win the deal, but use large numbers of lower-quality coders to drive up the cost. Studies show that more substantial teams do not improve project success rates and require more client manpower to manage. Therefore in these scenarios, the vendor has a short-term incentive for supplying many lower-quality engineers. On the other hand, Agile co-innovation companies operate on the principle that small teams of highly trained specialists are more effective and easier to manage than larger, less skilled teams. They provide blueprints, accelerators, training for their engineers, and their delivery managers, so they are not just a staffing company, but an invested technology partner.', 'The RFP must also be structured to focus on KPI’s over project requirements to be successful. When requirement-based RFP’s do not focus enough on factors that lead to actual business results, there is an unintended consequence where satisfying requirements becomes prioritized over actual business results. Overall, all of these engagement issues prevent mutually productive relationships between customers and service providers and therefore hinder digital transformation efforts.', 'Popular engagement models like outsourcing and outstaffing have some excellent use cases. Still, they are not as effective at promoting knowledge sharing and bringing new ideas like co-innovation. This is because co-innovation engagements involve the client and vendor working together with the vendor bringing skills and experience, not just engineers to the engagement.', 'Outsourcing or managed services do not work well for large digital transformation projects because, using these types of services, the client does not gain an intimate knowledge of the resulting technology. If the client does not understand the technology, they become dependant on the provider, which is problematic for anyone who wants full control of their technology future. No knowledge transfer occurs because there is a wall between the client and the vendor, creating a technology black box.', 'Outstaffing is an engagement model that helps a client build a project team on-demand by filling in-house skill gaps with the technology vendor’s skilled personnel until the client can find full-time staff. A strategic partnership does not exist if the vendor supplies only skills of individual people, but not a knowledge of past implementations or use of blueprints. Merely filling a job position with temporary staff gives the client a short-term win but a long-term loss because the outstaffing vendor has less incentive to ensure project success through the services they provide.', 'Co-creation is a type of outstaffing model with some significant differences. The vendor’s personnel are not merely temps to be replaced by in-house workers as hiring ramps up. The vendor personnel is an integral part of the project, there for the duration or, in many cases, for many years spanning multiple programs. They also bring in engineering expertise and experience developed by the vendor, such as automation, accelerators, and blueprints, that can speed up and improve project results.', 'The key to a successful co-creation partnership is to find the right partner that can create differentiated digital experiences and optimize operational costs. For digital transformation, traditional outsourcing companies are the wrong type of vendor because they focus on managed services rather than innovation. Engineering firms focused on innovation use only highly trained and specialized experts that can complete challenging greenfield projects and bring new ideas while pushing back on unclear requirements and other issues in the interest of the client. These engineering firms build innovative teams that are continually trained, practice agile development, and high-performance engineering that is hard to create from the inside of client companies. Traditional vendors’ lower-quality coders may nor have these specialized skills and, therefore, may struggle to deliver digital transformation. Clients believe that innovative experiences differentiate them and provide a competitive advantage.', 'Given the history of issues with traditional vendors, contract structures, and engagement sourcing models, companies planning a digital transformation need to pursue Agile co-innovation vendors. ', ' are the best alternative when looking for strategic partnerships, knowledge transfer, innovation, top engineering talent, and an Agile and product mindset. Five key characteristics make an Agile co-creation vendor more successful than a traditional outsource vendor at accelerating digital transformation:', 'Agile co-innovation focuses on strategic partnerships and bringing business value to the client. A successful strategic partnership starts small and builds by growing trust over time. The goal is not a one-time fling, but a robust multi-year relationship. However, there is more to a strategic partnership than practicing trust-building. The client does not merely dictate project results. The vendor sits down with the client and discusses short and long-term goals for the team to ensure a mutual understanding and client success.', 'Client success is more than delivering a software package at the culmination of a project. It is about having the product and the team achieve business and technical KPIs. It includes knowledge sharing and helping the client establish a proper engineering and delivery culture within a team and spread this within the organization. Client success may also involve developing appropriate delivery processes with Agile, continuous delivery, automation, and SREs for long-term benefits.', 'Strategic partners also need to have the right technology vision for the future. This often comes from having strong visionary leadership that understands emerging technologies and how it will impact the industry. Strategic partners also offer their past technology experience, blueprints, and accelerators to ensure the client’s success. This kind of know-how accelerates and de-risks initiatives. A strategic partnership offers the most natural path to a fast and successful digital transformation journey. Combining the client’s core industry expertise with the vendor’s IT and engineering expertise complement each other perfectly.', 'Agile co-innovation enables the creation of differentiated digital experiences by injecting a high-performance engineering culture, Agile process, and product mindset into the client’s team. Agile co-creation companies provide the necessary tools and culture that brings about technology innovation. Developers need several ingredients to achieve a high-performance engineering culture that will enable faster development and a higher quality output:', 'Each of these factors contributes to an environment that enables innovation. ', ' emphasizes experimentation, constant course corrections, and a more flexible workflow. A ', ' is based on the idea that teams should think about initiatives as being products rather than projects. ', ' focuses on building a minimum viable product, then using data and experimentation to improve KPIs. ', ' are a means of shortening the development cycle without sacrificing quality. By building automated deployment and testing pipelines, changes can be made faster. This makes experimentation cheaper and more manageable. A ', ' usually consists of small, focused cross-functional service-oriented teams. These small teams are 100% focused on delivering results useful for experimentation and innovation. Lastly, ', ', where each team produces a service (via API or easy to use interface), removes unnecessary interaction, and improves cross-team collaboration.', 'Through the engineering services vendor, the client gains access to the newest technologies in big data, cloud computing, analytics, artificial intelligence, machine learning, user experience design, and customer experience.', 'A co-creation engagement provides the client with an opportunity to leverage the digital transformation expertise, blueprints, and technical program management of its engineering services vendor into their resource pool. Vendors who have previously completed digital transformation projects build up expertise through blueprints, accelerators, and practices that add considerable value to the client’s knowledge pool. Unlike the “black box” methods of a traditional outsourcing engagement, the co-creation engagement allows all parties to work side-by-side in technical, project, and program management. The client obtains a knowledge transfer via training from the engineering services vendor, which helps the client bring new skills in-house.', 'The optimal distribution of client to vendor personnel in co-created projects shift during the life of each project. Typically, the vendor represents the majority of the team members at the project’s inception. As the project progresses and the client gains more expertise, the ratio shifts toward the client. The vendor and client work together to ensure the team always has an optimal distribution ratio. The vendor provides program management, technical skills, and has a significant say in all aspects of the project. The client also has a considerable presence to take advantage of a knowledge transfer of Agile and high-performance engineering concepts.', 'Engineering services companies recruit top engineers through technical internships and relationships with the top universities around the world. These companies build a culture designed to attract and retain these highly specialized individuals with perks, such as technical internships, internal training, and top-quality technical resources. Highly skilled engineers generally pursue work on innovative projects using the latest technologies and have high retention rates.', 'Another reason agile co-creation companies have great engineers is that they excel at building high-performance engineering cultures. The client does not have easy access to such specialized markets and environments, so they struggle to quickly staff a team with the skills needed to guarantee project success. They also struggle internally to build a culture of engineering excellence that vendors have already created. Lastly, the cost to access a top-notch engineering talent through the engineering services company is less expensive than attempting to build this talent internally.', 'Customer success and “winning” are not mutually exclusive concepts. Both sides can, and do, win in a successful partnership arrangement. One way to structure a successful partnership arrangement is the use of soft contracts.', 'The concept behind a soft contract between two parties is that both parties become incentivized to collaborate for their mutual benefit. The contract is ‘soft’ because it does not specify what is to be delivered by each party but clarifies each side’s relationship and roles in producing mutually beneficial deliverables. A fruitful shared-success engagement model also includes key performance indicators that can deliver extra financial incentives to all for meeting exceptional goals. Agile co-creation vendor’s pursuit of these new types of contracts makes them easier to work with over time and ensure that the focus is on delivering results rather than contract terms.', 'A successful digital transformation benefits from new technology partnerships with Agile co-innovation companies. These companies bring a fresh perspective of co-creation through strategic partnerships, replacing the old concept of outsourcing or outstaffing. In this post, we have explained that traditional engagements may work fine for conventional projects; they fall short for large, strategic projects like digital transformation. Agile co-innovation requires finding the right engineering service vendor. One that enables innovation, has a global engineering pipeline, and focuses on client success through strategic partnerships. Both the client and vendor bring their strengths to the table, building an optimal team able to enable the innovation required to thrive in today’s business climate.If you are looking to work with an agile co-creation vendor start your vendor selection by downloading a free copy of the ', '.'], 'date': '\r\n\t\t\t\t\t\t\tDec 30, 2019\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEzra Berger', 'author_url': 'https://blog.griddynamics.com/author/ezraberger/', 'tag': ['CICD', 'E-commerce']}, {'title': 'ATG’s black box blocking your e-commerce view? Replatform with microservices', 'article url': 'https://blog.griddynamics.com/atgs-black-box-blocking-your-e-commerce-view/', 'first 160': ['If you want your customers to have a seamless shopping experience no matter where they are or what device they’re using, but you’re stuck with a black box legacy system like Oracle ATG, you may think a true omnichannel experience is beyond your reach.', 'We have helped many of the top retailers in the US to create true omni-channel experiences for their customers. Many of them poured thousands of hours and millions of dollars into customizing, integrating and tuning it, but legacy systems just don’t meet your needs anymore -- and their makers aren’t helping to resolve the issues. Instead of modernizing it, Oracle has Many retailers have found that their mission-critical commerce platform has turned into a monolithic black box impervious to innovation and ridiculously expensive to maintain. Go to the cloud? What cloud? Despite a few workarounds that may allow some cloud implementations, legacy systems have your company stuck in a fog so thick it may feel like you’ll never find a way out.', 'Replatforming to a microservices platform can help you achieve your goals as you migrate off of ATG. And we can show you how.', 'Why are we telling you about this now? ', ', we showed how the platform failed to support real-time inventory, couldn’t scale catalog services and crippled efforts to use the cloud to meet peak performance needs.', 'What we said then is more accurate now than ever. Hemmed in by ATG -- more on how coming up -- many large retailers already have migrated off this legacy platform. We believe many more are moving in that direction today, some to avoid renewing their licenses in the near-term and others to dump their data center leases as they migrate onto the cloud.', 'Nonetheless, many companies feel stuck because they think re-platforming is too time-consuming, risky and expensive even to consider it. ', 'get out from under ATG’s ponderous burdens, we’ve discovered that the experience doesn’t have to take long, isn’t particularly risky, and can cost less than sticking with ATG.', 'Let’s explore in detail why ATG no longer makes the grade:', 'What retailers actually want is a unified omnichannel platform, something legacy ATG struggles to be. But that’s where a headless microservices commerce component option can shine. It can be deployed in the cloud and at the edge, with UI components to cover the web, mobile apps, and the point of sale.', 'But the rapid pace of technological change makes less monolithic approaches more appealing. Instead, an API-first, headless commerce solution built on microservices is becoming the new e-commerce platform of choice.', 'Here are some of the key technology strategy principles to keep in mind:', 'As you build around your purchased pieces to weave together that mixed solution, we suggest focusing on these key build principles:', 'Once you’ve decided to re-platform, there are two main paths to accomplish this headless commerce component approach -- ', ' and ', '.', ' over the course of just a few months or at most a year involves one big-bang release of the new platform without any interim steps. The key business drivers behind this admittedly riskier approach are costs -- either your ATG license is about to expire or the lease is ending on your data center. If you don’t want to throw any more money at your legacy system, rapid re-platforming spins up your new headless component platform in a jiffy.', ': You make the leap to a modern platform quickly, getting out from under expensive data center leases and ongoing ATG costs.', 'But speed isn’t always of the essence. In fact, slow and steady may be the better approach.', ' can take up to three years, slowly stripping away services from ATG with the ultimate goal of getting rid of the legacy platform entirely. Working methodically, you can identify which domains and features require customization and then develop each component one by one, conducting A/B testing as you go along.', 'Focus first on components that can best take advantage of scalability and cloud resources, or areas requiring the most significant change because of new functional or performance requirements. Good initial candidates could include services that are loosely connected to the rest of the system or that require expensive licenses that can be retired after the transition.', ': Incremental re-platforming involves much less risk; it’s easy to back out a change if a problem crops up because ATG is still in place until you get the new component functioning. A key benefit to moving incrementally is that you can A/B test each component as you go along, verifying the value of the change and bolstering the overall business case for re-platforming.', '\nAn additional benefit is that transition costs are split across several budget cycles, easing the financial sting of the process. Ultimately, this slow and steady approach is less disruptive, allowing ATG to remain in place until it’s no longer needed.', 'The advantages are clear: A headless commerce component approach provides a modern e-commerce platform that legacy ATG just can’t achieve. If you want to lower costs, increase your flexibility, enjoy differentiated experiences and deploy your choice of best-of-breed tools, it’s time to move beyond a purely build or buy scenario. Sound too risky? Engage an experienced engineering services company like Grid Dynamics  that’s been through the process and knows how to guide you around any potential hazards.', 'In addition to building your differentiated solutions, a company like ours acts as an accelerator to decrease the overall cost of implementation and reduce time to deployment. Once you’ve got your customized components sorted, you can buy commodity pieces from best-of-breed platforms to fill in gaps.', ' for a streamlined re-platforming experience.'], 'date': '\r\n\t\t\t\t\t\t\tJan 16, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEzra Berger', 'author_url': 'https://blog.griddynamics.com/author/ezraberger/', 'tag': ['E-commerce']}, {'title': 'Relevant facets: how to select and promote facets with deep learning', 'article url': 'https://blog.griddynamics.com/relevant-facets-how-to-select-and-promote-facets-with-deep-learning/', 'first 160': ['Faceted navigation, a.k.a. guided navigation is a de-facto standard user experience for any serious online retailer. Facets seamlessly introduce your customers to the structure and attributes of an online catalog and provide shoppers with powerful tools to slice and dice their results with useful filters:', 'However, selecting what facets and filters to show to customers is not always that simple. Large department stores have hundreds of product types with thousands facetable attributes, and only a limited display area available to the category or search results page, especially on mobile devices.', 'The choice of facets, and how well they are ranked, makes a big difference and distinguishes a useful product discovery tool, which makes shopping easy and delightful from an annoying search bar, which occupies a large portion of the screen while only adding frustration to the shopping experience.', 'In this blog post we will review various strategies used by online retailers to present customers with relevant facets and will share our experience applying deep learning to this problem.', 'Retailers employ a variety of strategies to deal with facet selection and ranking. We can summarise those strategies with their pro and cons in following table:', 'Historically, retailers used a static list of most popular facets, such as Brand, Department or Price, and some are still using this outdated strategy. Many retailers have category-specific facets and are trying to reduce search results to one or few categories which dominate the result set. These days, a majority of retailers use some form of ', ' to react to a particular customer’s context and suggest a relevant facet list as defined by the merchandiser.', 'However, those approaches do not employ probably the most valuable data sources available to online retailers - customer behavior data. ', 'Let’s look into how we can use the power of natural language processing and deep learning to build a model which can suggest most relevant facets based on a current context, such as the customer query and already selected filters.', 'For starters, we will need customer engagement data in the following format:', 'Each row in this table represents a customer journey when they used filters refining search results from their original query.', "Let's look at one of the examples:", 'We will unfold this tree structure into the form useful for predicting next filter based on the current query and applied filters:', 'We can view our training goal as predicting the next event (filter application) in the series of events. For the purpose of dealing with sequences, the deep learning arsenal deploys a special kind of neural network: a recurrent neural network (RNN).', 'The recurrent neural network consumes input vectors one-by-one while building an internal vector representation of the sequence consumed so far. This internal representation is trained to be useful in predicting the target of learning.', 'Currently, the most popular RNN architectures are Gated Recurrent Units (GRU) and Long Short-Term Memory. The main idea behind those architectures is to selectively retain the necessary information about the sequence in a way useful for making predictions. For more details, we can recommend ', '. For the purpose of describing our model, we will proceed with an LSTM architecture. ', 'As the first order of business in training deep learning models, we have to vectorise our data. It generally consists of two different kinds of information: search query, which is a free-form word sequence and set of applied filters, which is better represented as a categorical data, since it contains a fixed and relatively limited number of options to select from.', 'For query vectorisation, we will use the popular fasttext library. It will represent (or embed) our search query as a matrix of 100-dimensional vectors, each vector representing a word in our query. Vectors in this matrix will be consumed by LSTM one-by-one, producing an internal representation of the query semantics within LSTM’s hidden state.', 'For applied filters, we will allow the model to learn how to represent them as vectors during training. This is how we will approach it:', 'We will define a matrix of all possible filter values assuming 100-dimensional representation of filter value. This matrix will initially be filled by random vectors. However, in the process of training, each filter’s vector will be updated to be more helpful in final prediction. This will result in filters with similar meaning having similar vectors, thus building a semantic vector space for filters and filter values. ', 'Putting the two vectorisation strategies together, we will arrive at the following scheme:', 'We will concatenate word embeddings with filter embeddings in the single matrix and push rows of this matrix through LSTM one-by-one letting LSTM build an internal representation of the whole query and applied filters sequence. The LSTM output vector will go through fully connected layers, trying to predict the next applied filter as a simple one-hot encoded value.', 'After the facet selection model training is complete, it will start producing recommendations for facet selection and order, like this:', 'Query: golden pendant', 'Recommended facets: price_range, chain_length, metal_color', "Interestingly enough, the model can capture variations in customer behavior depending on the filters applied. For example, consider the query “nike”, with applied filter product_type:shirts_tops model predicts different next most valuable filter depending on gender refinement. For men's products, the next predicted filter is subproduct_type, such as t-shirt or polo shirt, while for women products it predicts sales and promotions as the next most useful filter. ", 'To evaluate the quality of the facet selection model, we collected facet clicks for 10,000 most popular queries and calculated an ', ' metric which shows how likely customers are to click on the facet or facet value depending on its position on the facet list and the position of the value within the facet. Against a manually tuned facet ranking, we showed 8-10% uplift in this metric, while for value ranking the uplift was in the range of 15-20%.', 'The facet selection model can be integrated in multiple ways:', 'In this blog post, we talked about the problem of selecting the most relevant facets for online retailers and how to use deep learning to select and rank facets depending on customer behavior. ', 'We believe that deep learning has a bright future in search applications, and many more powerful features can be easily implemented using the modern natural language processing models.', 'Meanwhile, happy searching (now with relevant facets!).', ' If you are interested in faceted navigation and would like to discuss this blog post with the authors, don’t hesitate to reach out. ', '.'], 'date': '\r\n\t\t\t\t\t\t\tFeb 28, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'E-commerce', 'Machine Learning and Artificial Intelligence']}, {'title': 'Boosting product discovery with semantic search', 'article url': 'https://blog.griddynamics.com/boosting-product-discovery-with-semantic-search/', 'first 160': ['Search engines have been with us for several decades already, and we are using them daily in our digital lives, so why are we still talking about search? Isn’t search a solved problem already?', 'Search probably would have been a long solved problem if human language were a well-defined, thought-through, and strict construct. If it were, our daily communication would be fast, clear, precise -- and extremely colorless and dull. So, unfortunately for search engineers and fortunately for everybody else, human language is an extremely complicated system riddled with redundancies and ambiguities. For example, consider this:', 'One is a ', ', and the other is a ', '.', 'Same words, completely different meanings just by changing the order of words!', 'People quickly navigate this complexity, relying on the context of interaction and vast background knowledge about the world. They expect modern search engines to be as smart in understanding their requests, to base their understanding not just on the accidental matching of words, but on their meaning, also known as semantics. In other words, people want a ', '.', 'In this blog post, we discuss what language phenomena semantic search has to understand and how to approach semantic search with some of the powerful information retrieval techniques.', '\n', 'The semantic search extends the classical ', ' model which was used as a baseline for full text search engine implementations. It addresses many of its term-centric approach limitations with better support for peculiarities of natural language.', 'In general, semantic search aims to match documents that correspond to the meaning and intent of the query, not just its words like the full text search approach used to do. Because of this, semantic search departs from traditional ways of matching and counting words and focuses on matching concepts. Its advanced techniques allow it to improve precision compared to full text search without worsening its recall.', 'Originally full text search was designed to work with text documents, and its user interface allowed only one way of search results navigation – scrolling from top to bottom. Under these circumstances, seaerch engine could sacrifice precision in favor of recall. Suboptimal precision can be mitigated by advanced ranking which boosts most relevant documents to the top. This approach was generally acceptable, because the majority of users rarely went through enough pages to see irrelevant ones. This is what we still see in web search engine interfaces (even though their internal implementation has gone far away from classical full text search) - millions of matched results, but we rarely go beyond the first few pages.', 'As for e-commerce search, it contains additional elements of customer interaction, which can easily reveal irrelevant products and spoil the experience, even if the relevance-based ranking is doing a very good job:', 'Even if your customers prefer just scrolling down without filtering or re-sorting (which is very likely for mobile experience or products with high importance of visual factor, like fashion or home decor), poor precision still remains a frustrating issue. In the e-commerce domain, it is often very easy for a customer to detect irrelevant products just by looking at their images. If customers sees a lot of products, she will soon become disappointed by time wasted for scrolling through poor precision results.', '\n', '\n', '\n', 'Full text search algorithms were designed to handle loosely structured texts (articles, books, etc.), which contain a few headers and many paragraphs of texts.', 'On the opposite side, e-commerce search has quite a different data source: products are often well-attributed and each searchable value has a key which explains its meaning (brand, type, color, style, occasion, material, etc.)', '\nThis kind of input data becomes a great resource in semantic search implementation. To be honest, as a customer, I feel stunned when I see e-commerce sites with rich attribution (exposed to me via facets), but with poor search implementation.', 'Unfortunately, we cannot expect all aspects of customer intent to be always fully discoverable in key-value product attributes (even with help of synonyms). In reality there will always be scenarios when customers search for concepts located only in unstructured data (product names, descriptions or reviews). Because of it, the eventual solution is always hybrid, but if attributes are good enough to serve the majority of search requests, implementation of basic semantic search techniques can bring significant improvement. If unstructured data dominates, or quality of attribution is diverse across the catalog, it’s worth investing into attributes extraction: not only search, but also other aspects (like facet filters, product recommendations) will benefit from it.', 'The main insight behind semantic search is that one cannot just simply break a search query into tokens and use them to match products in reverse index.', 'Instead of it, semantic search must adhere to following principles:', 'Those principles can be applied by introducing the query understanding phase in the search pipeline.', 'This phase examines the query from different perspectives – and its findings help to build an optimal boolean query as well as ranking criteria for product search.', 'Depending on available data, building a boolean query can be implemented in different manner.', ' based on machine learning algorithms can be employed to convert customer search history data into predictions of search phrase category or product type and improve precision.', 'Semantic search relies on business domain knowledge base, which contains different types of relations between terms relevant within a particular domain:', 'If you have a ', ' associated with a certain keyword (like a category redirect for ', '), then this rule should fire not only for different spellings of this keyword (singular ', ' and its misspellings), but also for other keywords with same semantics (', '). In practice it is often achieved by explicit listing of all keywords, their forms and even popular misspellings in the rule itself, but such approach doesn’t scale well.', 'Once you have semantic search for products, it makes sense to extend it towards evaluation of rule keywords, so that all implemented techniques can be leveraged there. Eventually this approach looks obvious, because in both use cases your final goal is to express customer intention using domain vocabulary.', 'However, there is one common pitfall that you’d better be warned in advance.', '\nWhile hypernyms should be considered in product search, they would better be ignored in merchandising rules evaluation if such rule is intended to override the whole result.', 'Once you implement efficient query parsing, which can convert a text query into filters by attributes, then you can use it in other areas other than search. For example, it can be used to  improve ', ': you can not only remove poorly matched or misspelled phrases, but also to prevent showing semantically similar phrases.', 'If an attribute refers directly to some physical properties of a product, it can pose additional challenges for the semantic search engine, as people can use multiple ways to measure or explain the same physical characteristic.', 'Color search is a valuable use case in the domain of fashion, where the actual look of a product weighs heavily on customer decisions. However, product-to-color relation is not simple. There are different shades of the same color, or a product can use multiple different colors (as striped dress does). Below example shows how differently “blue dress” products can actually look.', '\n', '\n', '\nHere one can rely not only on the attribute values, but also on ', ' to resolve these issues. The model can ', ', and we can consider a color distance between the color mentioned in the query and the multiple colors associated with the product, giving the dominant color the maximum weight.', '\nIt can also include grouping products by their relation to this color: shade, coverage, patterns. etc.', 'Retailers have other use cases for color search. One of the more interesting examples is home improvement’s requirement to use color search for paint matching. A customer who wants to repaint an object like an interior wall, and is looking for paint used initially to match the existing color. Due to the variety of paint colors, the customer may not always use primary color names such as ', '. They may use official name variations such as ', ' or ', ', or unofficial terminology, like ', ' or ', '. The search system should recognize those colors and return the closest paint products. In this case, the color search is a critical property, so it may be advisable to build distinctive UI elements specifically for paint selections on this site.', 'Size search has additional challenges in domains, where real physical sizes are used. The semantic search should be able to handle the following aspects of “dimensional” size search:', 'In addition to it, Semantic size search cannot expect that every customer knows and obeys all above rules. So that when a customer makes a mistake (like using single apostrophe for something usually measured in inches), Semantic size search needs to handle it gracefully.', '\n', '\n', '\n', 'Age search becomes important for child-related products when using age as a measurement of different characteristics of a product, such as size for clothing or complexity for toys. Semantic search should have the ability to recognize ages formulated in different ways:', 'A customer may search, not for a particular price, but a price range instead. A typical example is a search for products below some price value using patterns like ', ' or ', '. Semantic search needs to recognize these patterns and convert them to the appropriate filters by price range in a Boolean query.', 'A semantic partial match is another example of semantic search benefits.', '\nWhen the query doesn’t match any product exactly, we have to relax matching requirements, which can lead to multiple alternative sub-queries to be evaluated.', '\n', '\n', 'In the above example, the query ', ' couldn’t match any products with all its words, so that eventually different types of black products were returned (jeans, skirts and even rugs). As a customer, I can easily notice that rugs are very irrelevant for this query, so I would expect it to be not only buried but filtered out.', 'One needs to decide how to choose those subqueries which are too irrelevant to be used. Full text search approach could base filtering out sub-queries on words-related factors, such as a number of preserved words in a query or their index frequency. Both approaches are far from optimal. For example, if your site doesn’t carry ', ' products, but carries ', ', ', ' and ', ' products, then “blue calvin klein” (3 preserved words) shouldn’t be considered as more relevant than ', ' (2 preserved words), should it?', 'When making a decision what words to omit in the query, semantic search has to consider the saliency of the attributes of omitted words in each sub-query.', 'In our example, dropping the product type ', ' is not a good idea (as customers are very unlikely to agree on getting other product type, but with same color or brand), so ', ' interpretation should be filtered out. Other two options (', ' vs. ', ') have similar relevance.', '\nAs a part of customer-oriented search experience, it is recommended to explain which words were omitted (so the customer can try to rephrase them) as well as group products by sub-queries (if more than one was eventually used for returned products).', '\n', '\n', '\n', 'When a customer is searching for brands not carried by your site, then just omitting a brand name in a query is not efficient. Instead, you should still be able to understand the intent of the search request and suggest a reasonable alternative. For example:', 'It is a good practice to explain the substitution explicitly to the customer (', '), so that the customer does not continue fruitlessly searching other phrases or categories, but explore provided suggestions.', '\n', '\n', '\n', 'When a customer is using a short query like ', ', ', ' or ', ', we have to deal with a situation when we have many products that seem to match the customer query perfectly. We may, after all, have hundreds of dresses, watches, or Nike products in the catalog. Which products should display on the first page? A short query is a common example of “head queries”, which are very frequent, quite short, and match many products equally well, so they get an equal relevance score from the search engine.', 'A popular approach is to break this tie by employing site-wide product-level business metrics such as sales, margins, inventory, rating, and combinations of thereof. Those metrics can produce a ranking score used as a secondary ranking criterion applied to break ties for equally relevant products.', 'However, while providing a solid baseline for the tiered relevancy ranking, it is not always the best thing we can do. Consider the following:', 'It is essential to resolve ambiguities to employ an understanding of customer intent obtained from the clickstream. Using clickstream data, we can train both catalog-wide and personalized ML models to ', ' and resolve the query ambiguity.', 'For doing spell correction it is not enough to merely identify an unknown word and correct it to a known word with the closest edit distance. It must be corrected with a word that fits into the context of adjacent words in a query. Moreover, such correction is required even if the site catalog knows the original word, but it still doesn’t fit the phrase context.', 'Proper implementation of semantic search helps you to establish a more efficient balance between precision and recall, improve query understanding and make customer search journey seamless and delightful.', 'However, the power of semantic search largely depends on the richness and quality of the domain data - product attribution as well as synonyms.', 'If your customers often perform out-of-dictionary search, then semantic search quality will suffer. It can include', 'Even though such use cases can be handled with the help of business rules and special synonyms, it is generally hard to scale. To deal with such tricky queries, we recommend to look into ', ' which can be used to augment capabilities of semantic search implementation.', 'Happy semantic searching!'], 'date': '\r\n\t\t\t\t\t\t\tMar 13, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'E-commerce']}, {'title': 'Tiered machine learned ranking improves relevance for the retail search', 'article url': 'https://blog.griddynamics.com/tiered-machine-learning-ranking-improves-relevance-for-the-retail-search/', 'first 160': ['Online retailers operating large catalogs are always looking to improve the quality of their product discovery functionality. Keyword search and category browse experience powered by their search engine is a first stage of sales funnel where fast and relevant search results can produce delightful and seamless shopping experience leading to higher conversion rates. ', ' rely on matching words in the queries to words in the products, and making ranking decisions based on frequency of the matches. Search relevance engineers spend a lot of time tuning ranking formulae to carefully balance the value of words matching in different attributes, making decisions like “match in brand field is twice as important than match in category field” or “partial match in product name is 5 times less important than full match”. They also take into account business signals, such as price, margins, sales, inventory, etc...', 'Even when ranking formulae is well-tuned, in many search sessions customers are using broad words, such as “jeans” or “shirt” or “Ralph Lauren” which match many products equally well. There is simply not enough information about shopping intent in the query to make a good ranking decision. At the same time, customer behavior data on the site provides a trove of information on what products customers are really interacting with when searching for particular phrases. ', 'Using machine learning, we can use this information to automatically devise optimal ranking formulae which will also self-learn and adapt to changes in customer behavior. This approach, known as learning-to-rank (aka LTR), is a de-facto standard ranking feature in modern retail search systems. However, because of high computational complexity of ML models, it can only be applied to dozens or hundreds of top search results, limiting its potential. ', 'In this blog post, we will describe a tiered learning-to-rank approach which allows us to work around this limitation. ', 'First, lets review how learning-to-rank is implemented in popular open source engines (', ' and ', ')', 'Search query is sweeping through the search index, producing candidate matches. Every match is scored according to ranking formulae and collected into priority queue to find the top best matches. Without learning-to-rank, those top matches are returned to the customer. With learning-to-rank, collected top matches are re-scored according to the ranking model. Ranking model require ', ', which are data points fueling decisions made by the model. Features can be as simple as a value of particular product attribute or as complex as a result of search subquery which has to be separately executed. Ranking model takes into account all those features to re-score the top N products which are returned to the customer.', 'As you can see, from a performance standpoint the feature extraction and ranking phases can be quite intensive and their complexity directly depends on how many products they have to examine and rank. This is fine to dozens and hundreds, but not for thousands or millions. This is why you can’t run full LTR model against all matching products - feature extraction and ranking will be way too slow.', 'In some search systems, re-ranking is happening not inside search engine, but in a dedicated microservice or even a third party system. Still, this doesn’t change the fact that only the top few dozen results get re-sorted by the machine learning algorithm. It also adds serialization and wiring overhead to the system. ', 'What if we are looking to run LTR ranking against all the products which are matching the query? Our goal is to surface products which are relevant from LTR perspective but can be missed by convenient ranking formulae. We still can do it, yet for performance reasons, the \xa0model should be pretty simple, so it can be used to quickly score every matching product. ', 'This way, we will have a two-tier ranking system. Our first tier of ranking will quickly evaluate every matching product with a simple model and find top N best matches, while the more sophisticated second tier model will take its time to exhaustively re-rank those matches to promote most relevant results.', "Let's review a practical case study of application of this technique to online commerce search. ", 'If we look at learn-to-rank as a machine learning problem, we are trying to predict a correct ranking (order) of the products based on the query features ', ', and product’s features ', 'In other words, we are trying to learn the ranking function ', " which will estimate a product's relevance score for the query.", 'As with any supervised learning problem, we will need some sort of ground truth about the relevance of particular products for particular query. Such ground truth can be based on expert judgement, or on crowdsourcing, but by far the most practical way of obtaining the ground truth signal is to use customer behavior as a proxy to relevance. ', 'Generally, there are three approaches used in learning-to-rank models: pointwise, pairwise and listwise.', 'The pointwise approach is trying to directly predict relevance score, pairwise approach predicts relative order of two candidate products, while listwise approach deals with a whole list of products at once. ', 'As usual in machine learning tasks, before we do any training we need to vectorize our data - e.g. convert our queries and documents to vectors, matrices or tensors.', 'In general, we will have 3 kinds of features:', 'Let’s assume we have following dataset capturing our queries and their relationships with discovered products:', 'In this dataset we introduce a bunch of query-document features', 'Now, we need to develop and train a model which can be used by the search engine to directly score the matching products. To achieve that we will use a neat trick of ', '. For mathematically inclined, we can refer ', '.', 'To the rest of us, the main idea can be described as follows:', 'Before we can proceed, we should figure out how to create a learning target, aka relevance judgement. In general, there is a separate art and science behind relevance judgement model. Researchers and practitioners are devising different kinds of learning targets from clickstream data, from just simply counting clicks on products to ', '.', 'Here, we will consider the following simple technique for generating relevance judgement. We will group all events by the query and will define relevance score as', 'This formula takes into account general attractiveness of the document in the result as ', ' as well as add-to-cart ratio as indicator of relevance. ', 'Now, let’s look at the example of pairwise transformation. We will start with the following simple dataset.', 'First, we generate pairs of products from it, encode differences between their features and create binary and float prediction targets.', 'Target after pairwise transformation shows which product in pair of products is considered more relevant for the query - ', ' or ', ', based on score. ', 'Next, we train a popular collection of "usual suspects" of linear models: logistic regression, linear regression, linear SVM. Resulting weights are transferred to rank products in the search results. In order to evaluate the quality of the model, we used following search quality technical metrics:', 'We define “binary target” as a simple relevance metric which indicates whether a product was added to cart after the query. We consider products which are never added to cart as irrelevant for the query.', '“Float target” is calculated as a score normalized for the maximum score within the query, and is used as a proxy for relevance when calculating NDCG and precision metrics.', 'For precision metrics, following diagram explains it visually:', 'After evaluation of the model results we selected logistic regression as a winning model. Other more complicated models didn’t show substantial uplift compared to logistic regression on our dataset. ', 'We compared our metrics with a baseline of standard search results (without existing LTR resorting of first few pages) and observed a massive uplift in all metrics:', 'This means that new model is much better than a baseline in predicting attractive and relevant products and pushing those products to first pages of search results.', 'In this blog post, we made a case and described a practical approach to building a tiered learn-to-rank search system which can extend the coverage of machine learned ranking to the whole set of matching products, not only to the first few pages of the results. Because of the linear nature of the model this can happen without significant impact on search performance. As a result, larger number of relevant products are pushed to the top few pages by the search engine where ranking can be further optimized by more sophisticated algorithms. ', "If you are interested in search quality and learning-to-rank and want to have a discussion with us, don't hesitate to reach out!", 'Happy searching! (now with full-fledged learn-to-rank)'], 'date': '\r\n\t\t\t\t\t\t\tMar 03, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'Machine Learning and Artificial Intelligence', 'E-commerce']}, {'title': 'Understanding search query intent with deep learning', 'article url': 'https://blog.griddynamics.com/understanding-search-query-intent-with-deep-learning/', 'first 160': ['Online retailers are always looking for ways to provide delightful and frictionless shopping experience to their customers. Product discovery, powered by search and category browsing, stays at the top of the sales funnel and has the highest impact in converting visitors to customers. \xa0At the same time, modern customers are not that easy to impress. Customers of the new AI age expect search systems to understand their shopping intent, even if their search queries are not very specific. ', 'In this blog post we will talk about under-specified queries and how deep learning NLP models help employ customer engagement data to provide relevant results and improve e-commerce revenue per session. ', 'In approximately 30%-40% of the sessions customers start their search journey with very broad queries like “mens clothes”, “nike” or “handbags” which provide very limited information about their actual shopping intent. Some of those queries, like “mens clothes”, match literally half of the catalog, and nothing in the search query itself can help the search system to determine in what order should the products be ranked. Should we show jeans at the top or dress shirts? Or perhaps t-shirts?', 'Another example of the same problem is term ambiguity.', 'When a customer is looking for a scarf, does she mean a style of the curtain in home decor or an item type in apparel? Of course, we can interpret the query to show results for both terms but this will definitely lead to showing lots of irrelevant results and thus generate customer frustration. ', 'Traditionally, merchandisers have employed business rules to deal with this ambiguity, either by promoting a particular product type, their best-sellers or high-margin products. However, such rules are pretty hard to maintain and scale to all possible scenarios. We should be able to do better than that. ', 'What we are looking for is a way to employ customer engagement data to resolve ambiguity in search phrases. By analysing customer search sessions with information about the products they eventually added to the basket or purchased, we can “understand” that when customers are searching “nike” they most often mean t-shirts first, and then shoes. This intent understanding can help us rank search results accordingly or filter them to show only relevant product types. ', 'However, as customer queries can come in endless variations, we need to be able to generalize across all those variations and still properly predict actual customer intent. This is where modern natural language processing models, powered by deep learning, can save the day.', 'In this blog post we will focus on classifying customer shopping intent by product type and subtype, which is arguably the most important piece of intent to understand. ', 'From the clickstream data we will build a ', ' - a dataset containing search queries along with distribution of customer’s interaction with a particular product type or subtype. This distribution of customer interaction will depend on seasonality, query volume, numbers of clicks, adds to cart and checkouts for each particular query and product type.', 'Our goal is to build a model which can predict proper product type distribution based on the search query and season. Product types getting the highest prediction score in the distribution will be recommended.', 'We will start by training a language model based on a large volume of unlabeled retail data. The language model will help represent our queries as semantic vectors, i.e. words with similar meaning will be clustered together in multi-dimensional vector space. This will greatly improve the quality of intent recognition. We will use a popular ', ' library to fine tune the word embedding model on all available data: search queries, product names and descriptions, reviews and attributes.', 'Once the fasttext language model is trained, it will be able to convert our search queries into matrixes where every word is represented by a 100-dimension vector:', 'We will also concatenate the seasonality vector to our word embedding in the form of one-hot encoded season vector. With this encoding, winter will be represented as [1,0,0,0], spring as [0,1,0,0] and so on.', 'Now, we need to employ a deep neural network to learn the patterns in query matrixes and classify queries to appropriate intents, represented as labels in our dataset.', 'We will be using a one-dimensional convolutional neural network (CNN) to learn the patterns in word combinations contained in the queries. Here is a conceptual framework for 1D-CNN processing of a vectorised query:', 'We have a matrix of word embeddings of a particular dimension, say 5 in this example. Every row of this matrix corresponds to a word in our query, up to a maximum supported query length. Shorter queries will be zero-padded to an appropriate length. ', 'We will process this matrix using 3 different sizes of 1D convolutional filters: 4,3,2. For each size of the filter, we will choose 2 filters for the network, getting 6 filters to learn during training. ', 'As we are dealing with 1D convolutions, our filters will produce feature maps as vectors of dimension (matrix_size - filter_size + 1), corresponding to all possible ways to position filter within the original matrix. Each feature map will be coarsed using “max pooling” to a single scalar to capture only the most salient feature. ', 'Resulting coarsed feature maps from all the filter of all the sizes will be collected and concatenated into a single semantic feature vector which will represent the original query. This vector will be an input to the fully connected and soft max layers which will perform actual classification of the queries.', 'A real-life CNN will be very similar to this example and differs only in the size of word embeddings (100), a number of filters to be learned, also 100, and a number of labels to classify:', 'To evaluate the quality of the model we will use the accuracy@N metric, which estimates in what percentage of predictions we get a correct product/subproduct type within top N results. In our experience, even this relatively simple model achieves 70% accuracy in top-1 predictions and 92% in top-5 predictions. If we estimate the accuracy based on the actual volume of queries, e.g. without removing duplicates, we will get 83% for top-1 and 95% for top-5 predictions. ', 'When we account for the seasonality signal, the model is able to predict that in autumn people searching for “nike” are most likely looking for hoodies, and in spring the same query shows a “running shoes” intent. ', 'The intent understanding model can be integrated with search and browse experience in several ways:', 'In our experience, integrating query intent understanding leads to a direct uplift of 2-5% in revenue per session, providing a solid ROI for a deep learning model training and integration.', 'In this blog post, we described a problem of underspecified and ambiguous queries, and how modern, deep learning-based, query intent understanding models can help tackle those queries in the most effective way so they can deliver precise and timely search results to customers.', 'Please do not hesitate to ', ' to learn more about search and natural language processing. ', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tFeb 22, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAleksandr Vasilev', 'author_url': 'https://blog.griddynamics.com/author/aleksandr/', 'tag': ['Search', 'E-commerce', 'Machine Learning and Artificial Intelligence']}, {'title': 'Understanding search query intent with deep learning', 'article url': 'https://blog.griddynamics.com/understanding-search-query-intent-with-deep-learning/', 'first 160': ['Online retailers are always looking for ways to provide delightful and frictionless shopping experience to their customers. Product discovery, powered by search and category browsing, stays at the top of the sales funnel and has the highest impact in converting visitors to customers. \xa0At the same time, modern customers are not that easy to impress. Customers of the new AI age expect search systems to understand their shopping intent, even if their search queries are not very specific. ', 'In this blog post we will talk about under-specified queries and how deep learning NLP models help employ customer engagement data to provide relevant results and improve e-commerce revenue per session. ', 'In approximately 30%-40% of the sessions customers start their search journey with very broad queries like “mens clothes”, “nike” or “handbags” which provide very limited information about their actual shopping intent. Some of those queries, like “mens clothes”, match literally half of the catalog, and nothing in the search query itself can help the search system to determine in what order should the products be ranked. Should we show jeans at the top or dress shirts? Or perhaps t-shirts?', 'Another example of the same problem is term ambiguity.', 'When a customer is looking for a scarf, does she mean a style of the curtain in home decor or an item type in apparel? Of course, we can interpret the query to show results for both terms but this will definitely lead to showing lots of irrelevant results and thus generate customer frustration. ', 'Traditionally, merchandisers have employed business rules to deal with this ambiguity, either by promoting a particular product type, their best-sellers or high-margin products. However, such rules are pretty hard to maintain and scale to all possible scenarios. We should be able to do better than that. ', 'What we are looking for is a way to employ customer engagement data to resolve ambiguity in search phrases. By analysing customer search sessions with information about the products they eventually added to the basket or purchased, we can “understand” that when customers are searching “nike” they most often mean t-shirts first, and then shoes. This intent understanding can help us rank search results accordingly or filter them to show only relevant product types. ', 'However, as customer queries can come in endless variations, we need to be able to generalize across all those variations and still properly predict actual customer intent. This is where modern natural language processing models, powered by deep learning, can save the day.', 'In this blog post we will focus on classifying customer shopping intent by product type and subtype, which is arguably the most important piece of intent to understand. ', 'From the clickstream data we will build a ', ' - a dataset containing search queries along with distribution of customer’s interaction with a particular product type or subtype. This distribution of customer interaction will depend on seasonality, query volume, numbers of clicks, adds to cart and checkouts for each particular query and product type.', 'Our goal is to build a model which can predict proper product type distribution based on the search query and season. Product types getting the highest prediction score in the distribution will be recommended.', 'We will start by training a language model based on a large volume of unlabeled retail data. The language model will help represent our queries as semantic vectors, i.e. words with similar meaning will be clustered together in multi-dimensional vector space. This will greatly improve the quality of intent recognition. We will use a popular ', ' library to fine tune the word embedding model on all available data: search queries, product names and descriptions, reviews and attributes.', 'Once the fasttext language model is trained, it will be able to convert our search queries into matrixes where every word is represented by a 100-dimension vector:', 'We will also concatenate the seasonality vector to our word embedding in the form of one-hot encoded season vector. With this encoding, winter will be represented as [1,0,0,0], spring as [0,1,0,0] and so on.', 'Now, we need to employ a deep neural network to learn the patterns in query matrixes and classify queries to appropriate intents, represented as labels in our dataset.', 'We will be using a one-dimensional convolutional neural network (CNN) to learn the patterns in word combinations contained in the queries. Here is a conceptual framework for 1D-CNN processing of a vectorised query:', 'We have a matrix of word embeddings of a particular dimension, say 5 in this example. Every row of this matrix corresponds to a word in our query, up to a maximum supported query length. Shorter queries will be zero-padded to an appropriate length. ', 'We will process this matrix using 3 different sizes of 1D convolutional filters: 4,3,2. For each size of the filter, we will choose 2 filters for the network, getting 6 filters to learn during training. ', 'As we are dealing with 1D convolutions, our filters will produce feature maps as vectors of dimension (matrix_size - filter_size + 1), corresponding to all possible ways to position filter within the original matrix. Each feature map will be coarsed using “max pooling” to a single scalar to capture only the most salient feature. ', 'Resulting coarsed feature maps from all the filter of all the sizes will be collected and concatenated into a single semantic feature vector which will represent the original query. This vector will be an input to the fully connected and soft max layers which will perform actual classification of the queries.', 'A real-life CNN will be very similar to this example and differs only in the size of word embeddings (100), a number of filters to be learned, also 100, and a number of labels to classify:', 'To evaluate the quality of the model we will use the accuracy@N metric, which estimates in what percentage of predictions we get a correct product/subproduct type within top N results. In our experience, even this relatively simple model achieves 70% accuracy in top-1 predictions and 92% in top-5 predictions. If we estimate the accuracy based on the actual volume of queries, e.g. without removing duplicates, we will get 83% for top-1 and 95% for top-5 predictions. ', 'When we account for the seasonality signal, the model is able to predict that in autumn people searching for “nike” are most likely looking for hoodies, and in spring the same query shows a “running shoes” intent. ', 'The intent understanding model can be integrated with search and browse experience in several ways:', 'In our experience, integrating query intent understanding leads to a direct uplift of 2-5% in revenue per session, providing a solid ROI for a deep learning model training and integration.', 'In this blog post, we described a problem of underspecified and ambiguous queries, and how modern, deep learning-based, query intent understanding models can help tackle those queries in the most effective way so they can deliver precise and timely search results to customers.', 'Please do not hesitate to ', ' to learn more about search and natural language processing. ', 'Happy searching!'], 'date': '\r\n\t\t\t\t\t\t\tFeb 22, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['Search', 'E-commerce', 'Machine Learning and Artificial Intelligence']}, {'title': 'How deep learning improves recommendations for 80% of your catalog', 'article url': 'https://blog.griddynamics.com/how-deep-learning-improves-recommendations-for-your-catalog/', 'first 160': ['Product recommendations have become an essential sales tool for e-commerce sites. These recommendation systems typically use ', ', a common approach for building behavior-based recommendation engines using customer interactions. Collaborative filtering works well when there are adequate user-item interactions, but falls short when interaction data is sparse (see example figure below). Following the ', ', 20% of a catalog will usually get 80% of the traffic, the rest of the catalog does not have enough interaction data for collaborative filtering to make meaningful behavior-driven recommendations. This is called a “', '”. When collaborative filtering fails, most e-commerce sites fall back on ', '. However, using deep learning we can make collaborative filtering approach viable even for products with minimal customer interaction data. In this blog, we discuss how we trained a deep learning model to  map product contents to collaborative filtering ', ' to provide behavior-driven recommendations even for products with sparse data.', 'First, let’s revisit the collaborative filtering approach. We model an interaction between users and items as an “', ',” where cell value represents how likely it is for a given user to select a given product. We have only partial information about the contents of this matrix based on explicit or observed customer behavior. Explicit behavior may be a product rating given by a customer. Observed behavior tries to deduce how much a customer likes the product by implicit signals, for example, when a customer views a product, adds it to their cart, or purchases it. Our goal is to build a model that can predict the values of the empty cells of this interaction matrix.', 'One of the most popular approaches to fill the interaction matrix is matrix factorization. We try to approximate the interaction matrix as a product of two matrices of lower dimensions, user factors and item factors:', 'The scalar product of a row of matrix U and a column of matrix V gives a predicted item rating for the missing cells. Predictions for known items should be as close to the ground truth as possible. We fit those two matrices with known data using optimization algorithms.', 'Once trained, user and item matrices contain latent factors, called embeddings, which represent preferences of a user and features of an item. Each embedding represented some aspect of the user preferences and item feature, which is useful in predicting their behavior. In a well-trained system, similar users and items cluster together in latent vector space, which gives the model its predictive power.', 'One of the classical approaches for matrix factorization is called Alternating Least Squares or ALS.', 'In this approach, we define loss function L and iteratively change user and item embeddings to minimize this loss function. The first part of the loss function is the quadratic deviation of the predicted rating from the ground truth. The second part is the regularization, which helps to prevent overfitting by suppressing embedding vector norms.', 'The convergence process is iterative. Each step solves a local optimization problem for one matrix considering the values of the second matrix as fixed constants. We solve the optimization problem for another matrix, and the process repeats. Due to this repeating process, the loss function at each step is convex; it has no local minima. This algorithm then converges very quickly.', 'Matrix factorization produces item embeddings when there is a lot of interaction data available. It falls catastrophically short, however, when it comes to ', ' items and new users, as embeddings become mostly random values. Visualizations of clusters begin to appear after training ALS:', 'In the center, a noise cluster forms from elements with a small number of user interactions. These are long-tail products where the number of interactions is less than 300. Green dots represent products with a large number of user interactions, where there is a cluster structure of similar products.', 'Using ALS embedding we get, not only user-item recommendations but also “similar items” or “item-to-item” recommendations using nearest neighbor search in product embedding latent space. The quality of such recommendations, however, quickly deteriorates when interaction data is sparse. In the below chart, we see the percentage of irrelevant products in the recommended output to the number of interactions as assessed by an expert curator.', 'We need to find another approach to provide high-quality recommendations in the last case while maintaining the structure of the ALS vector space. Enter deep learning. As viewed as universal function approximators, deep learning models can use the content of the items to predict ALS embedding for them. We can train a deep neural network which can take item attributes, text, features, and images to predict ALS embedding of this product.', 'When the neural network is trained to predict ALS embeddings, it takes into consideration user-to-item interactions. We train a neural network to capture the most critical features from the images, text and other available attributes to predict the most valuable information from the clickstream encoded in the ALS embeddings.', 'Shown below is the architecture of the neural network:', 'To vectorize text features, we use pre-trained ', ' (BERT). BERT obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. For image feature embeddings, we used several ', ' attribute level classifiers with a subsequent concatenation of embeddings. We also include other features, like price, brand and occasion. For minimizing space between predicted embedding and ALS embedding, we use MSE Loss.', 'After several cycles of training, we visualize the ALS latent space again:', 'The new space of items contains separate clusters. Now long-tail products with low customer interaction are intermingled with “fat head” products with a lot of customer interaction.', 'Let’s retry the nearest neighbors of the previous low popularity product with id=8757339 (top left). After users clicked on the target dress just three times, the results are far more relevant:', 'Now similar items for the products from the long tail looks much better.', 'Formal model evaluation metrics, calculated on low popular products, also show quite an improvement: we achieved an increase in ', ' (NDCG) @10 from 0.021 to 0.127 and average precision from 0.014 to 0.092.', 'By fixing the low volume product collaborative filtering issue with deep learning, we now have a powerful instrument at our disposal to improve the quality of our recommendation systems. To improve sparse data recommendations, we trained a neural network to approximate ALS embeddings for popular products. Using product features, like image and text, we can predict ALS embeddings for products from the “long tail.”', 'We can now show high-quality recommendations for the entire product catalog, for both popular and less popular products. We discussed how a retailer’s sales  concentrates on only 20% of the product catalog because the remaining products do not contain enough interaction data for the recommendation engine to analyze properly. Using long tail techniques discussed in this blog, we can now engage hard to reach customers with recommendations capable of serving 100% of the catalog! This is not a trivial problem. The Pareto principle is widespread and most catalogs experience this 80/20 phenomenon. Deep learning can overcome data interaction inequality and help solve other similar problems in e-commerce and beyond.'], 'date': '\r\n\t\t\t\t\t\t\tSep 25, 2019\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEugene Steinberg', 'author_url': 'https://blog.griddynamics.com/author/eugene-steinberg/', 'tag': ['E-commerce', 'Data Science', 'Machine Learning and Artificial Intelligence']}, {'title': 'iOS Architecture Patterns for Large-Scale Development, part 3: UI architecture approach', 'article url': 'https://blog.griddynamics.com/ui-architecture-approach/', 'first 160': ['After we made all the necessary decisions regarding the general architecture, it was a time to think about the implementation details. We needed to build all the necessary screens and organize navigation between them. Navigation details have already been described in the first part of the article, so let’s get into the subject of feature architecture.', 'In our projects we use various architectural patterns such as MVVM, MVP, VIPER, etc. They all have one thing in common - they make the view layer as thin as possible. The view layer knows nothing about application logic, data storages, and device services. It just displays the formatted data provided by the view models or presenters.', 'The view layer does not read and write data from the data storage directly, instead it uses presenters or data providers for this purpose. This approach has several advantages. First, it solves the problem of massive view controllers from the MVC architecture. We have no controllers with a thousand lines of code that are difficult to read and understand. On the contrary, the view controllers only display the formatted data from the view models and handle user interactions. The code becomes simple and understandable.', 'Secondly, the view layer is becoming more independent and replaceable. Mobile app design trends change quite often and a thin view layer allows us to change views without affecting the rest of the application. We can also change the business logic or the data storage layer without any changes in the view layer.', 'And last but not least, it significantly improves the testability of the view layer. The view layer depends on the UIKit framework and it has life cycle methods. That is why it is difficult to write unit tests for the view layer. If your view controller doesn’t have any business logic and only displays the data from the view model or presenter, you don’t need to test it. You can just test the view model and presenter. So, it doesn’t matter what architectural pattern you choose, if the view layer is thin you are on the right path.', 'The real applications screens can have a rather complicated layout. It can consist of several elements, and each of these elements has its own architecture and can be reused on other screens. Many applications, especially e-commerce applications, have a complicated UI on their home screens. In most cases, these screens have dynamic content. This content depends on many factors, such as upcoming public holidays, sales, new arrivals, etc. Sometimes we need to use several APIs to load this content. In order to solve this problem, we use an approach with a dynamic data source.', 'Each element of this data source is responsible for a specific section on the home screen. It takes data from the API, makes additional requests if needed and configures an appropriate view. We create an array of these data source elements based on the home page API response. Such an approach allows us to create the dynamic content of the page by combining appropriate view models.', 'The next question is how to display the views created by the data source elements. There are several common approaches in building dynamic screens:', "All of them have their own advantages and disadvantages, so let's look at them in more detail.", 'The main advantage of the table view and collection view is lazy data loading. We do not keep all views in the memory and load them only before they appear on the screen. Table view and collection view also use a cell reuse approach that also reduces memory usage. So, in the case of a large screen with many components it could significantly improve the performance of the application in terms of memory.', 'And what about disadvantages? In the case when you have only the API call on the screen, everything works perfectly. But problems begin when you need to make additional calls to load data. These processes are asynchronous and you need to update the data source and reload the view every time the API call ends. While you could wait until all the API calls end and reload the view only once, it will degrade the responsiveness of your screen.', 'The ', ' method of the table view (as well as collection view) runs several asynchronous processes and if you call it asynchronously you can crash the app with inconsistent data source exceptions. So you therefore need to be extremely careful when you update data in a table (or collection) view asynchronously.', 'Another disadvantage of the UICollectionView is that it doesn’t support automatic cell sizes. iOS 13 provides the compositional layout for collection views that supports the automatic cell sizes, but it works only on iOS 13.', 'The main advantage of UIStackView is a simple way to update the views. If you need to show/hide some particular view you just change the ', ' property for this view and that’s it. You don’t need to change the data source and reload the view. It’s very useful when we have several API calls and need to reload the screen asynchronously. We can add the view to UIStackView immediately when the screen appears and hide it until the appropriate API call ends. We don’t need to think about consistency of the data source every time we reload the view and it makes our life easier.', 'But UIStackView also has several disadvantages. The main one is performance. Benchmark measurements show that UIStackView performance is almost two times lower than that of auto layout and many times lower than that of manual layout calculation. This can cause scrolling lags on older devices. It also doesn’t support lazy loading and reusing of views. This fact could lead to memory issues in the case of screens with lots of elements.', 'The second disadvantage of UIStackView is its limited layout options. UIStackView supports only combinations of vertical and horizontal stacks of views. In the case of a complicated layout (with floating elements, clipped headers, etc.) it will be difficult to implement this using UIStackView.', 'One more disadvantage of UIStackView is the fact that it is not a view. It doesn’t have its own layer. It means we cannot change properties such as alpha, border size, and color, etc. We also cannot apply layer masks and layer transformations to it.', 'Summarizing all of the above, UIStackView provides an easier and safer way to refresh view frequently. It is suitable for cases when you have a limited number of elements on the screen and quite a simple layout. In cases of a large number of subviews, it could lead to memory and performance issues.', 'UITableView and especially UICollectionView work perfectly for complicated layouts with a large number of elements. UICollectionView allows us to implement any kind of layout. But you should be extremely careful if you need to refresh the view asynchronously. There is no way to hide or show elements without the data source changes and it is easy to catch inconsistent data source exceptions. So, you need to make a decision based on your screen needs and requirements.', 'In October, 2019 Apple introduced a new framework for building user interfaces, SwiftUI. But SwiftUI is not only a UI framework, it also changes the whole application building process. So, you can’t just take your project built on UIKit and transfer it to SwiftUI. It would be like building a completely new project. SwiftUI changes not only the UI layer but the whole application architecture. You can start a new project with SwiftUI but transferring the old one is not a good idea.', 'Let’s look at an example. Almost all e-commerce applications have a product details screen. This screen contains the product description, photos, configurations (like size, color, price set). It could also contain banner ads and similar products. These sections may be dynamic depending on the type of product because shirts and sofas have different configuration parameters and user input in one section may lead to changes in another section.', 'For instance, if the user selects a specific color we want to show the product photos with that color. To implement this we first need to create the data source. The root element of the data source should create an array of section models based on the product type. Each model should make additional API calls if needed and configure the view (in the case of UIStackView) or the cell (in the case of UITableView/UICollectionView).', 'So we now need to decide what kind of container we want to use to display these views. To do this we need to realize what problems need to be solved:', 'For this screen we are going to use the UIStackView as a container for the views. But if in the future the design changes and there will be more repeating elements or the layout becomes more complex, then perhaps we will change our mind in favor of the UITableView or UICollectionView. Thus, each time you need to choose which approach to use, you should make a choice based on the needs and tasks of a particular screen.', 'Our rewrite journey took approximately 12 months and is now almost complete. We set ambitious goals of taking the project development process to a new level but with a minimum of required effort and a smooth transition from the existing setup. There were some limitations and obligations but we did it successfully by balancing between business goals, engineering excellence, and level of expertise. Let’s summarize what we achieved:'], 'date': '\r\n\t\t\t\t\t\t\tOct 07, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tEgor Zubkov', 'author_url': 'https://blog.griddynamics.com/author/egor-zubkov/', 'tag': []}, {'title': 'How automated merchandising tools can improve online product discovery', 'article url': 'https://blog.griddynamics.com/merchandising-platforms-can-improve-product-discovery-for-retailers/', 'first 160': ['A modern merchandising system can provide a guided product discovery experience to shoppers, which is essential for retailers with large online catalogs. These systems and tools allow merchandisers to improve product listing relevancy by controlling product matching and ranking when a shopper sees a large number of similar items. With automated merchandising tools based on precise customer targeting, powerful ranking rules, business signals, and customer data, merchandisers no longer need to manually sort products in categories or create curated results for hundreds of popular search phrases.', 'Before we dive into further details, here are the topics we will cover in the remainder of the post:', 'That being said, let’s examine relevance and its place in the e-commerce world.', 'When defining a mission for product discovery teams, businesses often state that they should “provide the shortest path towards the most relevant products in the catalog”. While this goal is very clear on the surface, accomplishing it requires a deep understanding of relevance, a key concept which is pretty challenging to pinpoint. Academic papers written on ', ' often define relevance as “bringing users the content which satisfies their information need as specified by search query”. Yet, in e-commerce practice, relevance also relies on implicit signals, such as the knowledge of the customer, the context of the particular customer engagement, and the general business context.', 'For example, when a customer is searching for a “cocktail dress”, which of the 800 matching cocktail dresses in the catalog should we show first? Do we know the customer’s size? Their favorite designers? Color preferences? Price sensitivity? All these questions are important to determining the relevance of a product to a particular customer. On the business side, maybe we need to push high inventory items to the top of the results page to ensure that we don’t sell out during large sale events? Or maybe we have a deal with some designer to promote their products this week? In a broad sense, the product discovery system should strike a balance between a customer’s intent to buy a particular product and a business’s intent to sell that product.', 'So, what are some specific merchandising tools that can help merchandisers maintain this delicate balance in relevance?', 'Let’s take a look at the distribution of search queries on a typical e-commerce site. In most cases, we see a common, ', ' distribution picture:', 'This distribution shows that 50% of the query traffic is coming from a relatively small set of queries, called the “head”. The other 50% of query traffic comes from a huge number of rare or unique queries, which form a long tail of query distribution. Head queries often involve the most valuable items within an e-commerce catalog, such as the most popular product types, brands, and styles. Therefore, it’s no wonder that head phrases are responsible for the bulk of the retailer’s revenue.', 'Merchandisers almost universally focus their efforts on guiding customers through the search experience associated with head queries. The long tail, on the other hand, is largely left for search algorithms to address. For these queries, ', ', query classification, ', ', concept search, normalizations, sophisticated ranking, and other information retrieval and machine learning techniques can provide a solid relevance basis.', 'Head queries often present significant challenges for traditional information retrieval and ranking systems. One of the reasons for this is that head queries are typically short and simple phrases that match a large number of products. For example, a customer’s query for “handbag” doesn’t provide enough context for search engines. From the search engine perspective, all handbags in the catalog are equally relevant to the customer. These are the situations when search engines need an additional signal coming from the merchandiser or from the customer’s history to determine relevance. Since we can’t expect that the majority of customers have an interaction history deep enough to provide additional relevance signals, merchandisers often stand on their own in the quest for relevance.', 'To solve this problem of search engine “indifference”, merchandisers should be able to inject their bias into product scoring formulae based on business relevance. This consists of product and SKU business metrics, such as newness, popularity, inventory levels, SKU availability, review ratings, sales and returns numbers, and many others. The combination of these business signals provides an additional signal which will re-rank equally relevant products by their business value. This technique is especially valuable for category browse, when all products belonging to a category are equally relevant from the standpoint of a search engine.', 'Referring is another powerful tool for merchandisers. The customer experience can often be optimized by redirecting customers from a keyword search to a particular category, landing page, or even mini-shop created around a specific designer or style of the product type the customer is looking for. Sometimes, it is desirable to just send customers directly to the product page. This may happen when a customer is searching explicitly for a single product (by SKU, model number, or marketing ID). In other cases, we can detect that the customer is looking for a product which is not carried by the shop, and suggest an alternative instead of showing zero search results. This suggesting applies not only to a specific product, but also to whole product types and categories. For example, if the customer searches for “Hawaiian shirts”, and the retailer provides none, they should suggest colorful button-down shirts as a reasonable substitute. In all of these cases, the merchandiser should have a clear way to detect customer intent and configure the appropriate referring action.', 'At the heart of an effective online catalog, there is a carefully tuned category hierarchy which reflects how customers think about a retailer’s assortment of products. Managing this hierarchy and assigning products to categories manually is a tedious task for merchandisers. Modern merchandising platforms should therefore provide tools for automated product categorization which take category definition and category inheritance rules into account. New products should automatically become a part of a proper category based on their attributes and business metrics, such as: newness, price range, time-on-sale, and sales numbers. For example, merchandisers should be able to easily create categories such as “Gifts under $50”, “Most popular items”, or “New arrivals”.', 'Another powerful technique is to further organize the presentation of category browse or search results by grouping products by an attribute such as: color, style, purpose, or occasion.', 'Grouping products into collections like outfits, gift baskets, dinnerware, bedding sets, or tool sets will help to cross-sell products and further inspire customers.', 'In the operations of online retailers, there is often a need to override “natural relevancy” decisions made by the search engine from a keyword search perspective. There can be issues with product attribution, which causes the search engine to return bizarre results, or with special business constraints. For example, this might not allow results from several designers to be mixed together, even if the products match the user query. Other times, there is a need to pin some featured products on top of the search results, or take an opportunity to cross-sell a complementary product. Another situation for overriding natural relevancy decisions is when there’s a specific trendy term or marketing campaign name which can not be handled directly by the search engine.', 'Additionally, merchandisers and marketers should be able to easily create timed campaigns for seasonal and trendy products by boosting the corresponding product attributes, or by creating dynamic landing pages.', 'A merchandising platform is only as good as its targeting. Merchandisers should be able to design a customer experience based on the detailed customer and business context of the interaction. This context may include the customer query, a current and requested category, the detected customer intent, customer preferences, and other available signals.', 'For the merchandiser, it is important to avoid surprising results, and maintain a consistent customer experience throughout the navigation journey. For example, let’s say that a customer is looking for Hurley brand shoes.', 'There are many ways to arrive at essentially the same result set. The customer could search for “Hurley shoes”, or search for “Hurley” and select the “shoes” product type facet, or go to the shoes category and select “Hurley” in the brand facet. Sometimes, an additional query term does not change the result set. For example, queries for “Samsonite luggage” and “Samsonite” will essentially yield the same result.', 'From a merchandising perspective, the customer experience on a result page should mostly depend on the product assortment in terms of product ranking, facet selection, and featured products selection. It is more productive for the merchandiser to analyze the result set, rather than trying to figure out how to recreate the same result set. Thus, it should be possible to use characteristics of the result set in making merchandising decisions. For example, merchandisers should be able to configure the selection of shoes-related facets regardless of how a set of shoes was discovered. This can be done through keyword search, category browse with filters, or a combination of both. To achieve this unification of experience, the merchandising platform should support “result-based context”, in addition to “request-based context”.', 'The modern approach to merchandising calls for a certain amount of discipline, and a data-driven approach when it comes to merchandising decisions. The merchandising platform should seamlessly integrate with customer experimentation systems to allow for data-driven decision-making and A/B testing. In general, the customer allocation for experimentation should be treated as yet another interaction context parameter, and be allowed to tweak the customer navigation experience within the experimental group.', 'The large-scale implementation of a merchandising platform is likely to result in thousands of business rules, managed by dozens of merchandisers. The governance and management of these merchandising rules requires special support from the merchandising platform. It should be possible to easily search and dissect merchandising rules, and come up with usable analytics for them, such as highlighting rules which never or rarely fire, or frequently cause rule conflicts.', 'Merchandising platforms should offer instant help in rule creation, such as suggesting proper attribute names and making recommendations. For example, when selecting facets to show in a given situation, the platform should be able to recommend the most common and most salient attributes per product type.', 'In many cases, merchandisers should allow the platform to go on auto-pilot. This makes sense for decisions such as selection of the facets, featured products, and default product ranking. However, it should be easy to make a single override when needed.', 'Most importantly, merchandisers should be able to immediately preview the results of rule changes, and the platform should be able to clearly explain what rules have fired for any given interaction, and how they affect the customer experience. With a sufficient number of rules, conflict between rules becomes a frequent situation, so the platform should have a simple, yet powerful model to resolve such conflicts before they become an issue for customers. Transparency for rule changes and behavior is needed in these situations, and is therefore vital for a merchandising platform.', 'With modern merchandising systems, department stores can more easily provide a guided product discovery and selection experience through their expansive catalogs. Even better, these tools are largely automated, greatly reducing the time spent configuring results pages manually. Merchandising is important for any large department store with a massive online catalog, making these tools of great value.', 'Now that we’ve laid out the overview for what merchandising tools are required by large retailers, our next post in the series will be a technical explanation of how to build a merchandising platform that contains all these tools. If you are interested in learning more about this topic, and our experience with building and running sophisticated large-scale merchandising platforms, don’t hesitate to contact us — we will be happy to help.'], 'date': '\r\n\t\t\t\t\t\t\tDec 06, 2018\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tDmitry Sobolev', 'author_url': 'https://blog.griddynamics.com/author/dmitry-sobolev/', 'tag': ['E-commerce']}, {'title': 'How to use GCP and AWS big data and AI cloud services from Jupyter Notebook', 'article url': 'https://blog.griddynamics.com/work-with-google-cloud-and-aws-services-from-jupyter-notebook/', 'first 160': ['How does a data scientist typically perform their standard job stack such as data processing and training models? In the case of playing around with test data or models, all the work can be done locally. But when that isn’t the case, data specialists often resort to the help of cloud solutions, which requires a lot of additional knowledge about SDK cloud libraries, command-line tools, and code deployment. ', 'Typically a user can perform data processing or training jobs on cloud clusters using CLI tools either from the command line or in scripts and other automations. Another option is using a SDK for the language of choice. It enables developers to create and manage cloud services and provides an API as well as access to services in scripts and other automations. ', 'It is also quite often necessary to get information about both running and completed jobs. Users can do it using the same tools or via the cloud console. Using different tools for completing one task can be quite inconvenient so having an instrument that allows data specialists to perform multiple tasks and access desired data and results without having to leave a familiar work environment can be very practical and time-saving.', 'What is the main tool of a data specialist? As a rule it is Jupyter Notebook. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. ', 'A data science engineer’s role includes performing exploratory data analysis, data preparation, and training processes. The IPython kernel is the Python execution backend for Jupyter. Magics are instruments that are specific to and provided by the IPython kernel. There are various available Magics functions that IPython ships with. You can create and register your own Magics with IPython.', 'ML-DSL is an open source machine learning library developed to simplify the data specialist’s experience of interacting with cloud platforms such as Amazon AWS and the Google Cloud Platform. It lets data scientists and data analysts configure and execute ML/DS pipelines.', 'The main idea of ML-DSL is performing the standard data specialist’s job flow on cloud platforms from a Jupyter Notebook. Using specific Magics, you can prepare scripts to run spark jobs, train and deploy models, and get deployed model’s predictions.', 'The following features are currently available:', 'The ML-DSL provides a unified API and is responsible for interacting with cloud resources such as storages, distributed data processing, and calculations services that use Apache Hadoop and Spark stacks. It also provides machine learning platforms to build, train, and deploy machine learning models. ', 'There are two implementations:', 'There is a set of profile classes that let you define all the necessary information for submitting and running jobs on cloud clusters and platforms: name of cluster, region, default bucket on cloud storage, and job-specific features such as job name and arguments.', 'Another important part of the ML-DSL API is the class of Jupyter “magic” functions. They allow specialists to define what an action will be doing:', 'There is also a set of shortcuts that is useful to define the details of a job. For example -e (--exec) for executing script immediately as a Jupyter cell (for testing purposes) or -n (--name) defines the name of a python script.', 'In this article, we are going to walk you through the experience of using ML DSL to execute an end-to-end data science process of data preparation, model training, and model serving in the cloud directly from a Jupyter Notebook.', 'Imagine you are a data scientist and you have a dataset. It is a corpus of some reviews (movie reviews for instance) and you want to build a sentiment analysis model (LSTM neural network) to classify reviews as positive or negative. The dataset is large and is located on cloud storage (Google Storage or Amazon S3), but you also have a small part of it stored locally.', 'First, you need to of course clean and process the data for further usage. You have written several functions in Jupyter Notebook for reading, cleaning, and using some pretrained word embeddings.', 'Import modules:', 'Read glove.6B.50d.txt using pyspark:', 'Function to read review, tokenize each one, and replace words with indices in GloVe:', 'Join together positive and negative reviews in the train/test dataset and save it for further work:', 'And finally you can use a local sample of your dataset to prepare, train/test the dataset and save it:', 'It works as you expected and you have prepared the dataset. You also prepare code in the Jupyter Notebook for training your LSTM model on this data.', 'Import the python libraries you need here:', 'You have to load the training dataset and word embeddings:', 'Functions to build your neural network:', 'You also want to track losses and metrics and save charts with these:', 'And finally train the model:', 'It works well, returns training progress, and saves the model to a specified path. You’ve just prepared a dataset and trained the model locally on a small part of your dataset.', 'But the full dataset is huge and you would like to use it for training your model. You need a more powerful instance or multiple instances than your laptop to process the whole dataset and train the model.', 'The best idea is delegating a data processing job to a cloud cluster that uses Apache Spark or another open-source big data framework to process and analyze the data. This is a Dataproc for Google Cloud Platform or Amazon EMR for Amazon Web Services.', 'But how to organize it? You know you have credentials to interact with Google Cloud Platform services and there is a Dataproc cluster you can use for your task. So in fact you have all the information you need:', 'So how do you go about submitting the job to Dataproc? As a general rule this can be done by using the console, command-line tools, or client libraries. So your code has to be organized as a python script as well as with all additional code. Then by going to the Dataproc console or by starting a specific command in the shell or by writing additional python code you can submit the job to the cluster.', 'Using ML-DSL Magics you can do all necessary procedures in Jupyter Notebook.', 'First, you can organize your code for a pyspark job by using one of the ML-DSL Magics: ', ', ', ' or ', ' in your notebook. The commands save your code as a python file or load it from an already existing script. You can also run code locally with these Magics for testing.', 'The next step is providing all the information and credentials you need to your task. It is necessary in order to be able to submit the job. With ML-DSL it is possible to reach that by instantiating a PySparkJobProfile class. The class has attributes that describe the information you need to interact with the Cloud Platform: cluster, project (in case of GCP), region, and bucket name.', 'You also define specific attributes of the class: arguments, if there are any for your script, lists of files, jars, archives, and packages when it is necessary. You also have to define the name prefix of your job, for example “word_tokenizer” or “review_processing”. It is beneficial to use meaningful job names. ', 'The last step is to specify the cloud platform (Google Cloud Platform in our case). \xa0There is a specific ML-DSL class Platform for this.', 'And that’s it. You can finally start the job using ML-DSL Magic %py_data. Just add your PySparkJobProfile instance, platform instance, name of your python script, and output path (where the processed data will be saved) as arguments to this Magic and run the cell. ML-DSL will handle all the job tasks: prepare an instance of the spark job with your arguments, make a connection session to the cloud platform, and submit the job to the Dataproc cluster you’ve defined as the profile instance attribute cluster:', 'During execution, ML-DSL returns general information about the job description and status as well as links for the output path and to the logs of the job on the cloud platform \xa0to the output of the running cell.', 'However, what if you have access to the Amazon EMR cluster and want to run your job there? There are not a lot of changes required. You only need to redefine the PySparkJobProfile object and platform with the parameters specific to the case.', 'The command and it’s arguments for submitting the job to an EMR cluster are the same as to Dataproc. Actually ML-DSL does the same process: prepare an instance of the spark job with your arguments, make a connection session to the cloud platform, and submit the job. As you would expect, the final result is fairly similar (except that the data and output path is not on Google Cloud Storage but on S3).', 'So you’ve now successfully prepared your data on the cloud service without having to leave your working notebook. Now It’s time to train your sentiment classifier. You decided to train a simple LSTM model and, of course, want to train your model on a full dataset. The best option here is to run the training on the Google AI Platform. ', 'The standard process of running a training job on the platform includes such steps as: creating a python file with the model, adding code for downloading data from Google Cloud Storage so ML Platform can use it, adding code to export and save the model after the training is finished, and preparing a training application package and submitting the training job to the platform. This last step can be done by using the console, command-line tools, or client libraries).', 'You prepare your code so it contains everything described above: importing required libraries, downloading data, and training and exporting the model. So just like with data processing, you can organize this code to script, save it and test on local data using one of the ML-DSL Magics: %py_script, %py_script_open or %py_load.', 'The same way you define a setup.py file for packaging code for AI Platform:', 'To describe information required to submit the training job to the AI Platform, you should define the specific Profile and Platform GCP. In the case of AI PLatform instantiating, the AIProfile class helps. It has attributes as ai_region, scale_tier, runtime_version, python_version for the AI Platform Training service setting up resources for your job, package_name, and package_dst for packaging your code and your script’s arguments.', 'Now it’s time to train. You can start the task using ML-DSL Magic %py_train. You pass your AIProfile, platform, and name of your script as arguments for this Magic.', 'ML-DSL takes all the steps related to packaging the code and submits it to AI Platform:', 'There are a couple of key differences if you want to run your training job on Amazon SageMaker. Some inputs for the training script need to be defined using specific environment variables such as SM_CHANNEL_TRAINING, SM_MODEL_DIR etc. Instead of an instance of AIProfile, you create an object of the SageMakerProfile class. There are attributes specific to submitting a training job to SageMaker: type of container (XGBoost, PyTorch, TensorFlow etc), framework_version, instance_type, and intance_count for setting up service resources to run the job.', 'The training process is the same as in the case of Google AI Platform using %py_train. You pass your AIProfile, platform, and name of your script.', 'ML-DSL returns information about the training process and results after it finishes to the output of the running cell.', 'The training job on SageMaker returns the result of a trained estimator that can be deployed to an endpoint where the model is available to provide inferences. ML-DSL provides it to the next Jupyter cell and you can run this cell and use it for deployment as shown on the picture below (using estimator method “deploy”).', 'For deploying models on platforms you can create DeployAIProfile and SageMakerProfile for AI Platform and SageMaker respectively. It’s attributes let you include information about the saved model path, framework version, custom code, name of model endpoint, and version name.', 'The deployment can be done using ML-DSL Magic %py_deploy. You send as arguments the name of the model, your profile instance, and platform.', 'For deployment models on SageMaker, you can define the SageMakerProfile instance (just like for training) and run the cell with Magic %py-deploy.', 'Here the arguments are the same with one key difference: you add the name of the python script that is the entry point (contains methods predict_fn, output_fn etc) and path where the script is located.', 'The SageMaker deployment described above returns an estimator that can be used to provide inferences.', 'To get predictions of the model deployed on the AI Platform you can use the same DeployAIProfile profile as defined for the deployment process. Except that it’s necessary to prepare some test data and send it to ML-DSL %py_test magic as an argument as well as platform and profile instance:', "Running the cell returns output of the deployed model (in our case it is an output of a softmax layer in our neural network classifier) to the next cell. You can use this output to prepare some analytics about the model's quality: classification reports, confusion_matrix etc.", 'Congratulations! You have now completed a data scientist’s job circle, which includes running spark applications on cloud clusters (Dataproc and EMR) for data processing, training and deployment of a model using cloud services (AIPlatform and SageMaker), and getting inferences of the model in Jupyter Notebook. It’s good to have all the developments in one place so ML-DSL can be a very convenient tool for your job.', 'ML-DSL was used in Grid Dynamics’ Anomaly Detection project as part of its workflow design. It has two implementations:', 'There are two areas where ML-DSL performed its role:', 'In the Amazon Cloud Services case, the CloudFormation was used for the Anomaly Detection solution deployment. Deployment manifests require Sagemaker endpoints and EMR Spark Streaming jobs deployment. The simple Lambda function was implemented, which consumed the ML-DSL library, simplifying and unifying all the related operations.', 'ML-DSL is a useful tool that can help data specialists to simplify the experience of interacting with cloud platforms and organize all working developments in Jupyter Notebooks. Currently it supports configuring and executing ML/DS pipelines on Google Cloud Platform and Amazon Web Services. ', 'ML-DSL was used in several Grid Dynamics’ projects including the ', ' project. ', 'ML-DSL will be published as open source. There are also many plans on how to further develop it including:'], 'date': '\r\n\t\t\t\t\t\t\tAug 03, 2020\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAlex Rodin', 'author_url': 'https://blog.griddynamics.com/author/alex-rodin/', 'tag': ['MLOps', 'Data Science', 'Machine Learning', 'ML', 'AI', 'Artificial Intelligence', 'AWS', 'GCP', 'Jupyter', 'Cloud', 'Google Cloud', 'Google Dataproc', 'EMR', 'Sagemaker', 'Google AI Platform', 'ML Platform']}, {'title': 'Easily move your native app to Flutter', 'article url': 'https://blog.griddynamics.com/easily-move-your-native-app-to-flutter/', 'first 160': ['\nAfter you read this blog post, the answer will become obvious - to save time and money.', "\nYou won't need to! In this blog post, we will show you how new additions to your existing code can be written in Flutter.", 'There are a few cases to consider:', 'Let us consider the first case. You have both an Android and iOS app and it is time for a major update or a complete redesign. This is an ideal time to incorporate Flutter. Using this cross-platform development tool, you can cut your development resources in half, since you only need one development and QA team, instead of two. This can free up your second resource team for other projects. Additionally, development resources are generally about 10% more efficient than using a native language development platform. This amounts to a 55% savings in resources over using the traditional native language approach. Runtime performance and code size are on par with native language apps. The only downside is the requirement to learn a new programming language, Dart. It is easy to learn but nonetheless could somewhat cut into your 55% resource savings for your first Flutter endeavor. It is best to plan on 50% resource savings for the first development cycle.', 'In the second case, you have an app for both Android and iOS and you need to redesign one of the apps. Should you consider Flutter, or should you use a Native app language, like Swift or Kotlin? Even if you only built a single app, Flutter is still an excellent choice. If your newly developed app uses Flutter, you will have more options for the future. You may elect to replace your existing app immediately, or wait until the next refresh. In the next refresh, your development time will be near zero. The look and feel of both apps will also match. From that point forward, all development and maintenance has the advantages of a true cross-platform.', "The last case is not as obvious. You have two apps written in their native languages, which don't need replacements or major upgrades yet - you just want to add a feature. Well, Flutter should still be your choice. You can use all of your existing native-language code, while  writing your new features in Flutter. In the remainder of this whitepaper, we will present a technical case study where we did just that.", 'One of our clients came to us with a request we see often - to add a customer satisfaction survey into an app that had a long history. This would require adding a whole new user flow into both the iOS and Android versions of the application.', 'The iOS and Android apps had massive code bases with mixed technology stacks. They initially started out using Objective-C and Java, but the majority of the more recent code was written with Swift and Kotlin. The new features would need to be tested carefully, and the updated version would need to be delivered to users in a very short timeframe.', 'The task to add a survey appears to be pretty straightforward. All it essentially requires is to add a new item in the main menu that leads the user to a set of new screens containing blocks of questions, and a large “Submit” button. It will also need to contain a screen that rewards the user for taking the survey.', 'Traditionally, we would require assigning two developers (one for iOS and one for Android), as well as a QA engineer to cover both platforms with tests. With Flutter, only a single development team is required, and there is no need to throw away the legacy Android and iOS code. It is possible to create a new module, and implant it into existing applications without much effort.', 'In a ', ', it has some very cool advantages over taking a more traditional native approach. Its speed and performance is similar, but it has a true cross-platform nature. This means that code has to be written only once to be used across multiple platforms, including the Web. It also has the benefit that it needs to be tested once, as it supports modern testing frameworks.', 'After some preliminary investigation work and a long discussion of potential pros and cons with the client, our team decided to take on the challenge of trying to implement the desired survey feature using Flutter.', 'Now let’s dive a little deeper into the technical details.', 'First of all, we need to add Flutter into the current apps. There are two ways this can be achieved. The most obvious route is to create a new Flutter app with the ', ' command, and then replace the generated runner projects with the current ones. However, this is a much more difficult method because we may have to do many tweaks to the current projects, and if we miss something, everything will be ruined. It also forces you to relocate your current projects inside a Flutter project folder, which is almost always an unacceptable option.', 'Fortunately, Flutter gives us another way. We can create a dedicated module with the ', " module command. It creates a new project, but uses a slightly different structure. The main advantage over the previous solution is the opportunity to have a Flutter module separated from the main code as it's added as an external dependency. This process is described in detail on ", ', and only takes 10 minutes to connect and run the Flutter module from an existing app.', 'Now that the issue has been addressed, we can move onto discussing the next tricky thing: communication between the native code and Flutter module.', 'It is now time to introduce our new feature to the app - of course, we can’t share the actual design of the app, as our client doesn’t want it publicly exposed just yet. Instead, we will just use a recreated demo-app to illustrate the main concepts.', 'Here we have an app and a User Profile screen from where the user could run a survey:', 'The code of this screen is quite lengthy, but it is fairly obvious for anybody who is familiar with iOS development. We will just show the code for the function that is responsible for the “complete a survey” action:', 'Note that currently there’s no possibility to close the Flutter screen – the module knows nothing about a host app. This means that we need to build a communication channel between the native code and Flutter module. But before we dive into that, let’s go through a little bit of theory first.', 'For now, the most convenient way to pass data between the platform code and Flutter module is to use ', '. On a very basic level, it works in the same way as networking ports: two sides connect via a named channel, and send binary messages to each other.', 'Currently, Flutter offers two usable types of channels outside of basic binary: ', ', and ', '. The Message Channel is very convenient to pass data between Flutter and the native code, while the Method Channel is useful for calling functions.', 'But for now, the only way to pass data is to serialize it into a JSON string, as there is no binary compatibility between the frameworks. And the same goes for Method Channels – you need to specify the desired function name with a string, and then check it via pattern matching, which is extremely error-prone.', 'We will need to use the platform channels in two ways. First, we want to pass some data from the native app to the Flutter module. In the case of our survey, it would be nice to be able to address the user by name. Second, we are not going to create another networking layer on the Flutter side, as we already have it in native code. So our module will send the survey results via the current API layer instead.', 'Let’s return to our app. We need to add the ability to close a survey on demand. But before that, it would be helpful to create method channels on all sides.', 'On the iOS side, we would need to add the next lines right before presenting a controller with the Flutter module:', 'If you’re wondering what is happening here, we’re opening a named method channel with our ', ' object as a ', '. Note that it’s required for all sides of communication to use exactly the same channel name. Right after opening a channel, we need to pass a user’s full name to the Flutter module via prepare method, and then set up a handler for Flutter-module side invocations. Pretty simple.', 'On the Flutter side, there’s a little more work, though almost all of that is covered by the code of the module:', 'The next step is to make an actual survey list with Flutter. It’s quite straightforward, mainly involving tossing widgets around. Here’s the final result:', "To send collected answers to the backend, we're going to use a native call rather then Dart to accomplish this. We need to serialize our data from ", ' in the Flutter module to a JSON string. This can then be sent as an argument of a call in an already opened method channel to the native app to then transfer further to the backend.', 'On the iOS side, we need to add a handler for a new method:', 'As its demo code, we just send a successful result back to a caller after a short delay. In the real app, it should be an API call with asynchronous callback.', 'On the Flutter side, the most interesting part is the handler for the submit button:', 'Let’s explain this code in a little bit more detail. First, we want to show the activity indicator, which is what the ', ' variable is for. Then, we invoke a method via the platform channel and wait for the results. Note the simplicity and elegance of asynchronous calls handling in Dart, especially compared to Swift’s GCD. The final step is to check an invocation result to show either the reward page or an error alert. Take a look at how cool it is:', 'We have simplified things for the sake of clarity. For example, our demo app could be further expanded so iOS and Android designs always match, as they would in production code.', 'Feature delivery time is dependent on the specific task, but in general development speed is cut roughly in half using the Flutter cross-platform development tool.Flutter also solves many common UI development issues, and its hot reload feature dramatically reduces development time.', 'Whether you are developing an entirely new app for one or both mobile platforms, or simply adding a new feature to existing native apps, Flutter is the ideal platform for your project. If you need further details or assistance, a ', ' will gladly walk you through it.'], 'date': '\r\n\t\t\t\t\t\t\tJul 31, 2019\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAndrey Chevozerov', 'author_url': 'https://blog.griddynamics.com/author/andrey-chevozerov/', 'tag': ['Mobile', 'User Interface']}, {'title': 'Safety stock optimization for Ship-from-Store', 'article url': 'https://blog.griddynamics.com/safety-stock-optimization-for-ship-from-store/', 'first 160': ['Speed of delivery is one of the key success factors in online retail. Retailers fiercely compete with each other in this space, implementing new order fulfillment options such as Same-Day Shipping, Buy Online Pick Up in Store (BOPUS) and Ship from Store (SFS). These capabilities help to both improve customer experience, and utilize the in-store inventory more efficiently by making it available for online customers.', 'Working with a number of retail clients on implementation of BOPUS and SFS capabilities, we figured out that these use cases bring in a number of interesting optimization challenges related to inventory reservation. Moreover, the quality of these reservation decisions directly impacts both customer experience and inventory utilization, so that the business benefits offered by new fulfillment options can be severely damaged by a lack of analysis and optimization.', 'In this post, we describe the inventory optimization problem for BOPUS and SFS use cases, and provide a case study that shows how this problem has been solved in real-life settings using machine learning methods.', 'The main idea of BOPUS and SFS is to make in-store inventory available for purchase through digital channels, so that online customers can either reserve items in a local brick-and-mortar store for pick up, or get them shipped and delivered promptly. Both scenarios assume that orders are served directly from store shelves, and thus order fulfillment personnel (order pickers) compete with customers for available units:', 'Order processing has some latency, so the units sold online can become sold out by the time the order is being fulfilled, even if the inventory data is perfectly synchronized between the store and online systems (e.g. in-store points of sale update the availability database used by the e-commerce system in real time). This leads to the inventory reservation problem: given that the store has a certain number of units of some product on-hand, how many units can be made available for online purchase, and how many units need to be reserved for store customers? Let us use the terms Available to Promise (ATP) for the first threshold, and Safety Stock (SS) for the second one. The relationship between these thresholds is shown below:', 'We assume that the online order placement process checks ATP for products in the shopping cart, and accepts the order only if the ordered quantity is less than the product’s ATP. If the ordered quantity is greater than the ATP, the order is rejected to avoid a fulfillment exception.', 'We also assume that the system works in discrete time: the inventory is replenished by some external process at the beginning of each time interval, and the on-hand inventory in the beginning of the interval is known to the order placement process. Thus, the problem boils down to the optimization of safety stock levels for individual products, stores and time intervals:', '$$', '\n\\text{ATP(product, store, time)} = \\text{onhand(product, store, time)} - \\text{SS(product, store, time)}', '\n$$', 'For example, we worked with a retailer that was able to replenish their inventory overnight, so the goal was to set a safety stock level for each product, in each store, each day. The order placement process uses the ATP calculated through the above formula as a daily quota, and compares it with the running total of units ordered online.', 'If our algorithm produces safety stock estimates that exactly match the in-store demand, it is a perfect solution - we do not have order fulfillment exceptions caused by stockouts, and all the merchandise that is unsold in the store is available for online customers. If our safety stock estimate is above the true in-store demand, we still do not have fulfillment exceptions, but our online sales can be worse than potentially possible, as some fulfillable orders will be rejected. If our estimate is below the true demand, we will have fulfilment exceptions, because some units will be double-sold. In other words, the quality of our safety stock estimates can be measured using the following two conflicting metrics:', 'A retailer can balance these two metrics differently depending on their business goals, the cost of fulfillment exceptions and the cost of underexposed inventory. For example, some retailers can redirect SFS orders from one store to another in case of exceptions at a low cost, and thus  can accept a low pick rate to achieve high exposure rates. On the other hand, some retailers might be concerned about customer dissatisfaction caused by order fulfillment exceptions, and choose to maintain high pick rates at the expense of exposure rates. Consequently, it is important to develop a safety stock optimization model that enables retailers to seamlessly balance the two metics.', 'The most basic approach to the optimization problem defined above is to set some fixed safety stock level for all products (such as 1 unit, 2 units, and so on). While this clearly provides some flexibility in terms of balancing between the pick and exposure rates (the level of zero maximizes the exposure, the level of infinity maximizes the pick rate, etc.), differences between products and demand fluctuations are not taken into account.', 'We can expect to build a better solution for safety stock settings using predictive models that forecast the demand, taking into account product properties and sales data, as well as external signals such as weather. In the remainder of this article, we will discuss a case study on how such a model has been built, and then explain how it was wrapped into another econometric model to estimate and optimize pick and exposure rates.', 'The first step towards building a safety stock optimization model was to explore the available data sets to understand the gaps and challenges in the data.', 'The first challenge apparent from the preliminary data analysis was high sparsity of sales data at a store level. We had the sales history for several years for a catalog with tens of thousands products, so that the total number of product-date pairs was close to a hundred million. However, the sales rate for the absolute majority of the products was far less than one unit a day, as shown in the table below, and thus only about 0.7% of product-date pairs had non-zero sales numbers.', 'The second major challenge was that although we had store sales and catalog data at our disposal, the historical inventory data was not available. Consequently, it was not possible to distinguish between zero demand and out of stock situations, which made it even more challenging to deal with zeros in the sales data.', '\nThe third challenge was that we had data for only one store for the pilot project, so it was not possible to improve the quality of demand forecasting by learning common patterns across different stores.', 'The sparsity of the sales data makes it difficult to predict the demand using standard machine learning methods, which is the key to a good safety stock model. One possible approach to deal with this problem is to calculate special statistics that quantify the variability of the demand timing and magnitude, and then use these statistics to switch between different demand prediction models. ', ' Consider the following two metrics that can be used for this purpose:', 'These metrics can be used to classify the demand histories into the following four categories or patterns that can often be observed in practice:', 'The cut-off values that are commonly used to differentiate between high and low ADI and CV2 values are 1.32 and 0.49, respectively. In our case, the distribution of demand histories according to the above classification was as follows:', 'Only 0.94% of demand histories were smooth, 0.16% were erratic, 3.56% were lumpy, and the absolute majority of histories, or 95.33%, were intermittent.', 'The demand classification model above can be used to switch between different demand forecasting models and techniques. However, we eventually chose to use the notion of intermittent demand in a different way, and create features that help to combine individual zeros in the demand history into groups, then use these features as inputs to demand prediction models to combat data sparsity. We will discuss this approach in the next section in further detail.', 'We built several preliminary demand models using XGBoost and LightGBM, and found that it is quite challenging to achieve good forecast accuracy using just basic techniques. However, this preliminary analysis helped to identify several areas of improvement:', 'Weather has a significant impact on the model accuracy, so weather signals (historical data or forecasts) have to be incorporated into the model as well.Each of these four issues deserves a detailed discussion, and we will go through them one by one in the following sections.', 'As was mentioned earlier, we concluded that the problem with intermittent demand can be sufficiently mitigated by calculating the following three statistics for each date in the demand history, and using these series of statistics as inputs to the demand forecasting model:', 'These metrics help the forecasting model to group zeros together and find some regularities, which decreases the noise in the original data created by the sparsity of non-zero demand observations. This approach is conceptually similar to latent variables in Bayesian statistics.', 'Intermittent demand statistics improve the accuracy of the forecast by finding regularities in zero demand samples, so we expected to get even better results by building a model that discovers even more regularities caused by stockout (that were not observed explicitly because we lacked inventory data).', 'We started with a heuristic assumption that a long series of zeros in the demand history indicates that the item is out of stock. We used this assumption to attribute each date in the demand histories from the training set with a binary label that indicates whether or not the product is considered out of stock. This label was then used as a training label to build a classification model that uses demand history, product attributes and intermittent demand statistics as input features to predict stockouts for a given pair of a product and date.', 'We implemented this model using LightGBM and a Bayesian optimization from the GPyOpt package for hyperparameter tuning. This model in isolation achieved quite good accuracy on the test set, as shown in the confusion matrix below:', 'There are two elements to the problem of balancing pick and exposure rates:', 'The first problem can be solved by introducing a controllable bias into the demand prediction model. A model that systematically underestimates the demand will be biased towards higher exposure rate, and a model that systematically overestimates will be biased toward higher pick rate.', 'This idea can be implemented using an asymmetric loss function where the asymmetry is controlled by a hyperparameter. We decided to use the following loss function, which  can be readily implemented in LightGBM:', '\n$$', '\n\\begin{aligned}', '\nL(x) = \\begin{cases}', '\n\\beta \\cdot x^2, \\quad &x\\le 0 \\\\', '\nx^2, \\quad &x > 0', '\n\\end{cases}', '\n\\end{aligned}', '\n$$', '\nwhere $\\beta$ is the asymmetry parameter that controls the model bias. This asymmetric loss function enabled us to build a family of models for different values of the penalty parameter, and switch between them to achieve arbitrary tradeoffs.', 'The second problem with estimating the actual business metrics is much more challenging, and it required us to develop a special mathematical model which we will discuss later in this article. The model for business metrics is clearly important for safety stock optimization, but it does not directly impact the design of the demand prediction model.', 'The in-store demand is obviously influenced by weather conditions, so we used the data set provided by the National Oceanic and Atmospheric Administration (NOAA) to pull features like the following:', 'The final architecture includes two predictive models - the stockout classification model described above, and the second one for the final demand prediction. This layout is shown in the following diagram:', 'Both models are implemented using LightGBM and the GPyOpt library for hyperparameter tuning. This architecture was used to produce 20 different demand predictions for 20 different values of the asymmetry parameter.', 'The following chart shows how the accuracy of prediction gradually improved as more components and features were added (for the non-biased variant of the model):', 'The demand prediction model produces the outputs that can be used for setting safety stock. However, safety stock values alone are not enough to estimate the exposure and pick rates. In this section, we show how we developed a model that helps to estimate these business metrics. The estimation of exposure and pick rates is important because the inventory optimization system has to guarantee certain service level agreements (SLAs) that are defined or constrained in terms of such metrics, and cannot just blindly pick some value of the asymmetry parameter.', 'Let us denote the observed in-store demand (quantity sold) for product $i$ at time interval $t$ as $d_{it}$. The demand prediction model estimates this value as $\\widehat{d}_{it}$, and we set safety stock based on this estimate', '\n$$', '\n\\text{safety stock}_{it} = T(\\widehat{d}_{it})', '\n$$', 'where $T(x)$ is a rounding function, as we can stock and sell only integer numbers of product units. The estimation error is given by the following expression:', '\n$$', '\n\\varepsilon_{it} = d_{it} - \\widehat{d}_{it}', '\n$$', 'A positive error means that safety stock is underestimated (too few units allocated for in-store customers) and generally leads to double selling in-store and online. A negative error means an overestimate (too many units allocated for in-store customers), leading to low exposure rates. Let us also denote the on-hand inventory as $q_{it}$. Thus, the actual in-store residuals available for online sales (which is the ideal ATP) will be as follows:', '\n$$', '\nx_{it} = q_{it} - d_{it}', '\n$$', 'In our case, the on-hand inventory was not known, so we made an assumption that this quantity is proportional to several past demand observations:', '\n$$', '\n\\widehat{q}_{it} = \\alpha \\cdot \\frac{1}{n} \\sum_{\\tau=1}^n d_{i, t-\\tau}', '\n$$', 'where $\\alpha$ is a model parameter that controls or reflects the product replenishment policy. The smaller values of this parameter correspond to lower average stock levels (compared to the average demand), and the greater values correspond to higher stocks levels. From a business perspective, this parameter is linked to the inventory turnover rate, and reflects how conservative the inventory management strategy is. We can set the inventory level parameter based on our estimate of what this level actually is for the given replenishment policy, or we can evaluate the model for different levels, see how the stock level influences the pick and exposure rates, and adjust the replenishment policy based on the results.', 'The above assumption enables us to estimate the residuals for online sales as follows:', '\n$$', '\n\\widehat{x}_{it} = \\max \\left[ T(\\widehat{q}_{it} - d_{it}), \\ 0 \\right]', '\n$$', 'Using our models, we can now easily estimate the online exposure rate as the expected ratio between the predicted ATP (which includes the prediction error) and the ideal ATP (residuals) based on the historical data:', '\n$$', '\n\\text{ER}(\\alpha) = \\mathbb{E}_{it} \\left[ \\frac{\\widehat{x}_{it} + T(\\varepsilon_{it})}{\\widehat{x}_{it}} \\right]', '\n$$', 'Next, we need to estimate the pick rate, which is slightly more complicated, as it generally depends on the online demand. Let us start with the observation that an order fulfillment exception occurs when the online demand (let’s denote it as $d_{it}^o$) exceeds the in-store residual:', '\n$$', '\n\\text{fulfilment exception:}\\quad d_{it}^o > x_{it}', '\n$$', 'The pick rate is essentially a probability of the fulfilment without any exceptions that can be expressed through probabilities, and is conditioned on positive and negative prediction errors:', '\n$$', '\n\\begin{aligned}', '\n\\text{PR}(\\alpha) &= p(d_{it}^o \\le x_{it}) \\\\', '\n&= p(\\varepsilon_{it} \\le 0) \\cdot p(d_{it}^o \\le x_{it} \\ |\\ \\varepsilon \\le 0) -', '\np(\\varepsilon_{it} > 0) \\cdot p(d_{it}^o \\le x_{it} \\ |\\ \\varepsilon > 0)', '\n\\end{aligned}', '\n$$', 'In the first term, the probability of fulfillment without an exception given the non-positive prediction error equals 1 (the exception cannot occur because we overestimate the demand and thus set safety stock too conservatively).', 'The probability in the second term generally depends on the online demand distribution. If this distribution is known, then the term can be estimated straightforwardly. In our case, the distribution of online demand was unknown because the the safety stock model was developed in parallel with transactional systems for the ship from store functionality. We worked around this issue by making certain assumptions about the demand distribution. First, let us note that the online demand is bounded by the estimated ATP (the online system will simply start to reject orders once ATP is exhausted):', '\n$$', '\nd_{it}^o \\le x_{it} + T(\\varepsilon_{it})', '\n$$', 'Second, we can choose some parametric demand distribution over the interval from zero to the ATP based on what we know about the online demand. The simplest choice will be a uniform distribution:', '\n$$', '\nd_{it}^o \\sim \\text{uniform}(0,\\ x_{it} + T(\\varepsilon_{it}))', '\n$$', 'This assumption can be illustrated as follows:', 'Consequently, the probability that a demand sample falls into the availability zone is given by:', '\n$$', '\np\\left(d_{it}^o \\le x_{it}\\ |\\ \\varepsilon > 0\\right) = \\frac{x_{it}}{x_{it} + T(\\varepsilon_{it})}', '\n$$', 'Collecting everything together, we get the following expression for the pick rate that can be evaluated as empirical probabilities based on the historical data:', '\n$$', '\n\\text{PR}(\\alpha) = \\mathbb{E}_{it}\\left[ p(\\varepsilon_{it} \\le 0) - p(\\varepsilon_{it} > 0) \\cdot \\frac{x_{it}}{x_{it} + T(\\varepsilon_{it})} \\right]', '\n$$', 'The above model is a convenient and flexible framework for safety stock evaluation. This model makes a number of assumptions regarding the online demand and inventory distributions to work around the data gaps we had, but it is quite easy to adjust for other scenarios depending on the available information and data about the environment.', 'The predictive and econometric model defined above can be used to estimate the pick and exposure rates for different values of model parameters, and then choose the optimal tradeoff. Recall that these models have two major parameters:', 'In this section, we show how the full model was evaluated for different values of these two parameters to build safety stock optimization profiles, find optimal values, and estimate the uplift delivered by the model.', 'First, we plot the dependency between the pick rate and the replenishment parameter $\\alpha$ averaged by all products. The following plot shows this dependency for different values of the model bias parameter $\\beta$, and the curves that correspond to the baseline (naive) policy of setting safety stock to 1, 2 or 3 units:', 'We can achieve arbitrarily high pick rates by choosing large values of the replenishment parameter - the safety stock policy becomes immaterial when the stock level significantly exceeds the in-store demand. Consequently, all curves in the chart are monotonically increasing, and are approaching the ideal pick rate as the replenishment parameter increases. The steepness of the curves depends on how aggressive the reservation policy is - the higher the value of the safety stock, the steeper the curve. The curves produced by our predictive model are in between the curves for the baseline policies, as the predictive model is essentially trying to differentiate safety stock values by products that result in lower average safety stock levels.', 'Next, we can make a similar plot for the dependency between the mean exposure or the exposure rate and the replenishment parameter $\\alpha$. Again, the exposure metrics can be made arbitrarily good by increasing the average stock level controlled by the parameter $\\alpha$. For example, the mean ATP (measured in the number of product units) can be visualized as follows:', 'The above charts can help to make operational decisions, such as selecting the best parameter $\\beta$ for a given replenishment SLA parameter $\\alpha$. The drawback of this representation is that the pick and exposure rates are directly related, and the separate charts do not visualize the tradeoff between the two metrics. From that perspective, it makes sense to plot the pick and exposure rates on one plot, so that each curve corresponds to a certain value of $\\alpha$, and each point on a curve corresponds to a certain pair of parameters $\\alpha$ and $\\beta$:', 'This chart clearly shows the overall performance of the solution - the family of curves that correspond to our optimization model are well above the curves that correspond to the baseline policies (fixed safety stock values). This means that the optimization model consistently provides a better tradeoff between the pick rate and exposure objectives than the baseline policy. For example, the pick rate uplift in the above chart is about 10% for a wide range of exposure rates. We used this representation to find the optimal tradeoff based on the pick and exposure rate preferences, as well as stock level constraints reflected by the curves for different values of $\\alpha$. We were then able to choose the optimal value for the model bias parameter $\\beta$.', 'Innovations in the area of order delivery are critically important for online retail. These innovations often come with new operational and optimization challenges that can be addressed using advanced analytics and machine learning. In this article, we presented a case study on how such an optimization apparatus was developed for Ship from Store and Buy Online Pick Up in Store use cases.', 'We showed that demand prediction at the level of individual stores has major challenges caused by sporadic demand patterns, and these challenges can be partly addressed by specialized demand modeling techniques and choice of input signals. We also showed that predictive modeling does not fully solve the problem, and one needs to build an econometric model to properly interpret the outputs of the predictive model and the benefits from it. The econometric model we developed also illustrates some techniques that can be used to work around data gaps, which can be caused by various reasons, including parallel development of optimization and transactional systems, and organizational challenges.'], 'date': '\r\n\t\t\t\t\t\t\tMar 08, 2019\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tIlya Katsov', 'author_url': 'https://blog.griddynamics.com/author/ilya-katsov/', 'tag': ['Machine Learning and Artificial Intelligence', 'Data Science']}, {'title': 'Predictive analytics for promotion and price optimization', 'article url': 'https://blog.griddynamics.com/predictive-analytics-for-promotion-and-price-optimization/', 'first 160': ['Pricing decisions are critically important for any business, as pricing is directly linked to consumer demand and company profits. Even a slightly suboptimal decision-making process inevitably leads to tangible losses, and major mistakes can have grave consequences.', 'Optimal pricing is a challenging problem for several reasons. One is the complex structure of the price waterfall, which often includes multiple variables such as list prices, discounts, and special offers that need to be optimized. Another reason is the complexity of demand and profit forecasting, which makes it difficult to evaluate new pricing strategies accurately. Finally, non-optimal pricing decisions are often caused by a lack of coordination between the teams that are responsible for various aspects of pricing.', 'Although the fundamentals of price optimization are well understood, our experience with dozens of leading retailers clearly indicates that retail practitioners are struggling with certain pricing decisions. Even ones that presumably make the right pricing decisions are often uncertain if they have unharvested profits or avoidable losses due to suboptimal pricing, and the lack of tools and techniques to measure it quantitatively.', 'At Grid Dynamics, we know that economic modeling and machine learning can greatly help improve the quality of pricing decisions. To demonstrate how it works and simplify the development of similar solutions for our clients, we created a reference implementation of a price management tool that showcases the main capabilities of AI-based price optimization and also features several advanced techniques.', 'The price management process has to deal with many variables and use cases because pricing typically has a complex structure. As a basic example, consider a retailer who buys a certain product from a supplier at a supplier price, adds a markup to obtain a list price, optionally applies one or more markdowns, and finally accounts for variable and fixed costs to calculate the profit margin. The price structure of this imaginary retailer is visualized in the chart below (the so-called price waterfall):', 'Even this oversimplified environment requires building a price management system that understands the difference between markups and markdowns, knows how each of these two price components influences the demand and profit, and can find the optimal trade-off between the two. A more realistic setup is likely to include many more variables such as different promotion types, inventory constraints, differences between stores and regions, and product substitution effects. ', '. We will describe how to achieve this goal in the sections dedicated to solution design and predictive modeling.', 'Although the core system can be extensible enough to support a wide range of use cases, its interface and consumed data elements need to explicitly support concrete use cases. We decided to use promotion price optimization as the primary business case, and create case-specific services and interfaces that demonstrate how this aspect of price management can be solved using our system.', 'To define the problem in a more formal and detailed way, we have chosen a set of assumptions that are typical for many apparel retailers. First, we assumed that the product list prices are fixed, and that merchandisers maintain a database of promotions in which each promotion is configured by triggering rules (e.g., the purchase total must be more than ', '100) and actions (e.g., provide a ', '20 discount). For each transaction, the pricing engine pulls active promotions from this database based on products in the shopping cart, and then calculates the final sales price applying these promotions, as illustrated in the figure below:', 'Next, we decided to support several types of promotions that are typical for apparel retailers and department stores:', 'The cart-based promotions are particularly tricky. Since the rules are applied to individual shopping carts, the final sales price of a product is unique for each transaction and can vary depending on other products in the cart. Consequently, the sales price of any given product at any moment can be described only as a statistical distribution of prices rather than as a single number. This can be a significant challenge to building the price-demand model needed for promotion optimization, but this promotion type is very common and thus worth some research.', 'In the settings defined above, a merchandiser needs to make a number of decisions:', 'These questions are traditionally answered using last year’s data combined with keen tracking of the current sales data and trends. This approach may be more or less efficient depending on the nature of merchandise, properties of the customer base, skillfulness of merchandisers, and external factors such as market growth or decline. Our experience with many large retailers indicates that, on average, traditional techniques are not optimal in the sense that profits are partly lost due to “harmful” discounts that should be removed. Revenues are also much lower than they could be because of a mismatch between the price and demand. The impact of these issues may or may not be significant, but it is quite remarkable that most companies do not have the tools and analytical techniques needed to estimate this gap in optimal pricing, and thus do not really know whether they have a problem or not.', 'If we can build a digital model of a retailer and its customers that allows a what-if analysis of promotion-related scenarios (e.g. how profit will change if we introduce a 5% discount), one of the immediate benefits would be the ability to quantitatively assess pricing decisions. For example, a merchandiser would be able to forecast the incremental profit delivered by a new promotion and make sure that it does not interfere with other promotions before running a promotional campaign. This capability alone can help to prevent losses and streamline the promotion management process. The model then can be used to automatically answer all the questions listed in the beginning of this section: find profit-optimal combinations of promotions, tune promotion properties, and find new promotional opportunities.', 'We envisioned a system that provides a merchandiser with a simple, but powerful promotion optimization workflow. The flow starts with the selection of a product category, and once the category is selected, the system takes the merchandiser through the steps illustrated with the wireframes below:', 'The solution described above relies on the ability to accurately predict revenue, profit, and demand, taking the parameters of the planned promotional campaigns into account. It requires building predictive models for these values that can be later used for manual what-if analysis or automatic optimization. To better understand how these models can be designed and used, let us briefly review the basics of the price management theory.', 'The most basic scenario one can consider is the static optimization of the list price for a single product. Assuming that we have historical data where product price varied over time, we can try to train a predictive model to learn the price-demand dependency, and find a price point that maximizes the revenue, which is a product of price and demand (the gray rectangle in the figure below):', 'The above figure also suggests that having any single price may not be optimal because some revenue (and therefore profit) remains unharvested. At any given price point, we are likely to have customers who would be buying the product even at a higher price (and thus delivering additional profits). At the same time, we would have customers who do not buy the product, but potentially could  at a lower, but still profitable price. Marketers typically work around this limitation by dividing the market into segments and setting different price points in each segment to capture additional profits, as illustrated below:', 'Examples of such segmentation are: separation of mainstream and luxury sub-brands, different prices in areas with high and low incomes, and more. Limited-time promotions are also a segmentation technique, but the segmentation is done over time - a retailer first captures profits at a regular price from less price-sensitive customers, and then captures additional profits at a discounted price from more price-sensitive customers.', 'We can conclude that the price optimization system should be designed to predict future revenue, profit, or demand for a certain period of time as a function of variables like list price, discount depth, and competitor prices. This is done for various segments that can be defined in terms of customers, store locations, time periods, and products. The total profit can then be summed across all of the segments:', 'At a conceptual level, a solution designed this way provides promotion optimization capability (where time is the segmentation dimension and discount is the variable to be optimized), but also provides enough flexibility to support other business cases ranging from list price optimization to assortment optimization.', 'In some environments, revenue and profit can be straightforwardly and deterministically calculated from the demand: revenue is a product of price and demand, and profit is a product of margin and demand. Consequently, it can be enough to build only a demand prediction model and multiply its output by price or margin to obtain a revenue or profit function that, in turn, can be plugged into an optimization algorithm. Unfortunately, this is not possible in the environment we described above, as promotions can be applied to a shopping cart, which breaks down the simple relationship between the quantity sold and revenue: one product can be sold at different prices depending on other products in the cart. To work around this issue, we decided to create three different models to predict revenue, profits, and demand, respectively. These models share the same feature design, but are fitted separately for these three training labels.', 'The model is designed to predict the metric of interest (revenue, profit, or demand) for a given individual product at a given date. The input feature vector includes the following groups of variables:', 'Finally, price and promotion variables were calculated not only for the date to be predicted, but also for historical dates with lags of one week, one month, and one year to account for autocorrelations, as shown below:', 'To build the forecast, the system moves from left to right and fills in the predicted values for every single day. Note that the earliest predictions on the left can be used to build lagged features for the later predictions on the right. For example, the forecast for the 14th day can use the forecast for the 7th day as an input.', 'Another important thing to  consider is that the feature vector incorporates product attributes, and thus the same design can be used to forecast the demand and profit for new products without sales histories or shorts sales histories [1].', 'For training and validation, we used statistics from real retailers to create a probabilistic generative model that reflects the main purchasing and promotional patterns for orders that occur in the real world. This model was used to generate 3 years of order history for one product category of 10K products and 50K products, making up a total of 500K orders. This order history included 60 promotional campaigns total. The gradient boosted decision trees model was trained on this historical data, and was then used to predict a validation sample of 100 days. An example validation chart for the profit for one category is shown below:', 'The prediction accuracy of the model is generally good for practical purposes of promotion evaluation. However, one of the main challenges is accurately predicting sales spikes for slow moving products, as the model tends to underestimate such spikes. This problem partly stems from the limited amount of historical data that we used, which meant that the spikes in our data are rare. This issue can be mitigated by training on a larger number of spikes, or through more elaborated modeling specifically for this case.', 'Once we have revenue, profit, and demand prediction models for individual products, we can then plug them into various optimization algorithms:', 'The technical solution includes the modeling subsystem and promotion optimization subsystem. The modeling subsystem consolidates transactional, promotional, and catalog data, and makes this data available for a data scientist who does feature engineering and designs the predictive models. The models are scheduled for regular re-training and made available for the promotion optimization system. Technologically, the modeling part of the solution is based on Spark, Python, and Python ML libs.', 'The optimization system includes a user portal where promotion and campaigns can be configured, and an optimization server that forecasts the performance of individual promotions or the entire promotion mix. This architecture is illustrated below:', 'The user portal was created based on the solution vision mockups we described earlier in the corresponding section. The following screen recording demonstrates how a merchandiser does a what-if analysis of the promotion mix:', 'Before we wrap up this post, let’s briefly discuss several questions that were frequently asked in connection with this work:', '[1] Marshall Fisher and Ananth Raman, The New Science of Retailing: How Analytics are Transforming the Supply Chain and Improving Performance, Harvard Business Review Press, 2010'], 'date': '\r\n\t\t\t\t\t\t\tAug 07, 2018\r\n\t\t\t\t\t\t\t• ', 'author': '\r\n\t\t\t\t\t\t\t\t\t\t\tAlex Rodin', 'author_url': 'https://blog.griddynamics.com/author/alex-rodin/', 'tag': ['Machine Learning and Artificial Intelligence', 'Data Science']}]